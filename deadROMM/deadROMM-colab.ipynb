{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"deadROMM-colab.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ZeXXd1p9X9jw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1596648021454,"user_tz":240,"elapsed":86040,"user":{"displayName":"Phil Fahn-Lai","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgaLRzhwWuE2_ZNOkUUkb6lkk-noGiIsb7ucOzW=s64","userId":"13631433551768862211"}},"outputId":"10c2500e-c8cd-49e9-ae39-66f547edf24c"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","!pip install certifi chardet click easydict h5py~=2.7 intel-openmp imgaug ipython ipython-genutils matplotlib==3.0.3 moviepy numpy==1.16.4 opencv-python~=3.4 pandas patsy \n","!pip install python-dateutil pyyaml>=5.1 requests ruamel.yaml~=0.15 setuptools scikit-image scikit-learn scipy six statsmodels tables tensorpack>=0.9.7.1 tqdm wheel\n","%tensorflow_version 1.x\n","import numpy as np\n","import time\n","import sys\n","import os\n","import importlib\n","\n","from IPython.display import HTML, display\n","def set_css():\n","  display(HTML('''\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  '''))\n","get_ipython().events.register('pre_run_cell', set_css)\n","\n","from google.colab import output\n","def alert_done():\n","  output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/b/bd/Bienenkorbglocke.1133.Hz.ogg\").play()')\n","\n","os.environ[\"DLClight\"]=\"True\"\n","%cd drive/My\\ Drive/Development/DeepLabCut\n","import deeplabcut\n","from deadROMM import possumPolish\n","\n","alert_done()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (2020.6.20)\n","Requirement already satisfied: chardet in /usr/local/lib/python3.6/dist-packages (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (7.1.2)\n","Requirement already satisfied: easydict in /usr/local/lib/python3.6/dist-packages (1.9)\n","Requirement already satisfied: h5py~=2.7 in /usr/local/lib/python3.6/dist-packages (2.10.0)\n","Requirement already satisfied: intel-openmp in /usr/local/lib/python3.6/dist-packages (2020.0.133)\n","Requirement already satisfied: imgaug in /usr/local/lib/python3.6/dist-packages (0.2.9)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (5.5.0)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (0.2.0)\n","Collecting matplotlib==3.0.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/69/f5e05f578585ed9935247be3788b374f90701296a70c8871bcd6d21edb00/matplotlib-3.0.3-cp36-cp36m-manylinux1_x86_64.whl (13.0MB)\n","\u001b[K     |████████████████████████████████| 13.0MB 237kB/s \n","\u001b[?25hRequirement already satisfied: moviepy in /usr/local/lib/python3.6/dist-packages (0.2.3.5)\n","Collecting numpy==1.16.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/2d/e4656149cbadd3a8a0369fcd1a9c7d61cc7b87b3903b85389c70c989a696/numpy-1.16.4-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n","\u001b[K     |████████████████████████████████| 17.3MB 201kB/s \n","\u001b[?25hCollecting opencv-python~=3.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/f6/1c276a88cc13e76f8209e2004a6b797c9a4b4f9781d70839330984bef78e/opencv_python-3.4.10.35-cp36-cp36m-manylinux2014_x86_64.whl (43.3MB)\n","\u001b[K     |████████████████████████████████| 43.3MB 69kB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.5)\n","Requirement already satisfied: patsy in /usr/local/lib/python3.6/dist-packages (0.5.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py~=2.7) (1.15.0)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from imgaug) (2.4.1)\n","Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug) (0.16.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from imgaug) (7.0.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.4.1)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.7.0)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython) (0.8.1)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython) (4.3.3)\n","Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython) (4.8.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython) (49.2.0)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython) (1.0.18)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython) (2.1.3)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython) (0.7.5)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython) (4.4.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3) (1.2.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3) (2.8.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3) (2.4.7)\n","Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.6/dist-packages (from moviepy) (4.41.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug) (2.4)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug) (1.1.1)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython) (0.6.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython) (0.2.5)\n","\u001b[31mERROR: umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n","\u001b[31mERROR: plotnine 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.0.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: mizani 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.0.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: numpy, matplotlib, opencv-python\n","  Found existing installation: numpy 1.18.5\n","    Uninstalling numpy-1.18.5:\n","      Successfully uninstalled numpy-1.18.5\n","  Found existing installation: matplotlib 3.2.2\n","    Uninstalling matplotlib-3.2.2:\n","      Successfully uninstalled matplotlib-3.2.2\n","  Found existing installation: opencv-python 4.1.2.30\n","    Uninstalling opencv-python-4.1.2.30:\n","      Successfully uninstalled opencv-python-4.1.2.30\n","Successfully installed matplotlib-3.0.3 numpy-1.16.4 opencv-python-3.4.10.35\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["matplotlib","mpl_toolkits","numpy"]}}},"metadata":{"tags":[]}},{"output_type":"stream","text":["TensorFlow 1.x selected.\n","/content/drive/My Drive/Development/DeepLabCut\n","DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Tz2IjevXktzJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":238},"executionInfo":{"status":"ok","timestamp":1596648927237,"user_tz":240,"elapsed":1046,"user":{"displayName":"Phil Fahn-Lai","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgaLRzhwWuE2_ZNOkUUkb6lkk-noGiIsb7ucOzW=s64","userId":"13631433551768862211"}},"outputId":"89599615-83a5-46e9-d5bb-fe26448975c2"},"source":["root = '/content/drive/My Drive/Development/DeepLabCut'\n","\n","importlib.reload(possumPolish)\n","model = possumPolish.Project()\n","\n","# config_path = model.load('./deadROMM/profiles-colab.yaml','dv92', './dev/demo_blank_dv92_biceps_3pec_delt/config.yaml') #92\n","# config_path = model.load('./deadROMM/profiles-colab.yaml','dv101right', './dev/possum101right_biceps_triceps-Phil-2020-06-08/config.yaml') #101R\n","config_path = model.load('./deadROMM/profiles-colab.yaml','dv101left', './dev/possum101_11Apr-Phil-2020-04-13-diff/config.yaml') #101L\n","# config_path = model.load('./deadROMM/profiles-colab.yaml','dv92', './dev/dv92_biceps_3pec_delt-Phil-2020-06-09/config.yaml') #92\n","# config_path = model.load('./deadROMM/profiles-colab.yaml','dv85left', './dev/dv85_left_biceps_teres_lat-Phil-2020-06-09/config.yaml') #85L\n","# config_path = model.load('./deadROMM/profiles-colab.yaml','dv85right', './dev/dv85_right_triceps_lat-Phil-2020-06-09/config.yaml') #85R\n","# config_path = model.load('./deadROMM/profiles-colab.yaml','dv88left', './dev/dv88_left_teresmaj_pec-Phil-2020-06-09/config.yaml') #88L\n","# config_path = model.load('./deadROMM/profiles-colab.yaml','dv88right', './dev/dv88_trilong_pec-Phil-2020-06-09/config.yaml') #88R\n","# config_path = model.load('./deadROMM/profiles-colab.yaml','sm105', './dev/sm105-Phil-2020-06-09/config.yaml') #105\n","# config_path = model.load('./deadROMM/profiles-colab.yaml','sm108', './dev/sm108-Phil-2020-06-09/config.yaml') #108\n","# config_path = model.load('./deadROMM/profiles-colab.yaml','sm126', './dev/sm126-Phil-2020-06-09/config.yaml') #126\n","\n","# model.importXma(next(iter(model.config['history'].keys())))  #e.g. './dev/dv92_biceps...' no quotes\n","# model.dlc.create_training_dataset(model.yaml)\n","\n","alert_done()"],"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/content/drive/My Drive/Development/DeepLabCut/deadROMM/possumPolish.py:168: UnsafeLoaderWarning: \n","The default 'Loader' for 'load(stream)' without further arguments can be unsafe.\n","Use 'load(stream, Loader=ruamel.yaml.Loader)' explicitly if that is OK.\n","Alternatively include the following in your code:\n","\n","  import warnings\n","  warnings.simplefilter('ignore', ruamel.yaml.error.UnsafeLoaderWarning)\n","\n","In most other cases you should consider using 'safe_load(stream)'\n","  profiles = ruamel.yaml.load(open(self.profile_path))\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded profile dv101left\n","Generated absolute paths to project directories\n","Successfully loaded profile dv101left\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7MWE8GzUqAhf","colab_type":"text"},"source":["Increment iteration and retrain"]},{"cell_type":"code","metadata":{"id":"bP3zg_5go9CE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"626e9e27-8910-4ea1-9490-a3bb80337b60"},"source":["trainposeconfigfile,testposeconfigfile,snapshotfolder=model.dlc.return_train_network_path(config_path,1,0.95)\n","cfg_dlc=model.dlc.auxiliaryfunctions.read_plainconfig(trainposeconfigfile)\n","# cfg_dlc['augmentationprobability']=0.25\n","cfg_dlc['batch_size']=4\n","# cfg_dlc['hist_eq']=True\n","# cfg_dlc['gamma']=False\n","# cfg_dlc['logcontrast']=False\n","# cfg_dlc['allchannelsclahe']=True\n","cfg_dlc['optimizer'] =\"sgd\"\n","cfg_dlc['dataset_type']='imgaug'\n","# cfg_dlc['multi_step']=[[1e-4, 7500], [5*1e-5, 12000], [1e-5, 50000], [5e-6, 200000]]\n","# cfg_dlc['global_scale']=1.0\n","cfg_dlc['scale_jitter_lo'] = 1.0\n","cfg_dlc['scale_jitter_up'] = 1.0\n","cfg_dlc['global_scale'] = 1.0\n","\n","\n","# cfg_dlc['init_weights']= '/content/drive/My Drive/Development/DeepLabCut/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt'\n","# cfg_dlc['init_weights']=model.getLatestSnapshot(snapshotfolder) # to resume from latest snapshot\n","\n","model.dlc.auxiliaryfunctions.write_plainconfig(trainposeconfigfile,cfg_dlc)\n","model.dlc.train_network(config_path, saveiters=10000,displayiters=50,maxiters=300000,max_snapshots_to_keep=15, allow_growth=True)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Config:\n","{'all_joints': [[0],\n","                [1],\n","                [2],\n","                [3],\n","                [4],\n","                [5],\n","                [6],\n","                [7],\n","                [8],\n","                [9],\n","                [10],\n","                [11],\n","                [12],\n","                [13],\n","                [14],\n","                [15],\n","                [16],\n","                [17],\n","                [18],\n","                [19],\n","                [20],\n","                [21],\n","                [22],\n","                [23],\n","                [24],\n","                [25],\n","                [26],\n","                [27],\n","                [28],\n","                [29],\n","                [30],\n","                [31],\n","                [32],\n","                [33],\n","                [34],\n","                [35],\n","                [36],\n","                [37],\n","                [38],\n","                [39],\n","                [40],\n","                [41],\n","                [42],\n","                [43]],\n"," 'all_joints_names': ['Body_ds1_crn_cam1',\n","                      'Body_ds1_crn_cam2',\n","                      'Body_ds2_int_cam1',\n","                      'Body_ds2_int_cam2',\n","                      'Body_ds3_cdl_cam1',\n","                      'Body_ds3_cdl_cam2',\n","                      'Body_vn1_crn_cam1',\n","                      'Body_vn1_crn_cam2',\n","                      'Body_vn2_int_cam1',\n","                      'Body_vn2_int_cam2',\n","                      'Body_vn3_cdl_cam1',\n","                      'Body_vn3_cdl_cam2',\n","                      'Scapula_acr_cam1',\n","                      'Scapula_acr_cam2',\n","                      'Scapula_spi_cam1',\n","                      'Scapula_spi_cam2',\n","                      'Scapula_vtb_cam1',\n","                      'Scapula_vtb_cam2',\n","                      'Humerus_dpc_cam1',\n","                      'Humerus_dpc_cam2',\n","                      'Humerus_ent_cam1',\n","                      'Humerus_ent_cam2',\n","                      'Humerus_ect_cam1',\n","                      'Humerus_ect_cam2',\n","                      'Ulna_olc_cam1',\n","                      'Ulna_olc_cam2',\n","                      'Ulna_int_cam1',\n","                      'Ulna_int_cam2',\n","                      'Ulna_dst_cam1',\n","                      'Ulna_dst_cam2',\n","                      'Radius_prx_cam1',\n","                      'Radius_prx_cam2',\n","                      'Radius_int_cam1',\n","                      'Radius_int_cam2',\n","                      'Radius_dst_cam1',\n","                      'Radius_dst_cam2',\n","                      'Teres_maj_prx_cam1',\n","                      'Teres_maj_prx_cam2',\n","                      'Teres_maj_dst_cam1',\n","                      'Teres_maj_dst_cam2',\n","                      'Biceps_prx_cam1',\n","                      'Biceps_prx_cam2',\n","                      'Biceps_dst_cam1',\n","                      'Biceps_dst_cam2'],\n"," 'batch_size': 4,\n"," 'bottomheight': 400,\n"," 'crop': True,\n"," 'crop_pad': 0,\n"," 'cropratio': 0.4,\n"," 'dataset': 'training-datasets/iteration-5/UnaugmentedDataSet_possum101_11AprApr13/possum101_11Apr_Phil95shuffle1.mat',\n"," 'dataset_type': 'imgaug',\n"," 'deconvolutionstride': 2,\n"," 'deterministic': False,\n"," 'display_iters': 1000,\n"," 'fg_fraction': 0.25,\n"," 'global_scale': 1.0,\n"," 'init_weights': '/content/drive/My '\n","                 'Drive/Development/DeepLabCut/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',\n"," 'intermediate_supervision': False,\n"," 'intermediate_supervision_layer': 12,\n"," 'leftwidth': 400,\n"," 'location_refinement': True,\n"," 'locref_huber_loss': True,\n"," 'locref_loss_weight': 0.05,\n"," 'locref_stdev': 7.2801,\n"," 'log_dir': 'log',\n"," 'max_input_size': 1500,\n"," 'mean_pixel': [123.68, 116.779, 103.939],\n"," 'metadataset': 'training-datasets/iteration-5/UnaugmentedDataSet_possum101_11AprApr13/Documentation_data-possum101_11Apr_95shuffle1.pickle',\n"," 'min_input_size': 64,\n"," 'minsize': 100,\n"," 'mirror': False,\n"," 'multi_step': [[0.005, 10000],\n","                [0.02, 430000],\n","                [0.002, 730000],\n","                [0.001, 1030000]],\n"," 'net_type': 'resnet_50',\n"," 'num_joints': 44,\n"," 'optimizer': 'sgd',\n"," 'output_stride': 16,\n"," 'pos_dist_thresh': 17,\n"," 'project_path': '/content/drive/My '\n","                 'Drive/Development/DeepLabCut/dev/possum101_11Apr-Phil-2020-04-13-diff',\n"," 'regularize': False,\n"," 'rightwidth': 400,\n"," 'save_iters': 50000,\n"," 'scale_jitter_lo': 1.0,\n"," 'scale_jitter_up': 1.0,\n"," 'scoremap_dir': 'test',\n"," 'shuffle': True,\n"," 'snapshot_prefix': '/content/drive/My '\n","                    'Drive/Development/DeepLabCut/dev/possum101_11Apr-Phil-2020-04-13-diff/dlc-models/iteration-5/possum101_11AprApr13-trainset95shuffle1/train/snapshot',\n"," 'stride': 8.0,\n"," 'topheight': 400,\n"," 'weigh_negatives': False,\n"," 'weigh_only_present_joints': False,\n"," 'weigh_part_predictions': False,\n"," 'weight_decay': 0.0001}\n"],"name":"stderr"},{"output_type":"stream","text":["Starting with imgaug pose-dataset loader.\n","Batch Size is 4\n","Initializing ResNet\n","Loading ImageNet-pretrained resnet_50\n","INFO:tensorflow:Restoring parameters from /content/drive/My Drive/Development/DeepLabCut/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt\n","Max_iters overwritten as 300000\n","Display_iters overwritten as 50\n","Save_iters overwritten as 10000\n","Training parameter:\n","{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'weigh_only_present_joints': False, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': '/content/drive/My Drive/Development/DeepLabCut/dev/possum101_11Apr-Phil-2020-04-13-diff/dlc-models/iteration-5/possum101_11AprApr13-trainset95shuffle1/train/snapshot', 'log_dir': 'log', 'global_scale': 1.0, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'mirror': False, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 4, 'dataset_type': 'imgaug', 'deterministic': False, 'crop': True, 'cropratio': 0.4, 'minsize': 100, 'leftwidth': 400, 'rightwidth': 400, 'topheight': 400, 'bottomheight': 400, 'all_joints': [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43]], 'all_joints_names': ['Body_ds1_crn_cam1', 'Body_ds1_crn_cam2', 'Body_ds2_int_cam1', 'Body_ds2_int_cam2', 'Body_ds3_cdl_cam1', 'Body_ds3_cdl_cam2', 'Body_vn1_crn_cam1', 'Body_vn1_crn_cam2', 'Body_vn2_int_cam1', 'Body_vn2_int_cam2', 'Body_vn3_cdl_cam1', 'Body_vn3_cdl_cam2', 'Scapula_acr_cam1', 'Scapula_acr_cam2', 'Scapula_spi_cam1', 'Scapula_spi_cam2', 'Scapula_vtb_cam1', 'Scapula_vtb_cam2', 'Humerus_dpc_cam1', 'Humerus_dpc_cam2', 'Humerus_ent_cam1', 'Humerus_ent_cam2', 'Humerus_ect_cam1', 'Humerus_ect_cam2', 'Ulna_olc_cam1', 'Ulna_olc_cam2', 'Ulna_int_cam1', 'Ulna_int_cam2', 'Ulna_dst_cam1', 'Ulna_dst_cam2', 'Radius_prx_cam1', 'Radius_prx_cam2', 'Radius_int_cam1', 'Radius_int_cam2', 'Radius_dst_cam1', 'Radius_dst_cam2', 'Teres_maj_prx_cam1', 'Teres_maj_prx_cam2', 'Teres_maj_dst_cam1', 'Teres_maj_dst_cam2', 'Biceps_prx_cam1', 'Biceps_prx_cam2', 'Biceps_dst_cam1', 'Biceps_dst_cam2'], 'dataset': 'training-datasets/iteration-5/UnaugmentedDataSet_possum101_11AprApr13/possum101_11Apr_Phil95shuffle1.mat', 'display_iters': 1000, 'init_weights': '/content/drive/My Drive/Development/DeepLabCut/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt', 'max_input_size': 1500, 'metadataset': 'training-datasets/iteration-5/UnaugmentedDataSet_possum101_11AprApr13/Documentation_data-possum101_11Apr_95shuffle1.pickle', 'min_input_size': 64, 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'net_type': 'resnet_50', 'num_joints': 44, 'pos_dist_thresh': 17, 'project_path': '/content/drive/My Drive/Development/DeepLabCut/dev/possum101_11Apr-Phil-2020-04-13-diff', 'save_iters': 50000, 'scale_jitter_lo': 1.0, 'scale_jitter_up': 1.0, 'output_stride': 16, 'deconvolutionstride': 2}\n","Starting training....\n"],"name":"stdout"},{"output_type":"stream","text":["iteration: 50 loss: 0.2071 lr: 0.005\n","iteration: 100 loss: 0.0360 lr: 0.005\n","iteration: 150 loss: 0.0347 lr: 0.005\n","iteration: 200 loss: 0.0343 lr: 0.005\n","iteration: 250 loss: 0.0341 lr: 0.005\n","iteration: 300 loss: 0.0352 lr: 0.005\n","iteration: 350 loss: 0.0318 lr: 0.005\n","iteration: 400 loss: 0.0325 lr: 0.005\n","iteration: 450 loss: 0.0312 lr: 0.005\n","iteration: 500 loss: 0.0321 lr: 0.005\n","iteration: 550 loss: 0.0313 lr: 0.005\n","iteration: 600 loss: 0.0314 lr: 0.005\n","iteration: 650 loss: 0.0319 lr: 0.005\n","iteration: 700 loss: 0.0306 lr: 0.005\n","iteration: 750 loss: 0.0280 lr: 0.005\n","iteration: 800 loss: 0.0294 lr: 0.005\n","iteration: 850 loss: 0.0289 lr: 0.005\n","iteration: 900 loss: 0.0276 lr: 0.005\n","iteration: 950 loss: 0.0279 lr: 0.005\n","iteration: 1000 loss: 0.0272 lr: 0.005\n","iteration: 1050 loss: 0.0264 lr: 0.005\n","iteration: 1100 loss: 0.0252 lr: 0.005\n","iteration: 1150 loss: 0.0256 lr: 0.005\n","iteration: 1200 loss: 0.0239 lr: 0.005\n","iteration: 1250 loss: 0.0238 lr: 0.005\n","iteration: 1300 loss: 0.0240 lr: 0.005\n","iteration: 1350 loss: 0.0230 lr: 0.005\n","iteration: 1400 loss: 0.0218 lr: 0.005\n","iteration: 1450 loss: 0.0223 lr: 0.005\n","iteration: 1500 loss: 0.0223 lr: 0.005\n","iteration: 1550 loss: 0.0211 lr: 0.005\n","iteration: 1600 loss: 0.0200 lr: 0.005\n","iteration: 1650 loss: 0.0206 lr: 0.005\n","iteration: 1700 loss: 0.0200 lr: 0.005\n","iteration: 1750 loss: 0.0197 lr: 0.005\n","iteration: 1800 loss: 0.0194 lr: 0.005\n","iteration: 1850 loss: 0.0190 lr: 0.005\n","iteration: 1900 loss: 0.0188 lr: 0.005\n","iteration: 1950 loss: 0.0183 lr: 0.005\n","iteration: 2000 loss: 0.0173 lr: 0.005\n","iteration: 2050 loss: 0.0175 lr: 0.005\n","iteration: 2100 loss: 0.0175 lr: 0.005\n","iteration: 2150 loss: 0.0165 lr: 0.005\n","iteration: 2200 loss: 0.0172 lr: 0.005\n","iteration: 2250 loss: 0.0167 lr: 0.005\n","iteration: 2300 loss: 0.0160 lr: 0.005\n","iteration: 2350 loss: 0.0156 lr: 0.005\n","iteration: 2400 loss: 0.0161 lr: 0.005\n","iteration: 2450 loss: 0.0156 lr: 0.005\n","iteration: 2500 loss: 0.0162 lr: 0.005\n","iteration: 2550 loss: 0.0153 lr: 0.005\n","iteration: 2600 loss: 0.0153 lr: 0.005\n","iteration: 2650 loss: 0.0151 lr: 0.005\n","iteration: 2700 loss: 0.0144 lr: 0.005\n","iteration: 2750 loss: 0.0143 lr: 0.005\n","iteration: 2800 loss: 0.0143 lr: 0.005\n","iteration: 2850 loss: 0.0143 lr: 0.005\n","iteration: 2900 loss: 0.0136 lr: 0.005\n","iteration: 2950 loss: 0.0146 lr: 0.005\n","iteration: 3000 loss: 0.0129 lr: 0.005\n","iteration: 3050 loss: 0.0138 lr: 0.005\n","iteration: 3100 loss: 0.0130 lr: 0.005\n","iteration: 3150 loss: 0.0135 lr: 0.005\n","iteration: 3200 loss: 0.0136 lr: 0.005\n","iteration: 3250 loss: 0.0129 lr: 0.005\n","iteration: 3300 loss: 0.0131 lr: 0.005\n","iteration: 3350 loss: 0.0131 lr: 0.005\n","iteration: 3400 loss: 0.0128 lr: 0.005\n","iteration: 3450 loss: 0.0123 lr: 0.005\n","iteration: 3500 loss: 0.0124 lr: 0.005\n","iteration: 3550 loss: 0.0123 lr: 0.005\n","iteration: 3600 loss: 0.0124 lr: 0.005\n","iteration: 3650 loss: 0.0121 lr: 0.005\n","iteration: 3700 loss: 0.0119 lr: 0.005\n","iteration: 3750 loss: 0.0114 lr: 0.005\n","iteration: 3800 loss: 0.0117 lr: 0.005\n","iteration: 3850 loss: 0.0115 lr: 0.005\n","iteration: 3900 loss: 0.0112 lr: 0.005\n","iteration: 3950 loss: 0.0118 lr: 0.005\n","iteration: 4000 loss: 0.0114 lr: 0.005\n","iteration: 4050 loss: 0.0117 lr: 0.005\n","iteration: 4100 loss: 0.0111 lr: 0.005\n","iteration: 4150 loss: 0.0117 lr: 0.005\n","iteration: 4200 loss: 0.0110 lr: 0.005\n","iteration: 4250 loss: 0.0108 lr: 0.005\n","iteration: 4300 loss: 0.0109 lr: 0.005\n","iteration: 4350 loss: 0.0110 lr: 0.005\n","iteration: 4400 loss: 0.0110 lr: 0.005\n","iteration: 4450 loss: 0.0110 lr: 0.005\n","iteration: 4500 loss: 0.0104 lr: 0.005\n","iteration: 4550 loss: 0.0107 lr: 0.005\n","iteration: 4600 loss: 0.0105 lr: 0.005\n","iteration: 4650 loss: 0.0102 lr: 0.005\n","iteration: 4700 loss: 0.0102 lr: 0.005\n","iteration: 4750 loss: 0.0104 lr: 0.005\n","iteration: 4800 loss: 0.0101 lr: 0.005\n","iteration: 4850 loss: 0.0104 lr: 0.005\n","iteration: 4900 loss: 0.0098 lr: 0.005\n","iteration: 4950 loss: 0.0096 lr: 0.005\n","iteration: 5000 loss: 0.0099 lr: 0.005\n","iteration: 5050 loss: 0.0098 lr: 0.005\n","iteration: 5100 loss: 0.0101 lr: 0.005\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"2iDe5Jr9ZRuV","colab_type":"text"},"source":["## Round 2: Import outliers and retrain"]},{"cell_type":"code","metadata":{"id":"qHl3jQZ102M9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1596638932332,"user_tz":240,"elapsed":62851259,"user":{"displayName":"Phil Fahn-Lai","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgaLRzhwWuE2_ZNOkUUkb6lkk-noGiIsb7ucOzW=s64","userId":"13631433551768862211"}},"outputId":"4e2808ee-f1e4-42c6-ed80-fc8d2dd901a7"},"source":["# model.importXma('31Jul20_16h08m34s','./dev/possum101_11Apr-Phil-2020-04-13-diff/xma/outliers1.csv',outlier_mode=True)\n","# model.dlc.check_labels(model.yaml)\n","\n","trainposeconfigfile,testposeconfigfile,snapshotfolder=model.dlc.return_train_network_path(config_path,1,0.95)\n","cfg_dlc=model.dlc.auxiliaryfunctions.read_plainconfig(trainposeconfigfile)\n","# cfg_dlc['augmentationprobability']=0.25\n","cfg_dlc['batch_size']=4\n","# cfg_dlc['hist_eq']=True\n","# cfg_dlc['gamma']=False\n","# cfg_dlc['logcontrast']=False\n","# cfg_dlc['allchannelsclahe']=True\n","cfg_dlc['optimizer'] =\"sgd\"\n","cfg_dlc['dataset_type']='imgaug'\n","# cfg_dlc['multi_step']=[[1e-4, 7500], [5*1e-5, 12000], [1e-5, 50000], [5e-6, 200000]]\n","# cfg_dlc['global_scale']=1.0\n","cfg_dlc['scale_jitter_lo'] = 1.0\n","cfg_dlc['scale_jitter_up'] = 1.0\n","cfg_dlc['global_scale'] = 1.0\n","\n","\n","# cfg_dlc['init_weights']= '/content/drive/My Drive/Development/DeepLabCut/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt'\n","# cfg_dlc['init_weights']=model.getLatestSnapshot(snapshotfolder) # to resume from latest snapshot\n","\n","model.dlc.auxiliaryfunctions.write_plainconfig(trainposeconfigfile,cfg_dlc)\n","\n","\n","model.dlc.train_network(config_path, saveiters=10000,displayiters=50,maxiters=300000,max_snapshots_to_keep=15, allow_growth=True)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Config:\n","{'all_joints': [[0],\n","                [1],\n","                [2],\n","                [3],\n","                [4],\n","                [5],\n","                [6],\n","                [7],\n","                [8],\n","                [9],\n","                [10],\n","                [11],\n","                [12],\n","                [13],\n","                [14],\n","                [15],\n","                [16],\n","                [17],\n","                [18],\n","                [19],\n","                [20],\n","                [21],\n","                [22],\n","                [23],\n","                [24],\n","                [25],\n","                [26],\n","                [27],\n","                [28],\n","                [29],\n","                [30],\n","                [31],\n","                [32],\n","                [33],\n","                [34],\n","                [35],\n","                [36],\n","                [37],\n","                [38],\n","                [39],\n","                [40],\n","                [41],\n","                [42],\n","                [43]],\n"," 'all_joints_names': ['Body_ds1_crn_cam1',\n","                      'Body_ds1_crn_cam2',\n","                      'Body_ds2_int_cam1',\n","                      'Body_ds2_int_cam2',\n","                      'Body_ds3_cdl_cam1',\n","                      'Body_ds3_cdl_cam2',\n","                      'Body_vn1_crn_cam1',\n","                      'Body_vn1_crn_cam2',\n","                      'Body_vn2_int_cam1',\n","                      'Body_vn2_int_cam2',\n","                      'Body_vn3_cdl_cam1',\n","                      'Body_vn3_cdl_cam2',\n","                      'Scapula_acr_cam1',\n","                      'Scapula_acr_cam2',\n","                      'Scapula_spi_cam1',\n","                      'Scapula_spi_cam2',\n","                      'Scapula_vtb_cam1',\n","                      'Scapula_vtb_cam2',\n","                      'Humerus_dpc_cam1',\n","                      'Humerus_dpc_cam2',\n","                      'Humerus_ent_cam1',\n","                      'Humerus_ent_cam2',\n","                      'Humerus_ect_cam1',\n","                      'Humerus_ect_cam2',\n","                      'Ulna_olc_cam1',\n","                      'Ulna_olc_cam2',\n","                      'Ulna_int_cam1',\n","                      'Ulna_int_cam2',\n","                      'Ulna_dst_cam1',\n","                      'Ulna_dst_cam2',\n","                      'Radius_prx_cam1',\n","                      'Radius_prx_cam2',\n","                      'Radius_int_cam1',\n","                      'Radius_int_cam2',\n","                      'Radius_dst_cam1',\n","                      'Radius_dst_cam2',\n","                      'Teres_maj_prx_cam1',\n","                      'Teres_maj_prx_cam2',\n","                      'Teres_maj_dst_cam1',\n","                      'Teres_maj_dst_cam2',\n","                      'Biceps_prx_cam1',\n","                      'Biceps_prx_cam2',\n","                      'Biceps_dst_cam1',\n","                      'Biceps_dst_cam2'],\n"," 'batch_size': 4,\n"," 'bottomheight': 400,\n"," 'crop': True,\n"," 'crop_pad': 0,\n"," 'cropratio': 0.4,\n"," 'dataset': 'training-datasets/iteration-4/UnaugmentedDataSet_possum101_11AprApr13/possum101_11Apr_Phil95shuffle1.mat',\n"," 'dataset_type': 'imgaug',\n"," 'deterministic': False,\n"," 'display_iters': 1000,\n"," 'fg_fraction': 0.25,\n"," 'global_scale': 0.8,\n"," 'init_weights': '/content/drive/My '\n","                 'Drive/Development/DeepLabCut/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',\n"," 'intermediate_supervision': False,\n"," 'intermediate_supervision_layer': 12,\n"," 'leftwidth': 400,\n"," 'location_refinement': True,\n"," 'locref_huber_loss': True,\n"," 'locref_loss_weight': 0.05,\n"," 'locref_stdev': 7.2801,\n"," 'log_dir': 'log',\n"," 'max_input_size': 1500,\n"," 'mean_pixel': [123.68, 116.779, 103.939],\n"," 'metadataset': 'training-datasets/iteration-4/UnaugmentedDataSet_possum101_11AprApr13/Documentation_data-possum101_11Apr_95shuffle1.pickle',\n"," 'min_input_size': 64,\n"," 'minsize': 100,\n"," 'mirror': False,\n"," 'multi_step': [[0.005, 10000],\n","                [0.02, 430000],\n","                [0.002, 730000],\n","                [0.001, 1030000]],\n"," 'net_type': 'resnet_50',\n"," 'num_joints': 44,\n"," 'optimizer': 'sgd',\n"," 'pos_dist_thresh': 17,\n"," 'project_path': '/content/drive/My '\n","                 'Drive/Development/DeepLabCut/dev/possum101_11Apr-Phil-2020-04-13-diff',\n"," 'regularize': False,\n"," 'rightwidth': 400,\n"," 'save_iters': 50000,\n"," 'scale_jitter_lo': 0.5,\n"," 'scale_jitter_up': 1.25,\n"," 'scoremap_dir': 'test',\n"," 'shuffle': True,\n"," 'snapshot_prefix': '/content/drive/My '\n","                    'Drive/Development/DeepLabCut/dev/possum101_11Apr-Phil-2020-04-13-diff/dlc-models/iteration-4/possum101_11AprApr13-trainset95shuffle1/train/snapshot',\n"," 'stride': 8.0,\n"," 'topheight': 400,\n"," 'weigh_negatives': False,\n"," 'weigh_only_present_joints': False,\n"," 'weigh_part_predictions': False,\n"," 'weight_decay': 0.0001}\n"],"name":"stderr"},{"output_type":"stream","text":["Starting with imgaug pose-dataset loader.\n","Batch Size is 4\n","Initializing ResNet\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","WARNING:tensorflow:From /content/drive/My Drive/Development/DeepLabCut/deeplabcut/pose_estimation_tensorflow/nnet/pose_net.py:62: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Development/DeepLabCut/deeplabcut/pose_estimation_tensorflow/nnet/pose_net.py:160: The name tf.losses.sigmoid_cross_entropy is deprecated. Please use tf.compat.v1.losses.sigmoid_cross_entropy instead.\n","\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /content/drive/My Drive/Development/DeepLabCut/deeplabcut/pose_estimation_tensorflow/nnet/losses.py:38: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","Loading ImageNet-pretrained resnet_50\n","WARNING:tensorflow:From /content/drive/My Drive/Development/DeepLabCut/deeplabcut/pose_estimation_tensorflow/train.py:143: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","INFO:tensorflow:Restoring parameters from /content/drive/My Drive/Development/DeepLabCut/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt\n","Max_iters overwritten as 300000\n","Display_iters overwritten as 50\n","Save_iters overwritten as 10000\n","Training parameter:\n","{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'weigh_only_present_joints': False, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': '/content/drive/My Drive/Development/DeepLabCut/dev/possum101_11Apr-Phil-2020-04-13-diff/dlc-models/iteration-4/possum101_11AprApr13-trainset95shuffle1/train/snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'mirror': False, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 4, 'dataset_type': 'imgaug', 'deterministic': False, 'crop': True, 'cropratio': 0.4, 'minsize': 100, 'leftwidth': 400, 'rightwidth': 400, 'topheight': 400, 'bottomheight': 400, 'all_joints': [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43]], 'all_joints_names': ['Body_ds1_crn_cam1', 'Body_ds1_crn_cam2', 'Body_ds2_int_cam1', 'Body_ds2_int_cam2', 'Body_ds3_cdl_cam1', 'Body_ds3_cdl_cam2', 'Body_vn1_crn_cam1', 'Body_vn1_crn_cam2', 'Body_vn2_int_cam1', 'Body_vn2_int_cam2', 'Body_vn3_cdl_cam1', 'Body_vn3_cdl_cam2', 'Scapula_acr_cam1', 'Scapula_acr_cam2', 'Scapula_spi_cam1', 'Scapula_spi_cam2', 'Scapula_vtb_cam1', 'Scapula_vtb_cam2', 'Humerus_dpc_cam1', 'Humerus_dpc_cam2', 'Humerus_ent_cam1', 'Humerus_ent_cam2', 'Humerus_ect_cam1', 'Humerus_ect_cam2', 'Ulna_olc_cam1', 'Ulna_olc_cam2', 'Ulna_int_cam1', 'Ulna_int_cam2', 'Ulna_dst_cam1', 'Ulna_dst_cam2', 'Radius_prx_cam1', 'Radius_prx_cam2', 'Radius_int_cam1', 'Radius_int_cam2', 'Radius_dst_cam1', 'Radius_dst_cam2', 'Teres_maj_prx_cam1', 'Teres_maj_prx_cam2', 'Teres_maj_dst_cam1', 'Teres_maj_dst_cam2', 'Biceps_prx_cam1', 'Biceps_prx_cam2', 'Biceps_dst_cam1', 'Biceps_dst_cam2'], 'dataset': 'training-datasets/iteration-4/UnaugmentedDataSet_possum101_11AprApr13/possum101_11Apr_Phil95shuffle1.mat', 'display_iters': 1000, 'init_weights': '/content/drive/My Drive/Development/DeepLabCut/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt', 'max_input_size': 1500, 'metadataset': 'training-datasets/iteration-4/UnaugmentedDataSet_possum101_11AprApr13/Documentation_data-possum101_11Apr_95shuffle1.pickle', 'min_input_size': 64, 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'net_type': 'resnet_50', 'num_joints': 44, 'pos_dist_thresh': 17, 'project_path': '/content/drive/My Drive/Development/DeepLabCut/dev/possum101_11Apr-Phil-2020-04-13-diff', 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'output_stride': 16, 'deconvolutionstride': 2}\n","Starting training....\n"],"name":"stdout"},{"output_type":"stream","text":["iteration: 50 loss: 0.1876 lr: 0.005\n","iteration: 100 loss: 0.0265 lr: 0.005\n","iteration: 150 loss: 0.0276 lr: 0.005\n","iteration: 200 loss: 0.0267 lr: 0.005\n","iteration: 250 loss: 0.0285 lr: 0.005\n","iteration: 300 loss: 0.0274 lr: 0.005\n","iteration: 350 loss: 0.0267 lr: 0.005\n","iteration: 400 loss: 0.0248 lr: 0.005\n","iteration: 450 loss: 0.0263 lr: 0.005\n","iteration: 500 loss: 0.0260 lr: 0.005\n","iteration: 550 loss: 0.0233 lr: 0.005\n","iteration: 600 loss: 0.0222 lr: 0.005\n","iteration: 650 loss: 0.0245 lr: 0.005\n","iteration: 700 loss: 0.0262 lr: 0.005\n","iteration: 750 loss: 0.0250 lr: 0.005\n","iteration: 800 loss: 0.0229 lr: 0.005\n","iteration: 850 loss: 0.0226 lr: 0.005\n","iteration: 900 loss: 0.0229 lr: 0.005\n","iteration: 950 loss: 0.0249 lr: 0.005\n","iteration: 1000 loss: 0.0231 lr: 0.005\n","iteration: 1050 loss: 0.0205 lr: 0.005\n","iteration: 1100 loss: 0.0231 lr: 0.005\n","iteration: 1150 loss: 0.0233 lr: 0.005\n","iteration: 1200 loss: 0.0234 lr: 0.005\n","iteration: 1250 loss: 0.0220 lr: 0.005\n","iteration: 1300 loss: 0.0216 lr: 0.005\n","iteration: 1350 loss: 0.0215 lr: 0.005\n","iteration: 1400 loss: 0.0196 lr: 0.005\n","iteration: 1450 loss: 0.0201 lr: 0.005\n","iteration: 1500 loss: 0.0205 lr: 0.005\n","iteration: 1550 loss: 0.0191 lr: 0.005\n","iteration: 1600 loss: 0.0173 lr: 0.005\n","iteration: 1650 loss: 0.0186 lr: 0.005\n","iteration: 1700 loss: 0.0173 lr: 0.005\n","iteration: 1750 loss: 0.0195 lr: 0.005\n","iteration: 1800 loss: 0.0174 lr: 0.005\n","iteration: 1850 loss: 0.0153 lr: 0.005\n","iteration: 1900 loss: 0.0185 lr: 0.005\n","iteration: 1950 loss: 0.0174 lr: 0.005\n","iteration: 2000 loss: 0.0177 lr: 0.005\n","iteration: 2050 loss: 0.0159 lr: 0.005\n","iteration: 2100 loss: 0.0170 lr: 0.005\n","iteration: 2150 loss: 0.0152 lr: 0.005\n","iteration: 2200 loss: 0.0152 lr: 0.005\n","iteration: 2250 loss: 0.0166 lr: 0.005\n","iteration: 2300 loss: 0.0160 lr: 0.005\n","iteration: 2350 loss: 0.0152 lr: 0.005\n","iteration: 2400 loss: 0.0162 lr: 0.005\n","iteration: 2450 loss: 0.0153 lr: 0.005\n","iteration: 2500 loss: 0.0154 lr: 0.005\n","iteration: 2550 loss: 0.0147 lr: 0.005\n","iteration: 2600 loss: 0.0153 lr: 0.005\n","iteration: 2650 loss: 0.0153 lr: 0.005\n","iteration: 2700 loss: 0.0153 lr: 0.005\n","iteration: 2750 loss: 0.0143 lr: 0.005\n","iteration: 2800 loss: 0.0142 lr: 0.005\n","iteration: 2850 loss: 0.0134 lr: 0.005\n","iteration: 2900 loss: 0.0139 lr: 0.005\n","iteration: 2950 loss: 0.0135 lr: 0.005\n","iteration: 3000 loss: 0.0141 lr: 0.005\n","iteration: 3050 loss: 0.0136 lr: 0.005\n","iteration: 3100 loss: 0.0127 lr: 0.005\n","iteration: 3150 loss: 0.0130 lr: 0.005\n","iteration: 3200 loss: 0.0123 lr: 0.005\n","iteration: 3250 loss: 0.0124 lr: 0.005\n","iteration: 3300 loss: 0.0128 lr: 0.005\n","iteration: 3350 loss: 0.0127 lr: 0.005\n","iteration: 3400 loss: 0.0128 lr: 0.005\n","iteration: 3450 loss: 0.0123 lr: 0.005\n","iteration: 3500 loss: 0.0135 lr: 0.005\n","iteration: 3550 loss: 0.0126 lr: 0.005\n","iteration: 3600 loss: 0.0124 lr: 0.005\n","iteration: 3650 loss: 0.0120 lr: 0.005\n","iteration: 3700 loss: 0.0124 lr: 0.005\n","iteration: 3750 loss: 0.0114 lr: 0.005\n","iteration: 3800 loss: 0.0127 lr: 0.005\n","iteration: 3850 loss: 0.0123 lr: 0.005\n","iteration: 3900 loss: 0.0124 lr: 0.005\n","iteration: 3950 loss: 0.0119 lr: 0.005\n","iteration: 4000 loss: 0.0119 lr: 0.005\n","iteration: 4050 loss: 0.0120 lr: 0.005\n","iteration: 4100 loss: 0.0119 lr: 0.005\n","iteration: 4150 loss: 0.0113 lr: 0.005\n","iteration: 4200 loss: 0.0112 lr: 0.005\n","iteration: 4250 loss: 0.0114 lr: 0.005\n","iteration: 4300 loss: 0.0118 lr: 0.005\n","iteration: 4350 loss: 0.0113 lr: 0.005\n","iteration: 4400 loss: 0.0115 lr: 0.005\n","iteration: 4450 loss: 0.0111 lr: 0.005\n","iteration: 4500 loss: 0.0110 lr: 0.005\n","iteration: 4550 loss: 0.0114 lr: 0.005\n","iteration: 4600 loss: 0.0107 lr: 0.005\n","iteration: 4650 loss: 0.0109 lr: 0.005\n","iteration: 4700 loss: 0.0110 lr: 0.005\n","iteration: 4750 loss: 0.0115 lr: 0.005\n","iteration: 4800 loss: 0.0109 lr: 0.005\n","iteration: 4850 loss: 0.0112 lr: 0.005\n","iteration: 4900 loss: 0.0108 lr: 0.005\n","iteration: 4950 loss: 0.0108 lr: 0.005\n","iteration: 5000 loss: 0.0106 lr: 0.005\n","iteration: 5050 loss: 0.0109 lr: 0.005\n","iteration: 5100 loss: 0.0106 lr: 0.005\n","iteration: 5150 loss: 0.0108 lr: 0.005\n","iteration: 5200 loss: 0.0113 lr: 0.005\n","iteration: 5250 loss: 0.0110 lr: 0.005\n","iteration: 5300 loss: 0.0105 lr: 0.005\n","iteration: 5350 loss: 0.0106 lr: 0.005\n","iteration: 5400 loss: 0.0102 lr: 0.005\n","iteration: 5450 loss: 0.0103 lr: 0.005\n","iteration: 5500 loss: 0.0101 lr: 0.005\n","iteration: 5550 loss: 0.0098 lr: 0.005\n","iteration: 5600 loss: 0.0102 lr: 0.005\n","iteration: 5650 loss: 0.0107 lr: 0.005\n","iteration: 5700 loss: 0.0103 lr: 0.005\n","iteration: 5750 loss: 0.0103 lr: 0.005\n","iteration: 5800 loss: 0.0102 lr: 0.005\n","iteration: 5850 loss: 0.0104 lr: 0.005\n","iteration: 5900 loss: 0.0101 lr: 0.005\n","iteration: 5950 loss: 0.0099 lr: 0.005\n","iteration: 6000 loss: 0.0100 lr: 0.005\n","iteration: 6050 loss: 0.0099 lr: 0.005\n","iteration: 6100 loss: 0.0098 lr: 0.005\n","iteration: 6150 loss: 0.0095 lr: 0.005\n","iteration: 6200 loss: 0.0099 lr: 0.005\n","iteration: 6250 loss: 0.0103 lr: 0.005\n","iteration: 6300 loss: 0.0095 lr: 0.005\n","iteration: 6350 loss: 0.0101 lr: 0.005\n","iteration: 6400 loss: 0.0096 lr: 0.005\n","iteration: 6450 loss: 0.0096 lr: 0.005\n","iteration: 6500 loss: 0.0104 lr: 0.005\n","iteration: 6550 loss: 0.0097 lr: 0.005\n","iteration: 6600 loss: 0.0096 lr: 0.005\n","iteration: 6650 loss: 0.0097 lr: 0.005\n","iteration: 6700 loss: 0.0097 lr: 0.005\n","iteration: 6750 loss: 0.0096 lr: 0.005\n","iteration: 6800 loss: 0.0095 lr: 0.005\n","iteration: 6850 loss: 0.0097 lr: 0.005\n","iteration: 6900 loss: 0.0093 lr: 0.005\n","iteration: 6950 loss: 0.0092 lr: 0.005\n","iteration: 7000 loss: 0.0098 lr: 0.005\n","iteration: 7050 loss: 0.0093 lr: 0.005\n","iteration: 7100 loss: 0.0094 lr: 0.005\n","iteration: 7150 loss: 0.0092 lr: 0.005\n","iteration: 7200 loss: 0.0090 lr: 0.005\n","iteration: 7250 loss: 0.0091 lr: 0.005\n","iteration: 7300 loss: 0.0093 lr: 0.005\n","iteration: 7350 loss: 0.0091 lr: 0.005\n","iteration: 7400 loss: 0.0092 lr: 0.005\n","iteration: 7450 loss: 0.0092 lr: 0.005\n","iteration: 7500 loss: 0.0092 lr: 0.005\n","iteration: 7550 loss: 0.0090 lr: 0.005\n","iteration: 7600 loss: 0.0093 lr: 0.005\n","iteration: 7650 loss: 0.0089 lr: 0.005\n","iteration: 7700 loss: 0.0089 lr: 0.005\n","iteration: 7750 loss: 0.0089 lr: 0.005\n","iteration: 7800 loss: 0.0088 lr: 0.005\n","iteration: 7850 loss: 0.0090 lr: 0.005\n","iteration: 7900 loss: 0.0092 lr: 0.005\n","iteration: 7950 loss: 0.0089 lr: 0.005\n","iteration: 8000 loss: 0.0087 lr: 0.005\n","iteration: 8050 loss: 0.0089 lr: 0.005\n","iteration: 8100 loss: 0.0090 lr: 0.005\n","iteration: 8150 loss: 0.0089 lr: 0.005\n","iteration: 8200 loss: 0.0089 lr: 0.005\n","iteration: 8250 loss: 0.0091 lr: 0.005\n","iteration: 8300 loss: 0.0092 lr: 0.005\n","iteration: 8350 loss: 0.0088 lr: 0.005\n","iteration: 8400 loss: 0.0086 lr: 0.005\n","iteration: 8450 loss: 0.0087 lr: 0.005\n","iteration: 8500 loss: 0.0088 lr: 0.005\n","iteration: 8550 loss: 0.0085 lr: 0.005\n","iteration: 8600 loss: 0.0088 lr: 0.005\n","iteration: 8650 loss: 0.0089 lr: 0.005\n","iteration: 8700 loss: 0.0087 lr: 0.005\n","iteration: 8750 loss: 0.0089 lr: 0.005\n","iteration: 8800 loss: 0.0087 lr: 0.005\n","iteration: 8850 loss: 0.0088 lr: 0.005\n","iteration: 8900 loss: 0.0086 lr: 0.005\n","iteration: 8950 loss: 0.0088 lr: 0.005\n","iteration: 9000 loss: 0.0086 lr: 0.005\n","iteration: 9050 loss: 0.0089 lr: 0.005\n","iteration: 9100 loss: 0.0090 lr: 0.005\n","iteration: 9150 loss: 0.0084 lr: 0.005\n","iteration: 9200 loss: 0.0082 lr: 0.005\n","iteration: 9250 loss: 0.0083 lr: 0.005\n","iteration: 9300 loss: 0.0086 lr: 0.005\n","iteration: 9350 loss: 0.0080 lr: 0.005\n","iteration: 9400 loss: 0.0086 lr: 0.005\n","iteration: 9450 loss: 0.0082 lr: 0.005\n","iteration: 9500 loss: 0.0084 lr: 0.005\n","iteration: 9550 loss: 0.0083 lr: 0.005\n","iteration: 9600 loss: 0.0082 lr: 0.005\n","iteration: 9650 loss: 0.0085 lr: 0.005\n","iteration: 9700 loss: 0.0087 lr: 0.005\n","iteration: 9750 loss: 0.0080 lr: 0.005\n","iteration: 9800 loss: 0.0085 lr: 0.005\n","iteration: 9850 loss: 0.0084 lr: 0.005\n","iteration: 9900 loss: 0.0080 lr: 0.005\n","iteration: 9950 loss: 0.0081 lr: 0.005\n","iteration: 10000 loss: 0.0083 lr: 0.005\n","iteration: 10050 loss: 0.0093 lr: 0.02\n","iteration: 10100 loss: 0.0089 lr: 0.02\n","iteration: 10150 loss: 0.0088 lr: 0.02\n","iteration: 10200 loss: 0.0088 lr: 0.02\n","iteration: 10250 loss: 0.0092 lr: 0.02\n","iteration: 10300 loss: 0.0089 lr: 0.02\n","iteration: 10350 loss: 0.0085 lr: 0.02\n","iteration: 10400 loss: 0.0087 lr: 0.02\n","iteration: 10450 loss: 0.0086 lr: 0.02\n","iteration: 10500 loss: 0.0087 lr: 0.02\n","iteration: 10550 loss: 0.0082 lr: 0.02\n","iteration: 10600 loss: 0.0081 lr: 0.02\n","iteration: 10650 loss: 0.0086 lr: 0.02\n","iteration: 10700 loss: 0.0084 lr: 0.02\n","iteration: 10750 loss: 0.0086 lr: 0.02\n","iteration: 10800 loss: 0.0086 lr: 0.02\n","iteration: 10850 loss: 0.0082 lr: 0.02\n","iteration: 10900 loss: 0.0083 lr: 0.02\n","iteration: 10950 loss: 0.0079 lr: 0.02\n","iteration: 11000 loss: 0.0081 lr: 0.02\n","iteration: 11050 loss: 0.0079 lr: 0.02\n","iteration: 11100 loss: 0.0081 lr: 0.02\n","iteration: 11150 loss: 0.0079 lr: 0.02\n","iteration: 11200 loss: 0.0077 lr: 0.02\n","iteration: 11250 loss: 0.0076 lr: 0.02\n","iteration: 11300 loss: 0.0075 lr: 0.02\n","iteration: 11350 loss: 0.0079 lr: 0.02\n","iteration: 11400 loss: 0.0076 lr: 0.02\n","iteration: 11450 loss: 0.0079 lr: 0.02\n","iteration: 11500 loss: 0.0073 lr: 0.02\n","iteration: 11550 loss: 0.0077 lr: 0.02\n","iteration: 11600 loss: 0.0076 lr: 0.02\n","iteration: 11650 loss: 0.0074 lr: 0.02\n","iteration: 11700 loss: 0.0072 lr: 0.02\n","iteration: 11750 loss: 0.0075 lr: 0.02\n","iteration: 11800 loss: 0.0077 lr: 0.02\n","iteration: 11850 loss: 0.0078 lr: 0.02\n","iteration: 11900 loss: 0.0071 lr: 0.02\n","iteration: 11950 loss: 0.0070 lr: 0.02\n","iteration: 12000 loss: 0.0073 lr: 0.02\n","iteration: 12050 loss: 0.0073 lr: 0.02\n","iteration: 12100 loss: 0.0070 lr: 0.02\n","iteration: 12150 loss: 0.0073 lr: 0.02\n","iteration: 12200 loss: 0.0066 lr: 0.02\n","iteration: 12250 loss: 0.0071 lr: 0.02\n","iteration: 12300 loss: 0.0067 lr: 0.02\n","iteration: 12350 loss: 0.0068 lr: 0.02\n","iteration: 12400 loss: 0.0068 lr: 0.02\n","iteration: 12450 loss: 0.0067 lr: 0.02\n","iteration: 12500 loss: 0.0066 lr: 0.02\n","iteration: 12550 loss: 0.0069 lr: 0.02\n","iteration: 12600 loss: 0.0068 lr: 0.02\n","iteration: 12650 loss: 0.0069 lr: 0.02\n","iteration: 12700 loss: 0.0066 lr: 0.02\n","iteration: 12750 loss: 0.0067 lr: 0.02\n","iteration: 12800 loss: 0.0066 lr: 0.02\n","iteration: 12850 loss: 0.0066 lr: 0.02\n","iteration: 12900 loss: 0.0066 lr: 0.02\n","iteration: 12950 loss: 0.0066 lr: 0.02\n","iteration: 13000 loss: 0.0064 lr: 0.02\n","iteration: 13050 loss: 0.0065 lr: 0.02\n","iteration: 13100 loss: 0.0067 lr: 0.02\n","iteration: 13150 loss: 0.0065 lr: 0.02\n","iteration: 13200 loss: 0.0065 lr: 0.02\n","iteration: 13250 loss: 0.0064 lr: 0.02\n","iteration: 13300 loss: 0.0065 lr: 0.02\n","iteration: 13350 loss: 0.0061 lr: 0.02\n","iteration: 13400 loss: 0.0063 lr: 0.02\n","iteration: 13450 loss: 0.0062 lr: 0.02\n","iteration: 13500 loss: 0.0064 lr: 0.02\n","iteration: 13550 loss: 0.0065 lr: 0.02\n","iteration: 13600 loss: 0.0062 lr: 0.02\n","iteration: 13650 loss: 0.0062 lr: 0.02\n","iteration: 13700 loss: 0.0062 lr: 0.02\n","iteration: 13750 loss: 0.0063 lr: 0.02\n","iteration: 13800 loss: 0.0060 lr: 0.02\n","iteration: 13850 loss: 0.0064 lr: 0.02\n","iteration: 13900 loss: 0.0062 lr: 0.02\n","iteration: 13950 loss: 0.0063 lr: 0.02\n","iteration: 14000 loss: 0.0060 lr: 0.02\n","iteration: 14050 loss: 0.0063 lr: 0.02\n","iteration: 14100 loss: 0.0062 lr: 0.02\n","iteration: 14150 loss: 0.0059 lr: 0.02\n","iteration: 14200 loss: 0.0061 lr: 0.02\n","iteration: 14250 loss: 0.0063 lr: 0.02\n","iteration: 14300 loss: 0.0058 lr: 0.02\n","iteration: 14350 loss: 0.0060 lr: 0.02\n","iteration: 14400 loss: 0.0058 lr: 0.02\n","iteration: 14450 loss: 0.0058 lr: 0.02\n","iteration: 14500 loss: 0.0062 lr: 0.02\n","iteration: 14550 loss: 0.0061 lr: 0.02\n","iteration: 14600 loss: 0.0060 lr: 0.02\n","iteration: 14650 loss: 0.0060 lr: 0.02\n","iteration: 14700 loss: 0.0058 lr: 0.02\n","iteration: 14750 loss: 0.0057 lr: 0.02\n","iteration: 14800 loss: 0.0057 lr: 0.02\n","iteration: 14850 loss: 0.0058 lr: 0.02\n","iteration: 14900 loss: 0.0060 lr: 0.02\n","iteration: 14950 loss: 0.0057 lr: 0.02\n","iteration: 15000 loss: 0.0058 lr: 0.02\n","iteration: 15050 loss: 0.0057 lr: 0.02\n","iteration: 15100 loss: 0.0057 lr: 0.02\n","iteration: 15150 loss: 0.0055 lr: 0.02\n","iteration: 15200 loss: 0.0057 lr: 0.02\n","iteration: 15250 loss: 0.0057 lr: 0.02\n","iteration: 15300 loss: 0.0055 lr: 0.02\n","iteration: 15350 loss: 0.0057 lr: 0.02\n","iteration: 15400 loss: 0.0054 lr: 0.02\n","iteration: 15450 loss: 0.0054 lr: 0.02\n","iteration: 15500 loss: 0.0057 lr: 0.02\n","iteration: 15550 loss: 0.0055 lr: 0.02\n","iteration: 15600 loss: 0.0058 lr: 0.02\n","iteration: 15650 loss: 0.0058 lr: 0.02\n","iteration: 15700 loss: 0.0054 lr: 0.02\n","iteration: 15750 loss: 0.0053 lr: 0.02\n","iteration: 15800 loss: 0.0056 lr: 0.02\n","iteration: 15850 loss: 0.0055 lr: 0.02\n","iteration: 15900 loss: 0.0054 lr: 0.02\n","iteration: 15950 loss: 0.0056 lr: 0.02\n","iteration: 16000 loss: 0.0055 lr: 0.02\n","iteration: 16050 loss: 0.0055 lr: 0.02\n","iteration: 16100 loss: 0.0054 lr: 0.02\n","iteration: 16150 loss: 0.0055 lr: 0.02\n","iteration: 16200 loss: 0.0052 lr: 0.02\n","iteration: 16250 loss: 0.0054 lr: 0.02\n","iteration: 16300 loss: 0.0053 lr: 0.02\n","iteration: 16350 loss: 0.0057 lr: 0.02\n","iteration: 16400 loss: 0.0054 lr: 0.02\n","iteration: 16450 loss: 0.0053 lr: 0.02\n","iteration: 16500 loss: 0.0054 lr: 0.02\n","iteration: 16550 loss: 0.0052 lr: 0.02\n","iteration: 16600 loss: 0.0054 lr: 0.02\n","iteration: 16650 loss: 0.0053 lr: 0.02\n","iteration: 16700 loss: 0.0051 lr: 0.02\n","iteration: 16750 loss: 0.0053 lr: 0.02\n","iteration: 16800 loss: 0.0052 lr: 0.02\n","iteration: 16850 loss: 0.0052 lr: 0.02\n","iteration: 16900 loss: 0.0054 lr: 0.02\n","iteration: 16950 loss: 0.0055 lr: 0.02\n","iteration: 17000 loss: 0.0051 lr: 0.02\n","iteration: 17050 loss: 0.0051 lr: 0.02\n","iteration: 17100 loss: 0.0054 lr: 0.02\n","iteration: 17150 loss: 0.0054 lr: 0.02\n","iteration: 17200 loss: 0.0053 lr: 0.02\n","iteration: 17250 loss: 0.0055 lr: 0.02\n","iteration: 17300 loss: 0.0053 lr: 0.02\n","iteration: 17350 loss: 0.0049 lr: 0.02\n","iteration: 17400 loss: 0.0051 lr: 0.02\n","iteration: 17450 loss: 0.0050 lr: 0.02\n","iteration: 17500 loss: 0.0051 lr: 0.02\n","iteration: 17550 loss: 0.0050 lr: 0.02\n","iteration: 17600 loss: 0.0052 lr: 0.02\n","iteration: 17650 loss: 0.0050 lr: 0.02\n","iteration: 17700 loss: 0.0051 lr: 0.02\n","iteration: 17750 loss: 0.0051 lr: 0.02\n","iteration: 17800 loss: 0.0050 lr: 0.02\n","iteration: 17850 loss: 0.0050 lr: 0.02\n","iteration: 17900 loss: 0.0050 lr: 0.02\n","iteration: 17950 loss: 0.0049 lr: 0.02\n","iteration: 18000 loss: 0.0051 lr: 0.02\n","iteration: 18050 loss: 0.0048 lr: 0.02\n","iteration: 18100 loss: 0.0049 lr: 0.02\n","iteration: 18150 loss: 0.0052 lr: 0.02\n","iteration: 18200 loss: 0.0050 lr: 0.02\n","iteration: 18250 loss: 0.0049 lr: 0.02\n","iteration: 18300 loss: 0.0046 lr: 0.02\n","iteration: 18350 loss: 0.0048 lr: 0.02\n","iteration: 18400 loss: 0.0048 lr: 0.02\n","iteration: 18450 loss: 0.0051 lr: 0.02\n","iteration: 18500 loss: 0.0048 lr: 0.02\n","iteration: 18550 loss: 0.0047 lr: 0.02\n","iteration: 18600 loss: 0.0048 lr: 0.02\n","iteration: 18650 loss: 0.0047 lr: 0.02\n","iteration: 18700 loss: 0.0047 lr: 0.02\n","iteration: 18750 loss: 0.0048 lr: 0.02\n","iteration: 18800 loss: 0.0047 lr: 0.02\n","iteration: 18850 loss: 0.0046 lr: 0.02\n","iteration: 18900 loss: 0.0050 lr: 0.02\n","iteration: 18950 loss: 0.0048 lr: 0.02\n","iteration: 19000 loss: 0.0045 lr: 0.02\n","iteration: 19050 loss: 0.0047 lr: 0.02\n","iteration: 19100 loss: 0.0046 lr: 0.02\n","iteration: 19150 loss: 0.0047 lr: 0.02\n","iteration: 19200 loss: 0.0045 lr: 0.02\n","iteration: 19250 loss: 0.0046 lr: 0.02\n","iteration: 19300 loss: 0.0045 lr: 0.02\n","iteration: 19350 loss: 0.0046 lr: 0.02\n","iteration: 19400 loss: 0.0046 lr: 0.02\n","iteration: 19450 loss: 0.0045 lr: 0.02\n","iteration: 19500 loss: 0.0045 lr: 0.02\n","iteration: 19550 loss: 0.0045 lr: 0.02\n","iteration: 19600 loss: 0.0043 lr: 0.02\n","iteration: 19650 loss: 0.0047 lr: 0.02\n","iteration: 19700 loss: 0.0044 lr: 0.02\n","iteration: 19750 loss: 0.0045 lr: 0.02\n","iteration: 19800 loss: 0.0047 lr: 0.02\n","iteration: 19850 loss: 0.0046 lr: 0.02\n","iteration: 19900 loss: 0.0046 lr: 0.02\n","iteration: 19950 loss: 0.0046 lr: 0.02\n","iteration: 20000 loss: 0.0047 lr: 0.02\n","iteration: 20050 loss: 0.0046 lr: 0.02\n","iteration: 20100 loss: 0.0044 lr: 0.02\n","iteration: 20150 loss: 0.0046 lr: 0.02\n","iteration: 20200 loss: 0.0046 lr: 0.02\n","iteration: 20250 loss: 0.0045 lr: 0.02\n","iteration: 20300 loss: 0.0044 lr: 0.02\n","iteration: 20350 loss: 0.0045 lr: 0.02\n","iteration: 20400 loss: 0.0045 lr: 0.02\n","iteration: 20450 loss: 0.0043 lr: 0.02\n","iteration: 20500 loss: 0.0046 lr: 0.02\n","iteration: 20550 loss: 0.0045 lr: 0.02\n","iteration: 20600 loss: 0.0046 lr: 0.02\n","iteration: 20650 loss: 0.0044 lr: 0.02\n","iteration: 20700 loss: 0.0043 lr: 0.02\n","iteration: 20750 loss: 0.0045 lr: 0.02\n","iteration: 20800 loss: 0.0043 lr: 0.02\n","iteration: 20850 loss: 0.0045 lr: 0.02\n","iteration: 20900 loss: 0.0046 lr: 0.02\n","iteration: 20950 loss: 0.0042 lr: 0.02\n","iteration: 21000 loss: 0.0042 lr: 0.02\n","iteration: 21050 loss: 0.0043 lr: 0.02\n","iteration: 21100 loss: 0.0043 lr: 0.02\n","iteration: 21150 loss: 0.0045 lr: 0.02\n","iteration: 21200 loss: 0.0046 lr: 0.02\n","iteration: 21250 loss: 0.0042 lr: 0.02\n","iteration: 21300 loss: 0.0044 lr: 0.02\n","iteration: 21350 loss: 0.0045 lr: 0.02\n","iteration: 21400 loss: 0.0044 lr: 0.02\n","iteration: 21450 loss: 0.0043 lr: 0.02\n","iteration: 21500 loss: 0.0042 lr: 0.02\n","iteration: 21550 loss: 0.0044 lr: 0.02\n","iteration: 21600 loss: 0.0042 lr: 0.02\n","iteration: 21650 loss: 0.0044 lr: 0.02\n","iteration: 21700 loss: 0.0043 lr: 0.02\n","iteration: 21750 loss: 0.0043 lr: 0.02\n","iteration: 21800 loss: 0.0043 lr: 0.02\n","iteration: 21850 loss: 0.0043 lr: 0.02\n","iteration: 21900 loss: 0.0044 lr: 0.02\n","iteration: 21950 loss: 0.0043 lr: 0.02\n","iteration: 22000 loss: 0.0044 lr: 0.02\n","iteration: 22050 loss: 0.0044 lr: 0.02\n","iteration: 22100 loss: 0.0043 lr: 0.02\n","iteration: 22150 loss: 0.0039 lr: 0.02\n","iteration: 22200 loss: 0.0044 lr: 0.02\n","iteration: 22250 loss: 0.0040 lr: 0.02\n","iteration: 22300 loss: 0.0041 lr: 0.02\n","iteration: 22350 loss: 0.0041 lr: 0.02\n","iteration: 22400 loss: 0.0042 lr: 0.02\n","iteration: 22450 loss: 0.0042 lr: 0.02\n","iteration: 22500 loss: 0.0040 lr: 0.02\n","iteration: 22550 loss: 0.0043 lr: 0.02\n","iteration: 22600 loss: 0.0042 lr: 0.02\n","iteration: 22650 loss: 0.0041 lr: 0.02\n","iteration: 22700 loss: 0.0042 lr: 0.02\n","iteration: 22750 loss: 0.0041 lr: 0.02\n","iteration: 22800 loss: 0.0043 lr: 0.02\n","iteration: 22850 loss: 0.0044 lr: 0.02\n","iteration: 22900 loss: 0.0041 lr: 0.02\n","iteration: 22950 loss: 0.0039 lr: 0.02\n","iteration: 23000 loss: 0.0039 lr: 0.02\n","iteration: 23050 loss: 0.0040 lr: 0.02\n","iteration: 23100 loss: 0.0039 lr: 0.02\n","iteration: 23150 loss: 0.0039 lr: 0.02\n","iteration: 23200 loss: 0.0041 lr: 0.02\n","iteration: 23250 loss: 0.0040 lr: 0.02\n","iteration: 23300 loss: 0.0043 lr: 0.02\n","iteration: 23350 loss: 0.0041 lr: 0.02\n","iteration: 23400 loss: 0.0039 lr: 0.02\n","iteration: 23450 loss: 0.0040 lr: 0.02\n","iteration: 23500 loss: 0.0040 lr: 0.02\n","iteration: 23550 loss: 0.0040 lr: 0.02\n","iteration: 23600 loss: 0.0040 lr: 0.02\n","iteration: 23650 loss: 0.0040 lr: 0.02\n","iteration: 23700 loss: 0.0038 lr: 0.02\n","iteration: 23750 loss: 0.0039 lr: 0.02\n","iteration: 23800 loss: 0.0040 lr: 0.02\n","iteration: 23850 loss: 0.0037 lr: 0.02\n","iteration: 23900 loss: 0.0039 lr: 0.02\n","iteration: 23950 loss: 0.0039 lr: 0.02\n","iteration: 24000 loss: 0.0039 lr: 0.02\n","iteration: 24050 loss: 0.0040 lr: 0.02\n","iteration: 24100 loss: 0.0039 lr: 0.02\n","iteration: 24150 loss: 0.0039 lr: 0.02\n","iteration: 24200 loss: 0.0041 lr: 0.02\n","iteration: 24250 loss: 0.0041 lr: 0.02\n","iteration: 24300 loss: 0.0039 lr: 0.02\n","iteration: 24350 loss: 0.0038 lr: 0.02\n","iteration: 24400 loss: 0.0037 lr: 0.02\n","iteration: 24450 loss: 0.0038 lr: 0.02\n","iteration: 24500 loss: 0.0039 lr: 0.02\n","iteration: 24550 loss: 0.0039 lr: 0.02\n","iteration: 24600 loss: 0.0038 lr: 0.02\n","iteration: 24650 loss: 0.0039 lr: 0.02\n","iteration: 24700 loss: 0.0037 lr: 0.02\n","iteration: 24750 loss: 0.0040 lr: 0.02\n","iteration: 24800 loss: 0.0036 lr: 0.02\n","iteration: 24850 loss: 0.0039 lr: 0.02\n","iteration: 24900 loss: 0.0040 lr: 0.02\n","iteration: 24950 loss: 0.0040 lr: 0.02\n","iteration: 25000 loss: 0.0038 lr: 0.02\n","iteration: 25050 loss: 0.0038 lr: 0.02\n","iteration: 25100 loss: 0.0037 lr: 0.02\n","iteration: 25150 loss: 0.0040 lr: 0.02\n","iteration: 25200 loss: 0.0037 lr: 0.02\n","iteration: 25250 loss: 0.0038 lr: 0.02\n","iteration: 25300 loss: 0.0037 lr: 0.02\n","iteration: 25350 loss: 0.0039 lr: 0.02\n","iteration: 25400 loss: 0.0039 lr: 0.02\n","iteration: 25450 loss: 0.0040 lr: 0.02\n","iteration: 25500 loss: 0.0040 lr: 0.02\n","iteration: 25550 loss: 0.0039 lr: 0.02\n","iteration: 25600 loss: 0.0039 lr: 0.02\n","iteration: 25650 loss: 0.0038 lr: 0.02\n","iteration: 25700 loss: 0.0039 lr: 0.02\n","iteration: 25750 loss: 0.0038 lr: 0.02\n","iteration: 25800 loss: 0.0038 lr: 0.02\n","iteration: 25850 loss: 0.0039 lr: 0.02\n","iteration: 25900 loss: 0.0038 lr: 0.02\n","iteration: 25950 loss: 0.0037 lr: 0.02\n","iteration: 26000 loss: 0.0037 lr: 0.02\n","iteration: 26050 loss: 0.0034 lr: 0.02\n","iteration: 26100 loss: 0.0037 lr: 0.02\n","iteration: 26150 loss: 0.0038 lr: 0.02\n","iteration: 26200 loss: 0.0036 lr: 0.02\n","iteration: 26250 loss: 0.0037 lr: 0.02\n","iteration: 26300 loss: 0.0037 lr: 0.02\n","iteration: 26350 loss: 0.0036 lr: 0.02\n","iteration: 26400 loss: 0.0038 lr: 0.02\n","iteration: 26450 loss: 0.0038 lr: 0.02\n","iteration: 26500 loss: 0.0038 lr: 0.02\n","iteration: 26550 loss: 0.0038 lr: 0.02\n","iteration: 26600 loss: 0.0037 lr: 0.02\n","iteration: 26650 loss: 0.0037 lr: 0.02\n","iteration: 26700 loss: 0.0036 lr: 0.02\n","iteration: 26750 loss: 0.0038 lr: 0.02\n","iteration: 26800 loss: 0.0035 lr: 0.02\n","iteration: 26850 loss: 0.0037 lr: 0.02\n","iteration: 26900 loss: 0.0037 lr: 0.02\n","iteration: 26950 loss: 0.0035 lr: 0.02\n","iteration: 27000 loss: 0.0035 lr: 0.02\n","iteration: 27050 loss: 0.0036 lr: 0.02\n","iteration: 27100 loss: 0.0039 lr: 0.02\n","iteration: 27150 loss: 0.0036 lr: 0.02\n","iteration: 27200 loss: 0.0037 lr: 0.02\n","iteration: 27250 loss: 0.0037 lr: 0.02\n","iteration: 27300 loss: 0.0037 lr: 0.02\n","iteration: 27350 loss: 0.0037 lr: 0.02\n","iteration: 27400 loss: 0.0037 lr: 0.02\n","iteration: 27450 loss: 0.0036 lr: 0.02\n","iteration: 27500 loss: 0.0037 lr: 0.02\n","iteration: 27550 loss: 0.0036 lr: 0.02\n","iteration: 27600 loss: 0.0035 lr: 0.02\n","iteration: 27650 loss: 0.0036 lr: 0.02\n","iteration: 27700 loss: 0.0036 lr: 0.02\n","iteration: 27750 loss: 0.0034 lr: 0.02\n","iteration: 27800 loss: 0.0036 lr: 0.02\n","iteration: 27850 loss: 0.0036 lr: 0.02\n","iteration: 27900 loss: 0.0036 lr: 0.02\n","iteration: 27950 loss: 0.0036 lr: 0.02\n","iteration: 28000 loss: 0.0038 lr: 0.02\n","iteration: 28050 loss: 0.0035 lr: 0.02\n","iteration: 28100 loss: 0.0036 lr: 0.02\n","iteration: 28150 loss: 0.0035 lr: 0.02\n","iteration: 28200 loss: 0.0035 lr: 0.02\n","iteration: 28250 loss: 0.0035 lr: 0.02\n","iteration: 28300 loss: 0.0036 lr: 0.02\n","iteration: 28350 loss: 0.0037 lr: 0.02\n","iteration: 28400 loss: 0.0035 lr: 0.02\n","iteration: 28450 loss: 0.0036 lr: 0.02\n","iteration: 28500 loss: 0.0035 lr: 0.02\n","iteration: 28550 loss: 0.0036 lr: 0.02\n","iteration: 28600 loss: 0.0036 lr: 0.02\n","iteration: 28650 loss: 0.0032 lr: 0.02\n","iteration: 28700 loss: 0.0034 lr: 0.02\n","iteration: 28750 loss: 0.0034 lr: 0.02\n","iteration: 28800 loss: 0.0034 lr: 0.02\n","iteration: 28850 loss: 0.0034 lr: 0.02\n","iteration: 28900 loss: 0.0034 lr: 0.02\n","iteration: 28950 loss: 0.0033 lr: 0.02\n","iteration: 29000 loss: 0.0035 lr: 0.02\n","iteration: 29050 loss: 0.0033 lr: 0.02\n","iteration: 29100 loss: 0.0034 lr: 0.02\n","iteration: 29150 loss: 0.0035 lr: 0.02\n","iteration: 29200 loss: 0.0035 lr: 0.02\n","iteration: 29250 loss: 0.0036 lr: 0.02\n","iteration: 29300 loss: 0.0036 lr: 0.02\n","iteration: 29350 loss: 0.0035 lr: 0.02\n","iteration: 29400 loss: 0.0034 lr: 0.02\n","iteration: 29450 loss: 0.0033 lr: 0.02\n","iteration: 29500 loss: 0.0034 lr: 0.02\n","iteration: 29550 loss: 0.0036 lr: 0.02\n","iteration: 29600 loss: 0.0034 lr: 0.02\n","iteration: 29650 loss: 0.0034 lr: 0.02\n","iteration: 29700 loss: 0.0033 lr: 0.02\n","iteration: 29750 loss: 0.0035 lr: 0.02\n","iteration: 29800 loss: 0.0034 lr: 0.02\n","iteration: 29850 loss: 0.0034 lr: 0.02\n","iteration: 29900 loss: 0.0033 lr: 0.02\n","iteration: 29950 loss: 0.0033 lr: 0.02\n","iteration: 30000 loss: 0.0035 lr: 0.02\n","iteration: 30050 loss: 0.0033 lr: 0.02\n","iteration: 30100 loss: 0.0035 lr: 0.02\n","iteration: 30150 loss: 0.0035 lr: 0.02\n","iteration: 30200 loss: 0.0034 lr: 0.02\n","iteration: 30250 loss: 0.0034 lr: 0.02\n","iteration: 30300 loss: 0.0033 lr: 0.02\n","iteration: 30350 loss: 0.0032 lr: 0.02\n","iteration: 30400 loss: 0.0032 lr: 0.02\n","iteration: 30450 loss: 0.0034 lr: 0.02\n","iteration: 30500 loss: 0.0034 lr: 0.02\n","iteration: 30550 loss: 0.0034 lr: 0.02\n","iteration: 30600 loss: 0.0034 lr: 0.02\n","iteration: 30650 loss: 0.0032 lr: 0.02\n","iteration: 30700 loss: 0.0032 lr: 0.02\n","iteration: 30750 loss: 0.0035 lr: 0.02\n","iteration: 30800 loss: 0.0035 lr: 0.02\n","iteration: 30850 loss: 0.0033 lr: 0.02\n","iteration: 30900 loss: 0.0033 lr: 0.02\n","iteration: 30950 loss: 0.0033 lr: 0.02\n","iteration: 31000 loss: 0.0033 lr: 0.02\n","iteration: 31050 loss: 0.0033 lr: 0.02\n","iteration: 31100 loss: 0.0032 lr: 0.02\n","iteration: 31150 loss: 0.0034 lr: 0.02\n","iteration: 31200 loss: 0.0033 lr: 0.02\n","iteration: 31250 loss: 0.0034 lr: 0.02\n","iteration: 31300 loss: 0.0033 lr: 0.02\n","iteration: 31350 loss: 0.0033 lr: 0.02\n","iteration: 31400 loss: 0.0032 lr: 0.02\n","iteration: 31450 loss: 0.0033 lr: 0.02\n","iteration: 31500 loss: 0.0032 lr: 0.02\n","iteration: 31550 loss: 0.0032 lr: 0.02\n","iteration: 31600 loss: 0.0033 lr: 0.02\n","iteration: 31650 loss: 0.0033 lr: 0.02\n","iteration: 31700 loss: 0.0033 lr: 0.02\n","iteration: 31750 loss: 0.0032 lr: 0.02\n","iteration: 31800 loss: 0.0034 lr: 0.02\n","iteration: 31850 loss: 0.0032 lr: 0.02\n","iteration: 31900 loss: 0.0034 lr: 0.02\n","iteration: 31950 loss: 0.0032 lr: 0.02\n","iteration: 32000 loss: 0.0030 lr: 0.02\n","iteration: 32050 loss: 0.0037 lr: 0.02\n","iteration: 32100 loss: 0.0035 lr: 0.02\n","iteration: 32150 loss: 0.0032 lr: 0.02\n","iteration: 32200 loss: 0.0032 lr: 0.02\n","iteration: 32250 loss: 0.0032 lr: 0.02\n","iteration: 32300 loss: 0.0033 lr: 0.02\n","iteration: 32350 loss: 0.0033 lr: 0.02\n","iteration: 32400 loss: 0.0031 lr: 0.02\n","iteration: 32450 loss: 0.0034 lr: 0.02\n","iteration: 32500 loss: 0.0033 lr: 0.02\n","iteration: 32550 loss: 0.0033 lr: 0.02\n","iteration: 32600 loss: 0.0033 lr: 0.02\n","iteration: 32650 loss: 0.0031 lr: 0.02\n","iteration: 32700 loss: 0.0032 lr: 0.02\n","iteration: 32750 loss: 0.0032 lr: 0.02\n","iteration: 32800 loss: 0.0033 lr: 0.02\n","iteration: 32850 loss: 0.0031 lr: 0.02\n","iteration: 32900 loss: 0.0031 lr: 0.02\n","iteration: 32950 loss: 0.0030 lr: 0.02\n","iteration: 33000 loss: 0.0032 lr: 0.02\n","iteration: 33050 loss: 0.0031 lr: 0.02\n","iteration: 33100 loss: 0.0030 lr: 0.02\n","iteration: 33150 loss: 0.0032 lr: 0.02\n","iteration: 33200 loss: 0.0032 lr: 0.02\n","iteration: 33250 loss: 0.0031 lr: 0.02\n","iteration: 33300 loss: 0.0033 lr: 0.02\n","iteration: 33350 loss: 0.0031 lr: 0.02\n","iteration: 33400 loss: 0.0029 lr: 0.02\n","iteration: 33450 loss: 0.0033 lr: 0.02\n","iteration: 33500 loss: 0.0032 lr: 0.02\n","iteration: 33550 loss: 0.0030 lr: 0.02\n","iteration: 33600 loss: 0.0032 lr: 0.02\n","iteration: 33650 loss: 0.0031 lr: 0.02\n","iteration: 33700 loss: 0.0031 lr: 0.02\n","iteration: 33750 loss: 0.0032 lr: 0.02\n","iteration: 33800 loss: 0.0030 lr: 0.02\n","iteration: 33850 loss: 0.0029 lr: 0.02\n","iteration: 33900 loss: 0.0032 lr: 0.02\n","iteration: 33950 loss: 0.0030 lr: 0.02\n","iteration: 34000 loss: 0.0030 lr: 0.02\n","iteration: 34050 loss: 0.0030 lr: 0.02\n","iteration: 34100 loss: 0.0032 lr: 0.02\n","iteration: 34150 loss: 0.0030 lr: 0.02\n","iteration: 34200 loss: 0.0033 lr: 0.02\n","iteration: 34250 loss: 0.0030 lr: 0.02\n","iteration: 34300 loss: 0.0030 lr: 0.02\n","iteration: 34350 loss: 0.0030 lr: 0.02\n","iteration: 34400 loss: 0.0031 lr: 0.02\n","iteration: 34450 loss: 0.0033 lr: 0.02\n","iteration: 34500 loss: 0.0031 lr: 0.02\n","iteration: 34550 loss: 0.0032 lr: 0.02\n","iteration: 34600 loss: 0.0032 lr: 0.02\n","iteration: 34650 loss: 0.0034 lr: 0.02\n","iteration: 34700 loss: 0.0032 lr: 0.02\n","iteration: 34750 loss: 0.0031 lr: 0.02\n","iteration: 34800 loss: 0.0030 lr: 0.02\n","iteration: 34850 loss: 0.0029 lr: 0.02\n","iteration: 34900 loss: 0.0030 lr: 0.02\n","iteration: 34950 loss: 0.0031 lr: 0.02\n","iteration: 35000 loss: 0.0032 lr: 0.02\n","iteration: 35050 loss: 0.0031 lr: 0.02\n","iteration: 35100 loss: 0.0030 lr: 0.02\n","iteration: 35150 loss: 0.0031 lr: 0.02\n","iteration: 35200 loss: 0.0031 lr: 0.02\n","iteration: 35250 loss: 0.0031 lr: 0.02\n","iteration: 35300 loss: 0.0029 lr: 0.02\n","iteration: 35350 loss: 0.0029 lr: 0.02\n","iteration: 35400 loss: 0.0030 lr: 0.02\n","iteration: 35450 loss: 0.0029 lr: 0.02\n","iteration: 35500 loss: 0.0030 lr: 0.02\n","iteration: 35550 loss: 0.0030 lr: 0.02\n","iteration: 35600 loss: 0.0030 lr: 0.02\n","iteration: 35650 loss: 0.0030 lr: 0.02\n","iteration: 35700 loss: 0.0031 lr: 0.02\n","iteration: 35750 loss: 0.0032 lr: 0.02\n","iteration: 35800 loss: 0.0031 lr: 0.02\n","iteration: 35850 loss: 0.0030 lr: 0.02\n","iteration: 35900 loss: 0.0030 lr: 0.02\n","iteration: 35950 loss: 0.0029 lr: 0.02\n","iteration: 36000 loss: 0.0028 lr: 0.02\n","iteration: 36050 loss: 0.0028 lr: 0.02\n","iteration: 36100 loss: 0.0029 lr: 0.02\n","iteration: 36150 loss: 0.0029 lr: 0.02\n","iteration: 36200 loss: 0.0030 lr: 0.02\n","iteration: 36250 loss: 0.0028 lr: 0.02\n","iteration: 36300 loss: 0.0029 lr: 0.02\n","iteration: 36350 loss: 0.0029 lr: 0.02\n","iteration: 36400 loss: 0.0030 lr: 0.02\n","iteration: 36450 loss: 0.0031 lr: 0.02\n","iteration: 36500 loss: 0.0028 lr: 0.02\n","iteration: 36550 loss: 0.0031 lr: 0.02\n","iteration: 36600 loss: 0.0030 lr: 0.02\n","iteration: 36650 loss: 0.0031 lr: 0.02\n","iteration: 36700 loss: 0.0030 lr: 0.02\n","iteration: 36750 loss: 0.0029 lr: 0.02\n","iteration: 36800 loss: 0.0029 lr: 0.02\n","iteration: 36850 loss: 0.0032 lr: 0.02\n","iteration: 36900 loss: 0.0029 lr: 0.02\n","iteration: 36950 loss: 0.0032 lr: 0.02\n","iteration: 37000 loss: 0.0030 lr: 0.02\n","iteration: 37050 loss: 0.0029 lr: 0.02\n","iteration: 37100 loss: 0.0029 lr: 0.02\n","iteration: 37150 loss: 0.0031 lr: 0.02\n","iteration: 37200 loss: 0.0029 lr: 0.02\n","iteration: 37250 loss: 0.0029 lr: 0.02\n","iteration: 37300 loss: 0.0028 lr: 0.02\n","iteration: 37350 loss: 0.0030 lr: 0.02\n","iteration: 37400 loss: 0.0029 lr: 0.02\n","iteration: 37450 loss: 0.0029 lr: 0.02\n","iteration: 37500 loss: 0.0030 lr: 0.02\n","iteration: 37550 loss: 0.0031 lr: 0.02\n","iteration: 37600 loss: 0.0030 lr: 0.02\n","iteration: 37650 loss: 0.0029 lr: 0.02\n","iteration: 37700 loss: 0.0029 lr: 0.02\n","iteration: 37750 loss: 0.0029 lr: 0.02\n","iteration: 37800 loss: 0.0028 lr: 0.02\n","iteration: 37850 loss: 0.0029 lr: 0.02\n","iteration: 37900 loss: 0.0029 lr: 0.02\n","iteration: 37950 loss: 0.0027 lr: 0.02\n","iteration: 38000 loss: 0.0030 lr: 0.02\n","iteration: 38050 loss: 0.0027 lr: 0.02\n","iteration: 38100 loss: 0.0029 lr: 0.02\n","iteration: 38150 loss: 0.0028 lr: 0.02\n","iteration: 38200 loss: 0.0029 lr: 0.02\n","iteration: 38250 loss: 0.0030 lr: 0.02\n","iteration: 38300 loss: 0.0029 lr: 0.02\n","iteration: 38350 loss: 0.0028 lr: 0.02\n","iteration: 38400 loss: 0.0027 lr: 0.02\n","iteration: 38450 loss: 0.0030 lr: 0.02\n","iteration: 38500 loss: 0.0028 lr: 0.02\n","iteration: 38550 loss: 0.0028 lr: 0.02\n","iteration: 38600 loss: 0.0030 lr: 0.02\n","iteration: 38650 loss: 0.0029 lr: 0.02\n","iteration: 38700 loss: 0.0027 lr: 0.02\n","iteration: 38750 loss: 0.0027 lr: 0.02\n","iteration: 38800 loss: 0.0029 lr: 0.02\n","iteration: 38850 loss: 0.0030 lr: 0.02\n","iteration: 38900 loss: 0.0030 lr: 0.02\n","iteration: 38950 loss: 0.0027 lr: 0.02\n","iteration: 39000 loss: 0.0027 lr: 0.02\n","iteration: 39050 loss: 0.0028 lr: 0.02\n","iteration: 39100 loss: 0.0027 lr: 0.02\n","iteration: 39150 loss: 0.0028 lr: 0.02\n","iteration: 39200 loss: 0.0032 lr: 0.02\n","iteration: 39250 loss: 0.0029 lr: 0.02\n","iteration: 39300 loss: 0.0027 lr: 0.02\n","iteration: 39350 loss: 0.0032 lr: 0.02\n","iteration: 39400 loss: 0.0028 lr: 0.02\n","iteration: 39450 loss: 0.0027 lr: 0.02\n","iteration: 39500 loss: 0.0028 lr: 0.02\n","iteration: 39550 loss: 0.0027 lr: 0.02\n","iteration: 39600 loss: 0.0029 lr: 0.02\n","iteration: 39650 loss: 0.0029 lr: 0.02\n","iteration: 39700 loss: 0.0030 lr: 0.02\n","iteration: 39750 loss: 0.0029 lr: 0.02\n","iteration: 39800 loss: 0.0029 lr: 0.02\n","iteration: 39850 loss: 0.0027 lr: 0.02\n","iteration: 39900 loss: 0.0026 lr: 0.02\n","iteration: 39950 loss: 0.0028 lr: 0.02\n","iteration: 40000 loss: 0.0030 lr: 0.02\n","iteration: 40050 loss: 0.0030 lr: 0.02\n","iteration: 40100 loss: 0.0026 lr: 0.02\n","iteration: 40150 loss: 0.0028 lr: 0.02\n","iteration: 40200 loss: 0.0028 lr: 0.02\n","iteration: 40250 loss: 0.0028 lr: 0.02\n","iteration: 40300 loss: 0.0028 lr: 0.02\n","iteration: 40350 loss: 0.0028 lr: 0.02\n","iteration: 40400 loss: 0.0030 lr: 0.02\n","iteration: 40450 loss: 0.0027 lr: 0.02\n","iteration: 40500 loss: 0.0029 lr: 0.02\n","iteration: 40550 loss: 0.0028 lr: 0.02\n","iteration: 40600 loss: 0.0027 lr: 0.02\n","iteration: 40650 loss: 0.0028 lr: 0.02\n","iteration: 40700 loss: 0.0030 lr: 0.02\n","iteration: 40750 loss: 0.0027 lr: 0.02\n","iteration: 40800 loss: 0.0026 lr: 0.02\n","iteration: 40850 loss: 0.0026 lr: 0.02\n","iteration: 40900 loss: 0.0029 lr: 0.02\n","iteration: 40950 loss: 0.0027 lr: 0.02\n","iteration: 41000 loss: 0.0027 lr: 0.02\n","iteration: 41050 loss: 0.0029 lr: 0.02\n","iteration: 41100 loss: 0.0027 lr: 0.02\n","iteration: 41150 loss: 0.0026 lr: 0.02\n","iteration: 41200 loss: 0.0026 lr: 0.02\n","iteration: 41250 loss: 0.0028 lr: 0.02\n","iteration: 41300 loss: 0.0029 lr: 0.02\n","iteration: 41350 loss: 0.0029 lr: 0.02\n","iteration: 41400 loss: 0.0027 lr: 0.02\n","iteration: 41450 loss: 0.0026 lr: 0.02\n","iteration: 41500 loss: 0.0029 lr: 0.02\n","iteration: 41550 loss: 0.0026 lr: 0.02\n","iteration: 41600 loss: 0.0027 lr: 0.02\n","iteration: 41650 loss: 0.0028 lr: 0.02\n","iteration: 41700 loss: 0.0028 lr: 0.02\n","iteration: 41750 loss: 0.0026 lr: 0.02\n","iteration: 41800 loss: 0.0027 lr: 0.02\n","iteration: 41850 loss: 0.0028 lr: 0.02\n","iteration: 41900 loss: 0.0028 lr: 0.02\n","iteration: 41950 loss: 0.0026 lr: 0.02\n","iteration: 42000 loss: 0.0027 lr: 0.02\n","iteration: 42050 loss: 0.0028 lr: 0.02\n","iteration: 42100 loss: 0.0028 lr: 0.02\n","iteration: 42150 loss: 0.0027 lr: 0.02\n","iteration: 42200 loss: 0.0028 lr: 0.02\n","iteration: 42250 loss: 0.0027 lr: 0.02\n","iteration: 42300 loss: 0.0027 lr: 0.02\n","iteration: 42350 loss: 0.0027 lr: 0.02\n","iteration: 42400 loss: 0.0027 lr: 0.02\n","iteration: 42450 loss: 0.0029 lr: 0.02\n","iteration: 42500 loss: 0.0027 lr: 0.02\n","iteration: 42550 loss: 0.0030 lr: 0.02\n","iteration: 42600 loss: 0.0026 lr: 0.02\n","iteration: 42650 loss: 0.0027 lr: 0.02\n","iteration: 42700 loss: 0.0027 lr: 0.02\n","iteration: 42750 loss: 0.0027 lr: 0.02\n","iteration: 42800 loss: 0.0028 lr: 0.02\n","iteration: 42850 loss: 0.0029 lr: 0.02\n","iteration: 42900 loss: 0.0027 lr: 0.02\n","iteration: 42950 loss: 0.0027 lr: 0.02\n","iteration: 43000 loss: 0.0028 lr: 0.02\n","iteration: 43050 loss: 0.0026 lr: 0.02\n","iteration: 43100 loss: 0.0028 lr: 0.02\n","iteration: 43150 loss: 0.0029 lr: 0.02\n","iteration: 43200 loss: 0.0028 lr: 0.02\n","iteration: 43250 loss: 0.0027 lr: 0.02\n","iteration: 43300 loss: 0.0028 lr: 0.02\n","iteration: 43350 loss: 0.0026 lr: 0.02\n","iteration: 43400 loss: 0.0027 lr: 0.02\n","iteration: 43450 loss: 0.0027 lr: 0.02\n","iteration: 43500 loss: 0.0027 lr: 0.02\n","iteration: 43550 loss: 0.0029 lr: 0.02\n","iteration: 43600 loss: 0.0028 lr: 0.02\n","iteration: 43650 loss: 0.0028 lr: 0.02\n","iteration: 43700 loss: 0.0027 lr: 0.02\n","iteration: 43750 loss: 0.0027 lr: 0.02\n","iteration: 43800 loss: 0.0027 lr: 0.02\n","iteration: 43850 loss: 0.0027 lr: 0.02\n","iteration: 43900 loss: 0.0027 lr: 0.02\n","iteration: 43950 loss: 0.0027 lr: 0.02\n","iteration: 44000 loss: 0.0026 lr: 0.02\n","iteration: 44050 loss: 0.0026 lr: 0.02\n","iteration: 44100 loss: 0.0028 lr: 0.02\n","iteration: 44150 loss: 0.0028 lr: 0.02\n","iteration: 44200 loss: 0.0028 lr: 0.02\n","iteration: 44250 loss: 0.0025 lr: 0.02\n","iteration: 44300 loss: 0.0027 lr: 0.02\n","iteration: 44350 loss: 0.0028 lr: 0.02\n","iteration: 44400 loss: 0.0027 lr: 0.02\n","iteration: 44450 loss: 0.0025 lr: 0.02\n","iteration: 44500 loss: 0.0027 lr: 0.02\n","iteration: 44550 loss: 0.0026 lr: 0.02\n","iteration: 44600 loss: 0.0026 lr: 0.02\n","iteration: 44650 loss: 0.0027 lr: 0.02\n","iteration: 44700 loss: 0.0027 lr: 0.02\n","iteration: 44750 loss: 0.0026 lr: 0.02\n","iteration: 44800 loss: 0.0026 lr: 0.02\n","iteration: 44850 loss: 0.0025 lr: 0.02\n","iteration: 44900 loss: 0.0026 lr: 0.02\n","iteration: 44950 loss: 0.0027 lr: 0.02\n","iteration: 45000 loss: 0.0026 lr: 0.02\n","iteration: 45050 loss: 0.0027 lr: 0.02\n","iteration: 45100 loss: 0.0028 lr: 0.02\n","iteration: 45150 loss: 0.0026 lr: 0.02\n","iteration: 45200 loss: 0.0028 lr: 0.02\n","iteration: 45250 loss: 0.0026 lr: 0.02\n","iteration: 45300 loss: 0.0026 lr: 0.02\n","iteration: 45350 loss: 0.0027 lr: 0.02\n","iteration: 45400 loss: 0.0025 lr: 0.02\n","iteration: 45450 loss: 0.0025 lr: 0.02\n","iteration: 45500 loss: 0.0027 lr: 0.02\n","iteration: 45550 loss: 0.0024 lr: 0.02\n","iteration: 45600 loss: 0.0024 lr: 0.02\n","iteration: 45650 loss: 0.0027 lr: 0.02\n","iteration: 45700 loss: 0.0026 lr: 0.02\n","iteration: 45750 loss: 0.0025 lr: 0.02\n","iteration: 45800 loss: 0.0026 lr: 0.02\n","iteration: 45850 loss: 0.0026 lr: 0.02\n","iteration: 45900 loss: 0.0027 lr: 0.02\n","iteration: 45950 loss: 0.0026 lr: 0.02\n","iteration: 46000 loss: 0.0025 lr: 0.02\n","iteration: 46050 loss: 0.0026 lr: 0.02\n","iteration: 46100 loss: 0.0025 lr: 0.02\n","iteration: 46150 loss: 0.0027 lr: 0.02\n","iteration: 46200 loss: 0.0025 lr: 0.02\n","iteration: 46250 loss: 0.0026 lr: 0.02\n","iteration: 46300 loss: 0.0027 lr: 0.02\n","iteration: 46350 loss: 0.0026 lr: 0.02\n","iteration: 46400 loss: 0.0026 lr: 0.02\n","iteration: 46450 loss: 0.0025 lr: 0.02\n","iteration: 46500 loss: 0.0025 lr: 0.02\n","iteration: 46550 loss: 0.0027 lr: 0.02\n","iteration: 46600 loss: 0.0027 lr: 0.02\n","iteration: 46650 loss: 0.0024 lr: 0.02\n","iteration: 46700 loss: 0.0025 lr: 0.02\n","iteration: 46750 loss: 0.0026 lr: 0.02\n","iteration: 46800 loss: 0.0026 lr: 0.02\n","iteration: 46850 loss: 0.0026 lr: 0.02\n","iteration: 46900 loss: 0.0027 lr: 0.02\n","iteration: 46950 loss: 0.0026 lr: 0.02\n","iteration: 47000 loss: 0.0025 lr: 0.02\n","iteration: 47050 loss: 0.0027 lr: 0.02\n","iteration: 47100 loss: 0.0025 lr: 0.02\n","iteration: 47150 loss: 0.0026 lr: 0.02\n","iteration: 47200 loss: 0.0026 lr: 0.02\n","iteration: 47250 loss: 0.0025 lr: 0.02\n","iteration: 47300 loss: 0.0027 lr: 0.02\n","iteration: 47350 loss: 0.0025 lr: 0.02\n","iteration: 47400 loss: 0.0024 lr: 0.02\n","iteration: 47450 loss: 0.0025 lr: 0.02\n","iteration: 47500 loss: 0.0025 lr: 0.02\n","iteration: 47550 loss: 0.0025 lr: 0.02\n","iteration: 47600 loss: 0.0024 lr: 0.02\n","iteration: 47650 loss: 0.0025 lr: 0.02\n","iteration: 47700 loss: 0.0025 lr: 0.02\n","iteration: 47750 loss: 0.0025 lr: 0.02\n","iteration: 47800 loss: 0.0024 lr: 0.02\n","iteration: 47850 loss: 0.0025 lr: 0.02\n","iteration: 47900 loss: 0.0025 lr: 0.02\n","iteration: 47950 loss: 0.0025 lr: 0.02\n","iteration: 48000 loss: 0.0026 lr: 0.02\n","iteration: 48050 loss: 0.0026 lr: 0.02\n","iteration: 48100 loss: 0.0023 lr: 0.02\n","iteration: 48150 loss: 0.0026 lr: 0.02\n","iteration: 48200 loss: 0.0026 lr: 0.02\n","iteration: 48250 loss: 0.0025 lr: 0.02\n","iteration: 48300 loss: 0.0025 lr: 0.02\n","iteration: 48350 loss: 0.0027 lr: 0.02\n","iteration: 48400 loss: 0.0026 lr: 0.02\n","iteration: 48450 loss: 0.0025 lr: 0.02\n","iteration: 48500 loss: 0.0025 lr: 0.02\n","iteration: 48550 loss: 0.0024 lr: 0.02\n","iteration: 48600 loss: 0.0026 lr: 0.02\n","iteration: 48650 loss: 0.0026 lr: 0.02\n","iteration: 48700 loss: 0.0026 lr: 0.02\n","iteration: 48750 loss: 0.0024 lr: 0.02\n","iteration: 48800 loss: 0.0024 lr: 0.02\n","iteration: 48850 loss: 0.0026 lr: 0.02\n","iteration: 48900 loss: 0.0027 lr: 0.02\n","iteration: 48950 loss: 0.0025 lr: 0.02\n","iteration: 49000 loss: 0.0025 lr: 0.02\n","iteration: 49050 loss: 0.0025 lr: 0.02\n","iteration: 49100 loss: 0.0024 lr: 0.02\n","iteration: 49150 loss: 0.0024 lr: 0.02\n","iteration: 49200 loss: 0.0026 lr: 0.02\n","iteration: 49250 loss: 0.0026 lr: 0.02\n","iteration: 49300 loss: 0.0025 lr: 0.02\n","iteration: 49350 loss: 0.0024 lr: 0.02\n","iteration: 49400 loss: 0.0025 lr: 0.02\n","iteration: 49450 loss: 0.0024 lr: 0.02\n","iteration: 49500 loss: 0.0025 lr: 0.02\n","iteration: 49550 loss: 0.0025 lr: 0.02\n","iteration: 49600 loss: 0.0025 lr: 0.02\n","iteration: 49650 loss: 0.0024 lr: 0.02\n","iteration: 49700 loss: 0.0025 lr: 0.02\n","iteration: 49750 loss: 0.0025 lr: 0.02\n","iteration: 49800 loss: 0.0025 lr: 0.02\n","iteration: 49850 loss: 0.0024 lr: 0.02\n","iteration: 49900 loss: 0.0027 lr: 0.02\n","iteration: 49950 loss: 0.0025 lr: 0.02\n","iteration: 50000 loss: 0.0025 lr: 0.02\n","iteration: 50050 loss: 0.0024 lr: 0.02\n","iteration: 50100 loss: 0.0025 lr: 0.02\n","iteration: 50150 loss: 0.0024 lr: 0.02\n","iteration: 50200 loss: 0.0025 lr: 0.02\n","iteration: 50250 loss: 0.0024 lr: 0.02\n","iteration: 50300 loss: 0.0024 lr: 0.02\n","iteration: 50350 loss: 0.0024 lr: 0.02\n","iteration: 50400 loss: 0.0025 lr: 0.02\n","iteration: 50450 loss: 0.0025 lr: 0.02\n","iteration: 50500 loss: 0.0025 lr: 0.02\n","iteration: 50550 loss: 0.0026 lr: 0.02\n","iteration: 50600 loss: 0.0024 lr: 0.02\n","iteration: 50650 loss: 0.0023 lr: 0.02\n","iteration: 50700 loss: 0.0023 lr: 0.02\n","iteration: 50750 loss: 0.0026 lr: 0.02\n","iteration: 50800 loss: 0.0024 lr: 0.02\n","iteration: 50850 loss: 0.0026 lr: 0.02\n","iteration: 50900 loss: 0.0025 lr: 0.02\n","iteration: 50950 loss: 0.0023 lr: 0.02\n","iteration: 51000 loss: 0.0025 lr: 0.02\n","iteration: 51050 loss: 0.0026 lr: 0.02\n","iteration: 51100 loss: 0.0024 lr: 0.02\n","iteration: 51150 loss: 0.0023 lr: 0.02\n","iteration: 51200 loss: 0.0024 lr: 0.02\n","iteration: 51250 loss: 0.0024 lr: 0.02\n","iteration: 51300 loss: 0.0024 lr: 0.02\n","iteration: 51350 loss: 0.0024 lr: 0.02\n","iteration: 51400 loss: 0.0024 lr: 0.02\n","iteration: 51450 loss: 0.0024 lr: 0.02\n","iteration: 51500 loss: 0.0024 lr: 0.02\n","iteration: 51550 loss: 0.0026 lr: 0.02\n","iteration: 51600 loss: 0.0024 lr: 0.02\n","iteration: 51650 loss: 0.0024 lr: 0.02\n","iteration: 51700 loss: 0.0024 lr: 0.02\n","iteration: 51750 loss: 0.0025 lr: 0.02\n","iteration: 51800 loss: 0.0025 lr: 0.02\n","iteration: 51850 loss: 0.0024 lr: 0.02\n","iteration: 51900 loss: 0.0023 lr: 0.02\n","iteration: 51950 loss: 0.0026 lr: 0.02\n","iteration: 52000 loss: 0.0025 lr: 0.02\n","iteration: 52050 loss: 0.0024 lr: 0.02\n","iteration: 52100 loss: 0.0024 lr: 0.02\n","iteration: 52150 loss: 0.0024 lr: 0.02\n","iteration: 52200 loss: 0.0025 lr: 0.02\n","iteration: 52250 loss: 0.0025 lr: 0.02\n","iteration: 52300 loss: 0.0025 lr: 0.02\n","iteration: 52350 loss: 0.0026 lr: 0.02\n","iteration: 52400 loss: 0.0024 lr: 0.02\n","iteration: 52450 loss: 0.0026 lr: 0.02\n","iteration: 52500 loss: 0.0025 lr: 0.02\n","iteration: 52550 loss: 0.0023 lr: 0.02\n","iteration: 52600 loss: 0.0024 lr: 0.02\n","iteration: 52650 loss: 0.0024 lr: 0.02\n","iteration: 52700 loss: 0.0023 lr: 0.02\n","iteration: 52750 loss: 0.0025 lr: 0.02\n","iteration: 52800 loss: 0.0025 lr: 0.02\n","iteration: 52850 loss: 0.0023 lr: 0.02\n","iteration: 52900 loss: 0.0026 lr: 0.02\n","iteration: 52950 loss: 0.0023 lr: 0.02\n","iteration: 53000 loss: 0.0024 lr: 0.02\n","iteration: 53050 loss: 0.0022 lr: 0.02\n","iteration: 53100 loss: 0.0025 lr: 0.02\n","iteration: 53150 loss: 0.0023 lr: 0.02\n","iteration: 53200 loss: 0.0022 lr: 0.02\n","iteration: 53250 loss: 0.0025 lr: 0.02\n","iteration: 53300 loss: 0.0027 lr: 0.02\n","iteration: 53350 loss: 0.0024 lr: 0.02\n","iteration: 53400 loss: 0.0024 lr: 0.02\n","iteration: 53450 loss: 0.0023 lr: 0.02\n","iteration: 53500 loss: 0.0023 lr: 0.02\n","iteration: 53550 loss: 0.0024 lr: 0.02\n","iteration: 53600 loss: 0.0023 lr: 0.02\n","iteration: 53650 loss: 0.0024 lr: 0.02\n","iteration: 53700 loss: 0.0023 lr: 0.02\n","iteration: 53750 loss: 0.0024 lr: 0.02\n","iteration: 53800 loss: 0.0024 lr: 0.02\n","iteration: 53850 loss: 0.0024 lr: 0.02\n","iteration: 53900 loss: 0.0024 lr: 0.02\n","iteration: 53950 loss: 0.0023 lr: 0.02\n","iteration: 54000 loss: 0.0023 lr: 0.02\n","iteration: 54050 loss: 0.0024 lr: 0.02\n","iteration: 54100 loss: 0.0022 lr: 0.02\n","iteration: 54150 loss: 0.0022 lr: 0.02\n","iteration: 54200 loss: 0.0026 lr: 0.02\n","iteration: 54250 loss: 0.0023 lr: 0.02\n","iteration: 54300 loss: 0.0025 lr: 0.02\n","iteration: 54350 loss: 0.0024 lr: 0.02\n","iteration: 54400 loss: 0.0023 lr: 0.02\n","iteration: 54450 loss: 0.0024 lr: 0.02\n","iteration: 54500 loss: 0.0024 lr: 0.02\n","iteration: 54550 loss: 0.0023 lr: 0.02\n","iteration: 54600 loss: 0.0023 lr: 0.02\n","iteration: 54650 loss: 0.0023 lr: 0.02\n","iteration: 54700 loss: 0.0023 lr: 0.02\n","iteration: 54750 loss: 0.0024 lr: 0.02\n","iteration: 54800 loss: 0.0024 lr: 0.02\n","iteration: 54850 loss: 0.0024 lr: 0.02\n","iteration: 54900 loss: 0.0022 lr: 0.02\n","iteration: 54950 loss: 0.0024 lr: 0.02\n","iteration: 55000 loss: 0.0021 lr: 0.02\n","iteration: 55050 loss: 0.0022 lr: 0.02\n","iteration: 55100 loss: 0.0024 lr: 0.02\n","iteration: 55150 loss: 0.0025 lr: 0.02\n","iteration: 55200 loss: 0.0025 lr: 0.02\n","iteration: 55250 loss: 0.0025 lr: 0.02\n","iteration: 55300 loss: 0.0023 lr: 0.02\n","iteration: 55350 loss: 0.0023 lr: 0.02\n","iteration: 55400 loss: 0.0023 lr: 0.02\n","iteration: 55450 loss: 0.0025 lr: 0.02\n","iteration: 55500 loss: 0.0024 lr: 0.02\n","iteration: 55550 loss: 0.0022 lr: 0.02\n","iteration: 55600 loss: 0.0022 lr: 0.02\n","iteration: 55650 loss: 0.0024 lr: 0.02\n","iteration: 55700 loss: 0.0024 lr: 0.02\n","iteration: 55750 loss: 0.0026 lr: 0.02\n","iteration: 55800 loss: 0.0025 lr: 0.02\n","iteration: 55850 loss: 0.0024 lr: 0.02\n","iteration: 55900 loss: 0.0023 lr: 0.02\n","iteration: 55950 loss: 0.0024 lr: 0.02\n","iteration: 56000 loss: 0.0022 lr: 0.02\n","iteration: 56050 loss: 0.0022 lr: 0.02\n","iteration: 56100 loss: 0.0022 lr: 0.02\n","iteration: 56150 loss: 0.0023 lr: 0.02\n","iteration: 56200 loss: 0.0024 lr: 0.02\n","iteration: 56250 loss: 0.0024 lr: 0.02\n","iteration: 56300 loss: 0.0025 lr: 0.02\n","iteration: 56350 loss: 0.0023 lr: 0.02\n","iteration: 56400 loss: 0.0024 lr: 0.02\n","iteration: 56450 loss: 0.0025 lr: 0.02\n","iteration: 56500 loss: 0.0024 lr: 0.02\n","iteration: 56550 loss: 0.0024 lr: 0.02\n","iteration: 56600 loss: 0.0022 lr: 0.02\n","iteration: 56650 loss: 0.0022 lr: 0.02\n","iteration: 56700 loss: 0.0023 lr: 0.02\n","iteration: 56750 loss: 0.0023 lr: 0.02\n","iteration: 56800 loss: 0.0024 lr: 0.02\n","iteration: 56850 loss: 0.0023 lr: 0.02\n","iteration: 56900 loss: 0.0024 lr: 0.02\n","iteration: 56950 loss: 0.0022 lr: 0.02\n","iteration: 57000 loss: 0.0022 lr: 0.02\n","iteration: 57050 loss: 0.0022 lr: 0.02\n","iteration: 57100 loss: 0.0024 lr: 0.02\n","iteration: 57150 loss: 0.0023 lr: 0.02\n","iteration: 57200 loss: 0.0022 lr: 0.02\n","iteration: 57250 loss: 0.0022 lr: 0.02\n","iteration: 57300 loss: 0.0021 lr: 0.02\n","iteration: 57350 loss: 0.0022 lr: 0.02\n","iteration: 57400 loss: 0.0023 lr: 0.02\n","iteration: 57450 loss: 0.0024 lr: 0.02\n","iteration: 57500 loss: 0.0022 lr: 0.02\n","iteration: 57550 loss: 0.0024 lr: 0.02\n","iteration: 57600 loss: 0.0026 lr: 0.02\n","iteration: 57650 loss: 0.0024 lr: 0.02\n","iteration: 57700 loss: 0.0023 lr: 0.02\n","iteration: 57750 loss: 0.0024 lr: 0.02\n","iteration: 57800 loss: 0.0023 lr: 0.02\n","iteration: 57850 loss: 0.0024 lr: 0.02\n","iteration: 57900 loss: 0.0023 lr: 0.02\n","iteration: 57950 loss: 0.0022 lr: 0.02\n","iteration: 58000 loss: 0.0023 lr: 0.02\n","iteration: 58050 loss: 0.0024 lr: 0.02\n","iteration: 58100 loss: 0.0022 lr: 0.02\n","iteration: 58150 loss: 0.0022 lr: 0.02\n","iteration: 58200 loss: 0.0022 lr: 0.02\n","iteration: 58250 loss: 0.0022 lr: 0.02\n","iteration: 58300 loss: 0.0024 lr: 0.02\n","iteration: 58350 loss: 0.0022 lr: 0.02\n","iteration: 58400 loss: 0.0023 lr: 0.02\n","iteration: 58450 loss: 0.0023 lr: 0.02\n","iteration: 58500 loss: 0.0023 lr: 0.02\n","iteration: 58550 loss: 0.0024 lr: 0.02\n","iteration: 58600 loss: 0.0023 lr: 0.02\n","iteration: 58650 loss: 0.0023 lr: 0.02\n","iteration: 58700 loss: 0.0021 lr: 0.02\n","iteration: 58750 loss: 0.0023 lr: 0.02\n","iteration: 58800 loss: 0.0023 lr: 0.02\n","iteration: 58850 loss: 0.0022 lr: 0.02\n","iteration: 58900 loss: 0.0024 lr: 0.02\n","iteration: 58950 loss: 0.0023 lr: 0.02\n","iteration: 59000 loss: 0.0022 lr: 0.02\n","iteration: 59050 loss: 0.0021 lr: 0.02\n","iteration: 59100 loss: 0.0023 lr: 0.02\n","iteration: 59150 loss: 0.0022 lr: 0.02\n","iteration: 59200 loss: 0.0022 lr: 0.02\n","iteration: 59250 loss: 0.0021 lr: 0.02\n","iteration: 59300 loss: 0.0022 lr: 0.02\n","iteration: 59350 loss: 0.0023 lr: 0.02\n","iteration: 59400 loss: 0.0024 lr: 0.02\n","iteration: 59450 loss: 0.0022 lr: 0.02\n","iteration: 59500 loss: 0.0022 lr: 0.02\n","iteration: 59550 loss: 0.0023 lr: 0.02\n","iteration: 59600 loss: 0.0023 lr: 0.02\n","iteration: 59650 loss: 0.0021 lr: 0.02\n","iteration: 59700 loss: 0.0022 lr: 0.02\n","iteration: 59750 loss: 0.0023 lr: 0.02\n","iteration: 59800 loss: 0.0022 lr: 0.02\n","iteration: 59850 loss: 0.0024 lr: 0.02\n","iteration: 59900 loss: 0.0023 lr: 0.02\n","iteration: 59950 loss: 0.0021 lr: 0.02\n","iteration: 60000 loss: 0.0022 lr: 0.02\n","iteration: 60050 loss: 0.0022 lr: 0.02\n","iteration: 60100 loss: 0.0022 lr: 0.02\n","iteration: 60150 loss: 0.0023 lr: 0.02\n","iteration: 60200 loss: 0.0022 lr: 0.02\n","iteration: 60250 loss: 0.0022 lr: 0.02\n","iteration: 60300 loss: 0.0021 lr: 0.02\n","iteration: 60350 loss: 0.0022 lr: 0.02\n","iteration: 60400 loss: 0.0023 lr: 0.02\n","iteration: 60450 loss: 0.0022 lr: 0.02\n","iteration: 60500 loss: 0.0022 lr: 0.02\n","iteration: 60550 loss: 0.0024 lr: 0.02\n","iteration: 60600 loss: 0.0024 lr: 0.02\n","iteration: 60650 loss: 0.0023 lr: 0.02\n","iteration: 60700 loss: 0.0024 lr: 0.02\n","iteration: 60750 loss: 0.0022 lr: 0.02\n","iteration: 60800 loss: 0.0022 lr: 0.02\n","iteration: 60850 loss: 0.0022 lr: 0.02\n","iteration: 60900 loss: 0.0023 lr: 0.02\n","iteration: 60950 loss: 0.0021 lr: 0.02\n","iteration: 61000 loss: 0.0023 lr: 0.02\n","iteration: 61050 loss: 0.0023 lr: 0.02\n","iteration: 61100 loss: 0.0023 lr: 0.02\n","iteration: 61150 loss: 0.0022 lr: 0.02\n","iteration: 61200 loss: 0.0023 lr: 0.02\n","iteration: 61250 loss: 0.0023 lr: 0.02\n","iteration: 61300 loss: 0.0022 lr: 0.02\n","iteration: 61350 loss: 0.0023 lr: 0.02\n","iteration: 61400 loss: 0.0023 lr: 0.02\n","iteration: 61450 loss: 0.0021 lr: 0.02\n","iteration: 61500 loss: 0.0021 lr: 0.02\n","iteration: 61550 loss: 0.0022 lr: 0.02\n","iteration: 61600 loss: 0.0022 lr: 0.02\n","iteration: 61650 loss: 0.0021 lr: 0.02\n","iteration: 61700 loss: 0.0022 lr: 0.02\n","iteration: 61750 loss: 0.0022 lr: 0.02\n","iteration: 61800 loss: 0.0021 lr: 0.02\n","iteration: 61850 loss: 0.0021 lr: 0.02\n","iteration: 61900 loss: 0.0021 lr: 0.02\n","iteration: 61950 loss: 0.0023 lr: 0.02\n","iteration: 62000 loss: 0.0022 lr: 0.02\n","iteration: 62050 loss: 0.0023 lr: 0.02\n","iteration: 62100 loss: 0.0023 lr: 0.02\n","iteration: 62150 loss: 0.0021 lr: 0.02\n","iteration: 62200 loss: 0.0023 lr: 0.02\n","iteration: 62250 loss: 0.0022 lr: 0.02\n","iteration: 62300 loss: 0.0021 lr: 0.02\n","iteration: 62350 loss: 0.0022 lr: 0.02\n","iteration: 62400 loss: 0.0020 lr: 0.02\n","iteration: 62450 loss: 0.0023 lr: 0.02\n","iteration: 62500 loss: 0.0021 lr: 0.02\n","iteration: 62550 loss: 0.0022 lr: 0.02\n","iteration: 62600 loss: 0.0021 lr: 0.02\n","iteration: 62650 loss: 0.0023 lr: 0.02\n","iteration: 62700 loss: 0.0023 lr: 0.02\n","iteration: 62750 loss: 0.0021 lr: 0.02\n","iteration: 62800 loss: 0.0022 lr: 0.02\n","iteration: 62850 loss: 0.0021 lr: 0.02\n","iteration: 62900 loss: 0.0022 lr: 0.02\n","iteration: 62950 loss: 0.0021 lr: 0.02\n","iteration: 63000 loss: 0.0023 lr: 0.02\n","iteration: 63050 loss: 0.0021 lr: 0.02\n","iteration: 63100 loss: 0.0021 lr: 0.02\n","iteration: 63150 loss: 0.0021 lr: 0.02\n","iteration: 63200 loss: 0.0021 lr: 0.02\n","iteration: 63250 loss: 0.0021 lr: 0.02\n","iteration: 63300 loss: 0.0022 lr: 0.02\n","iteration: 63350 loss: 0.0022 lr: 0.02\n","iteration: 63400 loss: 0.0021 lr: 0.02\n","iteration: 63450 loss: 0.0023 lr: 0.02\n","iteration: 63500 loss: 0.0023 lr: 0.02\n","iteration: 63550 loss: 0.0023 lr: 0.02\n","iteration: 63600 loss: 0.0024 lr: 0.02\n","iteration: 63650 loss: 0.0024 lr: 0.02\n","iteration: 63700 loss: 0.0022 lr: 0.02\n","iteration: 63750 loss: 0.0021 lr: 0.02\n","iteration: 63800 loss: 0.0021 lr: 0.02\n","iteration: 63850 loss: 0.0022 lr: 0.02\n","iteration: 63900 loss: 0.0022 lr: 0.02\n","iteration: 63950 loss: 0.0023 lr: 0.02\n","iteration: 64000 loss: 0.0022 lr: 0.02\n","iteration: 64050 loss: 0.0021 lr: 0.02\n","iteration: 64100 loss: 0.0021 lr: 0.02\n","iteration: 64150 loss: 0.0022 lr: 0.02\n","iteration: 64200 loss: 0.0022 lr: 0.02\n","iteration: 64250 loss: 0.0021 lr: 0.02\n","iteration: 64300 loss: 0.0020 lr: 0.02\n","iteration: 64350 loss: 0.0022 lr: 0.02\n","iteration: 64400 loss: 0.0022 lr: 0.02\n","iteration: 64450 loss: 0.0021 lr: 0.02\n","iteration: 64500 loss: 0.0022 lr: 0.02\n","iteration: 64550 loss: 0.0021 lr: 0.02\n","iteration: 64600 loss: 0.0021 lr: 0.02\n","iteration: 64650 loss: 0.0021 lr: 0.02\n","iteration: 64700 loss: 0.0021 lr: 0.02\n","iteration: 64750 loss: 0.0024 lr: 0.02\n","iteration: 64800 loss: 0.0021 lr: 0.02\n","iteration: 64850 loss: 0.0022 lr: 0.02\n","iteration: 64900 loss: 0.0021 lr: 0.02\n","iteration: 64950 loss: 0.0022 lr: 0.02\n","iteration: 65000 loss: 0.0021 lr: 0.02\n","iteration: 65050 loss: 0.0022 lr: 0.02\n","iteration: 65100 loss: 0.0021 lr: 0.02\n","iteration: 65150 loss: 0.0020 lr: 0.02\n","iteration: 65200 loss: 0.0021 lr: 0.02\n","iteration: 65250 loss: 0.0023 lr: 0.02\n","iteration: 65300 loss: 0.0021 lr: 0.02\n","iteration: 65350 loss: 0.0020 lr: 0.02\n","iteration: 65400 loss: 0.0022 lr: 0.02\n","iteration: 65450 loss: 0.0021 lr: 0.02\n","iteration: 65500 loss: 0.0022 lr: 0.02\n","iteration: 65550 loss: 0.0021 lr: 0.02\n","iteration: 65600 loss: 0.0021 lr: 0.02\n","iteration: 65650 loss: 0.0020 lr: 0.02\n","iteration: 65700 loss: 0.0020 lr: 0.02\n","iteration: 65750 loss: 0.0022 lr: 0.02\n","iteration: 65800 loss: 0.0022 lr: 0.02\n","iteration: 65850 loss: 0.0021 lr: 0.02\n","iteration: 65900 loss: 0.0023 lr: 0.02\n","iteration: 65950 loss: 0.0022 lr: 0.02\n","iteration: 66000 loss: 0.0021 lr: 0.02\n","iteration: 66050 loss: 0.0021 lr: 0.02\n","iteration: 66100 loss: 0.0022 lr: 0.02\n","iteration: 66150 loss: 0.0024 lr: 0.02\n","iteration: 66200 loss: 0.0020 lr: 0.02\n","iteration: 66250 loss: 0.0022 lr: 0.02\n","iteration: 66300 loss: 0.0020 lr: 0.02\n","iteration: 66350 loss: 0.0020 lr: 0.02\n","iteration: 66400 loss: 0.0021 lr: 0.02\n","iteration: 66450 loss: 0.0021 lr: 0.02\n","iteration: 66500 loss: 0.0022 lr: 0.02\n","iteration: 66550 loss: 0.0020 lr: 0.02\n","iteration: 66600 loss: 0.0022 lr: 0.02\n","iteration: 66650 loss: 0.0020 lr: 0.02\n","iteration: 66700 loss: 0.0023 lr: 0.02\n","iteration: 66750 loss: 0.0021 lr: 0.02\n","iteration: 66800 loss: 0.0021 lr: 0.02\n","iteration: 66850 loss: 0.0022 lr: 0.02\n","iteration: 66900 loss: 0.0022 lr: 0.02\n","iteration: 66950 loss: 0.0021 lr: 0.02\n","iteration: 67000 loss: 0.0021 lr: 0.02\n","iteration: 67050 loss: 0.0022 lr: 0.02\n","iteration: 67100 loss: 0.0021 lr: 0.02\n","iteration: 67150 loss: 0.0022 lr: 0.02\n","iteration: 67200 loss: 0.0022 lr: 0.02\n","iteration: 67250 loss: 0.0023 lr: 0.02\n","iteration: 67300 loss: 0.0021 lr: 0.02\n","iteration: 67350 loss: 0.0021 lr: 0.02\n","iteration: 67400 loss: 0.0021 lr: 0.02\n","iteration: 67450 loss: 0.0022 lr: 0.02\n","iteration: 67500 loss: 0.0021 lr: 0.02\n","iteration: 67550 loss: 0.0023 lr: 0.02\n","iteration: 67600 loss: 0.0021 lr: 0.02\n","iteration: 67650 loss: 0.0021 lr: 0.02\n","iteration: 67700 loss: 0.0020 lr: 0.02\n","iteration: 67750 loss: 0.0021 lr: 0.02\n","iteration: 67800 loss: 0.0020 lr: 0.02\n","iteration: 67850 loss: 0.0021 lr: 0.02\n","iteration: 67900 loss: 0.0021 lr: 0.02\n","iteration: 67950 loss: 0.0023 lr: 0.02\n","iteration: 68000 loss: 0.0023 lr: 0.02\n","iteration: 68050 loss: 0.0022 lr: 0.02\n","iteration: 68100 loss: 0.0021 lr: 0.02\n","iteration: 68150 loss: 0.0022 lr: 0.02\n","iteration: 68200 loss: 0.0022 lr: 0.02\n","iteration: 68250 loss: 0.0022 lr: 0.02\n","iteration: 68300 loss: 0.0021 lr: 0.02\n","iteration: 68350 loss: 0.0021 lr: 0.02\n","iteration: 68400 loss: 0.0022 lr: 0.02\n","iteration: 68450 loss: 0.0021 lr: 0.02\n","iteration: 68500 loss: 0.0021 lr: 0.02\n","iteration: 68550 loss: 0.0022 lr: 0.02\n","iteration: 68600 loss: 0.0021 lr: 0.02\n","iteration: 68650 loss: 0.0021 lr: 0.02\n","iteration: 68700 loss: 0.0020 lr: 0.02\n","iteration: 68750 loss: 0.0022 lr: 0.02\n","iteration: 68800 loss: 0.0020 lr: 0.02\n","iteration: 68850 loss: 0.0021 lr: 0.02\n","iteration: 68900 loss: 0.0020 lr: 0.02\n","iteration: 68950 loss: 0.0021 lr: 0.02\n","iteration: 69000 loss: 0.0021 lr: 0.02\n","iteration: 69050 loss: 0.0021 lr: 0.02\n","iteration: 69100 loss: 0.0022 lr: 0.02\n","iteration: 69150 loss: 0.0020 lr: 0.02\n","iteration: 69200 loss: 0.0020 lr: 0.02\n","iteration: 69250 loss: 0.0021 lr: 0.02\n","iteration: 69300 loss: 0.0021 lr: 0.02\n","iteration: 69350 loss: 0.0022 lr: 0.02\n","iteration: 69400 loss: 0.0022 lr: 0.02\n","iteration: 69450 loss: 0.0020 lr: 0.02\n","iteration: 69500 loss: 0.0022 lr: 0.02\n","iteration: 69550 loss: 0.0020 lr: 0.02\n","iteration: 69600 loss: 0.0020 lr: 0.02\n","iteration: 69650 loss: 0.0023 lr: 0.02\n","iteration: 69700 loss: 0.0021 lr: 0.02\n","iteration: 69750 loss: 0.0021 lr: 0.02\n","iteration: 69800 loss: 0.0021 lr: 0.02\n","iteration: 69850 loss: 0.0021 lr: 0.02\n","iteration: 69900 loss: 0.0019 lr: 0.02\n","iteration: 69950 loss: 0.0023 lr: 0.02\n","iteration: 70000 loss: 0.0021 lr: 0.02\n","iteration: 70050 loss: 0.0021 lr: 0.02\n","iteration: 70100 loss: 0.0022 lr: 0.02\n","iteration: 70150 loss: 0.0019 lr: 0.02\n","iteration: 70200 loss: 0.0021 lr: 0.02\n","iteration: 70250 loss: 0.0022 lr: 0.02\n","iteration: 70300 loss: 0.0021 lr: 0.02\n","iteration: 70350 loss: 0.0020 lr: 0.02\n","iteration: 70400 loss: 0.0021 lr: 0.02\n","iteration: 70450 loss: 0.0021 lr: 0.02\n","iteration: 70500 loss: 0.0020 lr: 0.02\n","iteration: 70550 loss: 0.0021 lr: 0.02\n","iteration: 70600 loss: 0.0020 lr: 0.02\n","iteration: 70650 loss: 0.0021 lr: 0.02\n","iteration: 70700 loss: 0.0020 lr: 0.02\n","iteration: 70750 loss: 0.0022 lr: 0.02\n","iteration: 70800 loss: 0.0019 lr: 0.02\n","iteration: 70850 loss: 0.0021 lr: 0.02\n","iteration: 70900 loss: 0.0021 lr: 0.02\n","iteration: 70950 loss: 0.0021 lr: 0.02\n","iteration: 71000 loss: 0.0020 lr: 0.02\n","iteration: 71050 loss: 0.0021 lr: 0.02\n","iteration: 71100 loss: 0.0021 lr: 0.02\n","iteration: 71150 loss: 0.0020 lr: 0.02\n","iteration: 71200 loss: 0.0021 lr: 0.02\n","iteration: 71250 loss: 0.0021 lr: 0.02\n","iteration: 71300 loss: 0.0020 lr: 0.02\n","iteration: 71350 loss: 0.0019 lr: 0.02\n","iteration: 71400 loss: 0.0020 lr: 0.02\n","iteration: 71450 loss: 0.0020 lr: 0.02\n","iteration: 71500 loss: 0.0021 lr: 0.02\n","iteration: 71550 loss: 0.0021 lr: 0.02\n","iteration: 71600 loss: 0.0020 lr: 0.02\n","iteration: 71650 loss: 0.0022 lr: 0.02\n","iteration: 71700 loss: 0.0020 lr: 0.02\n","iteration: 71750 loss: 0.0019 lr: 0.02\n","iteration: 71800 loss: 0.0021 lr: 0.02\n","iteration: 71850 loss: 0.0020 lr: 0.02\n","iteration: 71900 loss: 0.0020 lr: 0.02\n","iteration: 71950 loss: 0.0021 lr: 0.02\n","iteration: 72000 loss: 0.0020 lr: 0.02\n","iteration: 72050 loss: 0.0019 lr: 0.02\n","iteration: 72100 loss: 0.0021 lr: 0.02\n","iteration: 72150 loss: 0.0020 lr: 0.02\n","iteration: 72200 loss: 0.0021 lr: 0.02\n","iteration: 72250 loss: 0.0020 lr: 0.02\n","iteration: 72300 loss: 0.0021 lr: 0.02\n","iteration: 72350 loss: 0.0019 lr: 0.02\n","iteration: 72400 loss: 0.0021 lr: 0.02\n","iteration: 72450 loss: 0.0020 lr: 0.02\n","iteration: 72500 loss: 0.0020 lr: 0.02\n","iteration: 72550 loss: 0.0019 lr: 0.02\n","iteration: 72600 loss: 0.0021 lr: 0.02\n","iteration: 72650 loss: 0.0019 lr: 0.02\n","iteration: 72700 loss: 0.0021 lr: 0.02\n","iteration: 72750 loss: 0.0020 lr: 0.02\n","iteration: 72800 loss: 0.0019 lr: 0.02\n","iteration: 72850 loss: 0.0018 lr: 0.02\n","iteration: 72900 loss: 0.0020 lr: 0.02\n","iteration: 72950 loss: 0.0021 lr: 0.02\n","iteration: 73000 loss: 0.0020 lr: 0.02\n","iteration: 73050 loss: 0.0020 lr: 0.02\n","iteration: 73100 loss: 0.0020 lr: 0.02\n","iteration: 73150 loss: 0.0021 lr: 0.02\n","iteration: 73200 loss: 0.0019 lr: 0.02\n","iteration: 73250 loss: 0.0021 lr: 0.02\n","iteration: 73300 loss: 0.0020 lr: 0.02\n","iteration: 73350 loss: 0.0020 lr: 0.02\n","iteration: 73400 loss: 0.0019 lr: 0.02\n","iteration: 73450 loss: 0.0021 lr: 0.02\n","iteration: 73500 loss: 0.0019 lr: 0.02\n","iteration: 73550 loss: 0.0019 lr: 0.02\n","iteration: 73600 loss: 0.0020 lr: 0.02\n","iteration: 73650 loss: 0.0019 lr: 0.02\n","iteration: 73700 loss: 0.0020 lr: 0.02\n","iteration: 73750 loss: 0.0019 lr: 0.02\n","iteration: 73800 loss: 0.0021 lr: 0.02\n","iteration: 73850 loss: 0.0022 lr: 0.02\n","iteration: 73900 loss: 0.0021 lr: 0.02\n","iteration: 73950 loss: 0.0021 lr: 0.02\n","iteration: 74000 loss: 0.0019 lr: 0.02\n","iteration: 74050 loss: 0.0020 lr: 0.02\n","iteration: 74100 loss: 0.0020 lr: 0.02\n","iteration: 74150 loss: 0.0020 lr: 0.02\n","iteration: 74200 loss: 0.0020 lr: 0.02\n","iteration: 74250 loss: 0.0021 lr: 0.02\n","iteration: 74300 loss: 0.0021 lr: 0.02\n","iteration: 74350 loss: 0.0020 lr: 0.02\n","iteration: 74400 loss: 0.0021 lr: 0.02\n","iteration: 74450 loss: 0.0021 lr: 0.02\n","iteration: 74500 loss: 0.0019 lr: 0.02\n","iteration: 74550 loss: 0.0021 lr: 0.02\n","iteration: 74600 loss: 0.0020 lr: 0.02\n","iteration: 74650 loss: 0.0020 lr: 0.02\n","iteration: 74700 loss: 0.0019 lr: 0.02\n","iteration: 74750 loss: 0.0019 lr: 0.02\n","iteration: 74800 loss: 0.0019 lr: 0.02\n","iteration: 74850 loss: 0.0019 lr: 0.02\n","iteration: 74900 loss: 0.0020 lr: 0.02\n","iteration: 74950 loss: 0.0021 lr: 0.02\n","iteration: 75000 loss: 0.0022 lr: 0.02\n","iteration: 75050 loss: 0.0021 lr: 0.02\n","iteration: 75100 loss: 0.0020 lr: 0.02\n","iteration: 75150 loss: 0.0018 lr: 0.02\n","iteration: 75200 loss: 0.0020 lr: 0.02\n","iteration: 75250 loss: 0.0019 lr: 0.02\n","iteration: 75300 loss: 0.0021 lr: 0.02\n","iteration: 75350 loss: 0.0020 lr: 0.02\n","iteration: 75400 loss: 0.0019 lr: 0.02\n","iteration: 75450 loss: 0.0019 lr: 0.02\n","iteration: 75500 loss: 0.0019 lr: 0.02\n","iteration: 75550 loss: 0.0019 lr: 0.02\n","iteration: 75600 loss: 0.0019 lr: 0.02\n","iteration: 75650 loss: 0.0020 lr: 0.02\n","iteration: 75700 loss: 0.0021 lr: 0.02\n","iteration: 75750 loss: 0.0022 lr: 0.02\n","iteration: 75800 loss: 0.0020 lr: 0.02\n","iteration: 75850 loss: 0.0020 lr: 0.02\n","iteration: 75900 loss: 0.0019 lr: 0.02\n","iteration: 75950 loss: 0.0020 lr: 0.02\n","iteration: 76000 loss: 0.0019 lr: 0.02\n","iteration: 76050 loss: 0.0019 lr: 0.02\n","iteration: 76100 loss: 0.0020 lr: 0.02\n","iteration: 76150 loss: 0.0021 lr: 0.02\n","iteration: 76200 loss: 0.0020 lr: 0.02\n","iteration: 76250 loss: 0.0019 lr: 0.02\n","iteration: 76300 loss: 0.0019 lr: 0.02\n","iteration: 76350 loss: 0.0019 lr: 0.02\n","iteration: 76400 loss: 0.0019 lr: 0.02\n","iteration: 76450 loss: 0.0021 lr: 0.02\n","iteration: 76500 loss: 0.0021 lr: 0.02\n","iteration: 76550 loss: 0.0019 lr: 0.02\n","iteration: 76600 loss: 0.0021 lr: 0.02\n","iteration: 76650 loss: 0.0021 lr: 0.02\n","iteration: 76700 loss: 0.0021 lr: 0.02\n","iteration: 76750 loss: 0.0020 lr: 0.02\n","iteration: 76800 loss: 0.0020 lr: 0.02\n","iteration: 76850 loss: 0.0020 lr: 0.02\n","iteration: 76900 loss: 0.0021 lr: 0.02\n","iteration: 76950 loss: 0.0018 lr: 0.02\n","iteration: 77000 loss: 0.0020 lr: 0.02\n","iteration: 77050 loss: 0.0020 lr: 0.02\n","iteration: 77100 loss: 0.0020 lr: 0.02\n","iteration: 77150 loss: 0.0020 lr: 0.02\n","iteration: 77200 loss: 0.0019 lr: 0.02\n","iteration: 77250 loss: 0.0019 lr: 0.02\n","iteration: 77300 loss: 0.0020 lr: 0.02\n","iteration: 77350 loss: 0.0019 lr: 0.02\n","iteration: 77400 loss: 0.0020 lr: 0.02\n","iteration: 77450 loss: 0.0020 lr: 0.02\n","iteration: 77500 loss: 0.0019 lr: 0.02\n","iteration: 77550 loss: 0.0020 lr: 0.02\n","iteration: 77600 loss: 0.0019 lr: 0.02\n","iteration: 77650 loss: 0.0020 lr: 0.02\n","iteration: 77700 loss: 0.0020 lr: 0.02\n","iteration: 77750 loss: 0.0019 lr: 0.02\n","iteration: 77800 loss: 0.0020 lr: 0.02\n","iteration: 77850 loss: 0.0020 lr: 0.02\n","iteration: 77900 loss: 0.0023 lr: 0.02\n","iteration: 77950 loss: 0.0020 lr: 0.02\n","iteration: 78000 loss: 0.0019 lr: 0.02\n","iteration: 78050 loss: 0.0020 lr: 0.02\n","iteration: 78100 loss: 0.0020 lr: 0.02\n","iteration: 78150 loss: 0.0019 lr: 0.02\n","iteration: 78200 loss: 0.0019 lr: 0.02\n","iteration: 78250 loss: 0.0020 lr: 0.02\n","iteration: 78300 loss: 0.0019 lr: 0.02\n","iteration: 78350 loss: 0.0020 lr: 0.02\n","iteration: 78400 loss: 0.0019 lr: 0.02\n","iteration: 78450 loss: 0.0019 lr: 0.02\n","iteration: 78500 loss: 0.0021 lr: 0.02\n","iteration: 78550 loss: 0.0019 lr: 0.02\n","iteration: 78600 loss: 0.0020 lr: 0.02\n","iteration: 78650 loss: 0.0020 lr: 0.02\n","iteration: 78700 loss: 0.0021 lr: 0.02\n","iteration: 78750 loss: 0.0022 lr: 0.02\n","iteration: 78800 loss: 0.0020 lr: 0.02\n","iteration: 78850 loss: 0.0021 lr: 0.02\n","iteration: 78900 loss: 0.0019 lr: 0.02\n","iteration: 78950 loss: 0.0021 lr: 0.02\n","iteration: 79000 loss: 0.0019 lr: 0.02\n","iteration: 79050 loss: 0.0019 lr: 0.02\n","iteration: 79100 loss: 0.0020 lr: 0.02\n","iteration: 79150 loss: 0.0020 lr: 0.02\n","iteration: 79200 loss: 0.0019 lr: 0.02\n","iteration: 79250 loss: 0.0019 lr: 0.02\n","iteration: 79300 loss: 0.0020 lr: 0.02\n","iteration: 79350 loss: 0.0020 lr: 0.02\n","iteration: 79400 loss: 0.0017 lr: 0.02\n","iteration: 79450 loss: 0.0019 lr: 0.02\n","iteration: 79500 loss: 0.0019 lr: 0.02\n","iteration: 79550 loss: 0.0019 lr: 0.02\n","iteration: 79600 loss: 0.0020 lr: 0.02\n","iteration: 79650 loss: 0.0019 lr: 0.02\n","iteration: 79700 loss: 0.0019 lr: 0.02\n","iteration: 79750 loss: 0.0019 lr: 0.02\n","iteration: 79800 loss: 0.0019 lr: 0.02\n","iteration: 79850 loss: 0.0020 lr: 0.02\n","iteration: 79900 loss: 0.0019 lr: 0.02\n","iteration: 79950 loss: 0.0019 lr: 0.02\n","iteration: 80000 loss: 0.0020 lr: 0.02\n","iteration: 80050 loss: 0.0020 lr: 0.02\n","iteration: 80100 loss: 0.0020 lr: 0.02\n","iteration: 80150 loss: 0.0019 lr: 0.02\n","iteration: 80200 loss: 0.0020 lr: 0.02\n","iteration: 80250 loss: 0.0020 lr: 0.02\n","iteration: 80300 loss: 0.0019 lr: 0.02\n","iteration: 80350 loss: 0.0018 lr: 0.02\n","iteration: 80400 loss: 0.0020 lr: 0.02\n","iteration: 80450 loss: 0.0020 lr: 0.02\n","iteration: 80500 loss: 0.0019 lr: 0.02\n","iteration: 80550 loss: 0.0020 lr: 0.02\n","iteration: 80600 loss: 0.0020 lr: 0.02\n","iteration: 80650 loss: 0.0019 lr: 0.02\n","iteration: 80700 loss: 0.0019 lr: 0.02\n","iteration: 80750 loss: 0.0019 lr: 0.02\n","iteration: 80800 loss: 0.0018 lr: 0.02\n","iteration: 80850 loss: 0.0021 lr: 0.02\n","iteration: 80900 loss: 0.0020 lr: 0.02\n","iteration: 80950 loss: 0.0019 lr: 0.02\n","iteration: 81000 loss: 0.0020 lr: 0.02\n","iteration: 81050 loss: 0.0020 lr: 0.02\n","iteration: 81100 loss: 0.0020 lr: 0.02\n","iteration: 81150 loss: 0.0020 lr: 0.02\n","iteration: 81200 loss: 0.0019 lr: 0.02\n","iteration: 81250 loss: 0.0020 lr: 0.02\n","iteration: 81300 loss: 0.0020 lr: 0.02\n","iteration: 81350 loss: 0.0020 lr: 0.02\n","iteration: 81400 loss: 0.0019 lr: 0.02\n","iteration: 81450 loss: 0.0019 lr: 0.02\n","iteration: 81500 loss: 0.0019 lr: 0.02\n","iteration: 81550 loss: 0.0019 lr: 0.02\n","iteration: 81600 loss: 0.0019 lr: 0.02\n","iteration: 81650 loss: 0.0020 lr: 0.02\n","iteration: 81700 loss: 0.0021 lr: 0.02\n","iteration: 81750 loss: 0.0018 lr: 0.02\n","iteration: 81800 loss: 0.0020 lr: 0.02\n","iteration: 81850 loss: 0.0018 lr: 0.02\n","iteration: 81900 loss: 0.0020 lr: 0.02\n","iteration: 81950 loss: 0.0020 lr: 0.02\n","iteration: 82000 loss: 0.0019 lr: 0.02\n","iteration: 82050 loss: 0.0019 lr: 0.02\n","iteration: 82100 loss: 0.0018 lr: 0.02\n","iteration: 82150 loss: 0.0018 lr: 0.02\n","iteration: 82200 loss: 0.0018 lr: 0.02\n","iteration: 82250 loss: 0.0020 lr: 0.02\n","iteration: 82300 loss: 0.0018 lr: 0.02\n","iteration: 82350 loss: 0.0020 lr: 0.02\n","iteration: 82400 loss: 0.0020 lr: 0.02\n","iteration: 82450 loss: 0.0019 lr: 0.02\n","iteration: 82500 loss: 0.0020 lr: 0.02\n","iteration: 82550 loss: 0.0020 lr: 0.02\n","iteration: 82600 loss: 0.0020 lr: 0.02\n","iteration: 82650 loss: 0.0020 lr: 0.02\n","iteration: 82700 loss: 0.0019 lr: 0.02\n","iteration: 82750 loss: 0.0019 lr: 0.02\n","iteration: 82800 loss: 0.0019 lr: 0.02\n","iteration: 82850 loss: 0.0021 lr: 0.02\n","iteration: 82900 loss: 0.0019 lr: 0.02\n","iteration: 82950 loss: 0.0019 lr: 0.02\n","iteration: 83000 loss: 0.0020 lr: 0.02\n","iteration: 83050 loss: 0.0020 lr: 0.02\n","iteration: 83100 loss: 0.0020 lr: 0.02\n","iteration: 83150 loss: 0.0019 lr: 0.02\n","iteration: 83200 loss: 0.0018 lr: 0.02\n","iteration: 83250 loss: 0.0018 lr: 0.02\n","iteration: 83300 loss: 0.0019 lr: 0.02\n","iteration: 83350 loss: 0.0019 lr: 0.02\n","iteration: 83400 loss: 0.0018 lr: 0.02\n","iteration: 83450 loss: 0.0019 lr: 0.02\n","iteration: 83500 loss: 0.0019 lr: 0.02\n","iteration: 83550 loss: 0.0018 lr: 0.02\n","iteration: 83600 loss: 0.0020 lr: 0.02\n","iteration: 83650 loss: 0.0018 lr: 0.02\n","iteration: 83700 loss: 0.0017 lr: 0.02\n","iteration: 83750 loss: 0.0018 lr: 0.02\n","iteration: 83800 loss: 0.0019 lr: 0.02\n","iteration: 83850 loss: 0.0019 lr: 0.02\n","iteration: 83900 loss: 0.0018 lr: 0.02\n","iteration: 83950 loss: 0.0018 lr: 0.02\n","iteration: 84000 loss: 0.0019 lr: 0.02\n","iteration: 84050 loss: 0.0019 lr: 0.02\n","iteration: 84100 loss: 0.0019 lr: 0.02\n","iteration: 84150 loss: 0.0020 lr: 0.02\n","iteration: 84200 loss: 0.0019 lr: 0.02\n","iteration: 84250 loss: 0.0019 lr: 0.02\n","iteration: 84300 loss: 0.0019 lr: 0.02\n","iteration: 84350 loss: 0.0019 lr: 0.02\n","iteration: 84400 loss: 0.0019 lr: 0.02\n","iteration: 84450 loss: 0.0019 lr: 0.02\n","iteration: 84500 loss: 0.0020 lr: 0.02\n","iteration: 84550 loss: 0.0019 lr: 0.02\n","iteration: 84600 loss: 0.0019 lr: 0.02\n","iteration: 84650 loss: 0.0019 lr: 0.02\n","iteration: 84700 loss: 0.0018 lr: 0.02\n","iteration: 84750 loss: 0.0019 lr: 0.02\n","iteration: 84800 loss: 0.0019 lr: 0.02\n","iteration: 84850 loss: 0.0018 lr: 0.02\n","iteration: 84900 loss: 0.0019 lr: 0.02\n","iteration: 84950 loss: 0.0019 lr: 0.02\n","iteration: 85000 loss: 0.0019 lr: 0.02\n","iteration: 85050 loss: 0.0020 lr: 0.02\n","iteration: 85100 loss: 0.0019 lr: 0.02\n","iteration: 85150 loss: 0.0019 lr: 0.02\n","iteration: 85200 loss: 0.0019 lr: 0.02\n","iteration: 85250 loss: 0.0019 lr: 0.02\n","iteration: 85300 loss: 0.0018 lr: 0.02\n","iteration: 85350 loss: 0.0018 lr: 0.02\n","iteration: 85400 loss: 0.0020 lr: 0.02\n","iteration: 85450 loss: 0.0018 lr: 0.02\n","iteration: 85500 loss: 0.0019 lr: 0.02\n","iteration: 85550 loss: 0.0019 lr: 0.02\n","iteration: 85600 loss: 0.0019 lr: 0.02\n","iteration: 85650 loss: 0.0017 lr: 0.02\n","iteration: 85700 loss: 0.0016 lr: 0.02\n","iteration: 85750 loss: 0.0019 lr: 0.02\n","iteration: 85800 loss: 0.0020 lr: 0.02\n","iteration: 85850 loss: 0.0020 lr: 0.02\n","iteration: 85900 loss: 0.0019 lr: 0.02\n","iteration: 85950 loss: 0.0018 lr: 0.02\n","iteration: 86000 loss: 0.0020 lr: 0.02\n","iteration: 86050 loss: 0.0020 lr: 0.02\n","iteration: 86100 loss: 0.0020 lr: 0.02\n","iteration: 86150 loss: 0.0019 lr: 0.02\n","iteration: 86200 loss: 0.0018 lr: 0.02\n","iteration: 86250 loss: 0.0019 lr: 0.02\n","iteration: 86300 loss: 0.0019 lr: 0.02\n","iteration: 86350 loss: 0.0019 lr: 0.02\n","iteration: 86400 loss: 0.0018 lr: 0.02\n","iteration: 86450 loss: 0.0019 lr: 0.02\n","iteration: 86500 loss: 0.0019 lr: 0.02\n","iteration: 86550 loss: 0.0019 lr: 0.02\n","iteration: 86600 loss: 0.0019 lr: 0.02\n","iteration: 86650 loss: 0.0019 lr: 0.02\n","iteration: 86700 loss: 0.0018 lr: 0.02\n","iteration: 86750 loss: 0.0018 lr: 0.02\n","iteration: 86800 loss: 0.0018 lr: 0.02\n","iteration: 86850 loss: 0.0018 lr: 0.02\n","iteration: 86900 loss: 0.0018 lr: 0.02\n","iteration: 86950 loss: 0.0019 lr: 0.02\n","iteration: 87000 loss: 0.0017 lr: 0.02\n","iteration: 87050 loss: 0.0019 lr: 0.02\n","iteration: 87100 loss: 0.0019 lr: 0.02\n","iteration: 87150 loss: 0.0018 lr: 0.02\n","iteration: 87200 loss: 0.0020 lr: 0.02\n","iteration: 87250 loss: 0.0021 lr: 0.02\n","iteration: 87300 loss: 0.0017 lr: 0.02\n","iteration: 87350 loss: 0.0018 lr: 0.02\n","iteration: 87400 loss: 0.0019 lr: 0.02\n","iteration: 87450 loss: 0.0017 lr: 0.02\n","iteration: 87500 loss: 0.0019 lr: 0.02\n","iteration: 87550 loss: 0.0018 lr: 0.02\n","iteration: 87600 loss: 0.0019 lr: 0.02\n","iteration: 87650 loss: 0.0017 lr: 0.02\n","iteration: 87700 loss: 0.0018 lr: 0.02\n","iteration: 87750 loss: 0.0018 lr: 0.02\n","iteration: 87800 loss: 0.0019 lr: 0.02\n","iteration: 87850 loss: 0.0019 lr: 0.02\n","iteration: 87900 loss: 0.0017 lr: 0.02\n","iteration: 87950 loss: 0.0018 lr: 0.02\n","iteration: 88000 loss: 0.0018 lr: 0.02\n","iteration: 88050 loss: 0.0017 lr: 0.02\n","iteration: 88100 loss: 0.0019 lr: 0.02\n","iteration: 88150 loss: 0.0019 lr: 0.02\n","iteration: 88200 loss: 0.0019 lr: 0.02\n","iteration: 88250 loss: 0.0019 lr: 0.02\n","iteration: 88300 loss: 0.0018 lr: 0.02\n","iteration: 88350 loss: 0.0018 lr: 0.02\n","iteration: 88400 loss: 0.0019 lr: 0.02\n","iteration: 88450 loss: 0.0018 lr: 0.02\n","iteration: 88500 loss: 0.0019 lr: 0.02\n","iteration: 88550 loss: 0.0018 lr: 0.02\n","iteration: 88600 loss: 0.0018 lr: 0.02\n","iteration: 88650 loss: 0.0019 lr: 0.02\n","iteration: 88700 loss: 0.0018 lr: 0.02\n","iteration: 88750 loss: 0.0017 lr: 0.02\n","iteration: 88800 loss: 0.0019 lr: 0.02\n","iteration: 88850 loss: 0.0018 lr: 0.02\n","iteration: 88900 loss: 0.0017 lr: 0.02\n","iteration: 88950 loss: 0.0018 lr: 0.02\n","iteration: 89000 loss: 0.0018 lr: 0.02\n","iteration: 89050 loss: 0.0019 lr: 0.02\n","iteration: 89100 loss: 0.0017 lr: 0.02\n","iteration: 89150 loss: 0.0019 lr: 0.02\n","iteration: 89200 loss: 0.0018 lr: 0.02\n","iteration: 89250 loss: 0.0017 lr: 0.02\n","iteration: 89300 loss: 0.0019 lr: 0.02\n","iteration: 89350 loss: 0.0018 lr: 0.02\n","iteration: 89400 loss: 0.0018 lr: 0.02\n","iteration: 89450 loss: 0.0018 lr: 0.02\n","iteration: 89500 loss: 0.0018 lr: 0.02\n","iteration: 89550 loss: 0.0019 lr: 0.02\n","iteration: 89600 loss: 0.0018 lr: 0.02\n","iteration: 89650 loss: 0.0017 lr: 0.02\n","iteration: 89700 loss: 0.0018 lr: 0.02\n","iteration: 89750 loss: 0.0019 lr: 0.02\n","iteration: 89800 loss: 0.0018 lr: 0.02\n","iteration: 89850 loss: 0.0020 lr: 0.02\n","iteration: 89900 loss: 0.0018 lr: 0.02\n","iteration: 89950 loss: 0.0018 lr: 0.02\n","iteration: 90000 loss: 0.0018 lr: 0.02\n","iteration: 90050 loss: 0.0018 lr: 0.02\n","iteration: 90100 loss: 0.0020 lr: 0.02\n","iteration: 90150 loss: 0.0018 lr: 0.02\n","iteration: 90200 loss: 0.0019 lr: 0.02\n","iteration: 90250 loss: 0.0019 lr: 0.02\n","iteration: 90300 loss: 0.0020 lr: 0.02\n","iteration: 90350 loss: 0.0018 lr: 0.02\n","iteration: 90400 loss: 0.0020 lr: 0.02\n","iteration: 90450 loss: 0.0019 lr: 0.02\n","iteration: 90500 loss: 0.0018 lr: 0.02\n","iteration: 90550 loss: 0.0017 lr: 0.02\n","iteration: 90600 loss: 0.0018 lr: 0.02\n","iteration: 90650 loss: 0.0019 lr: 0.02\n","iteration: 90700 loss: 0.0019 lr: 0.02\n","iteration: 90750 loss: 0.0019 lr: 0.02\n","iteration: 90800 loss: 0.0017 lr: 0.02\n","iteration: 90850 loss: 0.0018 lr: 0.02\n","iteration: 90900 loss: 0.0019 lr: 0.02\n","iteration: 90950 loss: 0.0017 lr: 0.02\n","iteration: 91000 loss: 0.0020 lr: 0.02\n","iteration: 91050 loss: 0.0018 lr: 0.02\n","iteration: 91100 loss: 0.0018 lr: 0.02\n","iteration: 91150 loss: 0.0018 lr: 0.02\n","iteration: 91200 loss: 0.0018 lr: 0.02\n","iteration: 91250 loss: 0.0017 lr: 0.02\n","iteration: 91300 loss: 0.0018 lr: 0.02\n","iteration: 91350 loss: 0.0018 lr: 0.02\n","iteration: 91400 loss: 0.0019 lr: 0.02\n","iteration: 91450 loss: 0.0019 lr: 0.02\n","iteration: 91500 loss: 0.0018 lr: 0.02\n","iteration: 91550 loss: 0.0018 lr: 0.02\n","iteration: 91600 loss: 0.0018 lr: 0.02\n","iteration: 91650 loss: 0.0020 lr: 0.02\n","iteration: 91700 loss: 0.0018 lr: 0.02\n","iteration: 91750 loss: 0.0019 lr: 0.02\n","iteration: 91800 loss: 0.0018 lr: 0.02\n","iteration: 91850 loss: 0.0019 lr: 0.02\n","iteration: 91900 loss: 0.0018 lr: 0.02\n","iteration: 91950 loss: 0.0018 lr: 0.02\n","iteration: 92000 loss: 0.0017 lr: 0.02\n","iteration: 92050 loss: 0.0017 lr: 0.02\n","iteration: 92100 loss: 0.0019 lr: 0.02\n","iteration: 92150 loss: 0.0018 lr: 0.02\n","iteration: 92200 loss: 0.0018 lr: 0.02\n","iteration: 92250 loss: 0.0018 lr: 0.02\n","iteration: 92300 loss: 0.0019 lr: 0.02\n","iteration: 92350 loss: 0.0017 lr: 0.02\n","iteration: 92400 loss: 0.0017 lr: 0.02\n","iteration: 92450 loss: 0.0019 lr: 0.02\n","iteration: 92500 loss: 0.0017 lr: 0.02\n","iteration: 92550 loss: 0.0017 lr: 0.02\n","iteration: 92600 loss: 0.0018 lr: 0.02\n","iteration: 92650 loss: 0.0018 lr: 0.02\n","iteration: 92700 loss: 0.0020 lr: 0.02\n","iteration: 92750 loss: 0.0018 lr: 0.02\n","iteration: 92800 loss: 0.0018 lr: 0.02\n","iteration: 92850 loss: 0.0020 lr: 0.02\n","iteration: 92900 loss: 0.0018 lr: 0.02\n","iteration: 92950 loss: 0.0018 lr: 0.02\n","iteration: 93000 loss: 0.0018 lr: 0.02\n","iteration: 93050 loss: 0.0018 lr: 0.02\n","iteration: 93100 loss: 0.0018 lr: 0.02\n","iteration: 93150 loss: 0.0017 lr: 0.02\n","iteration: 93200 loss: 0.0018 lr: 0.02\n","iteration: 93250 loss: 0.0018 lr: 0.02\n","iteration: 93300 loss: 0.0018 lr: 0.02\n","iteration: 93350 loss: 0.0018 lr: 0.02\n","iteration: 93400 loss: 0.0018 lr: 0.02\n","iteration: 93450 loss: 0.0018 lr: 0.02\n","iteration: 93500 loss: 0.0019 lr: 0.02\n","iteration: 93550 loss: 0.0019 lr: 0.02\n","iteration: 93600 loss: 0.0018 lr: 0.02\n","iteration: 93650 loss: 0.0018 lr: 0.02\n","iteration: 93700 loss: 0.0018 lr: 0.02\n","iteration: 93750 loss: 0.0018 lr: 0.02\n","iteration: 93800 loss: 0.0019 lr: 0.02\n","iteration: 93850 loss: 0.0019 lr: 0.02\n","iteration: 93900 loss: 0.0018 lr: 0.02\n","iteration: 93950 loss: 0.0018 lr: 0.02\n","iteration: 94000 loss: 0.0018 lr: 0.02\n","iteration: 94050 loss: 0.0017 lr: 0.02\n","iteration: 94100 loss: 0.0018 lr: 0.02\n","iteration: 94150 loss: 0.0018 lr: 0.02\n","iteration: 94200 loss: 0.0018 lr: 0.02\n","iteration: 94250 loss: 0.0017 lr: 0.02\n","iteration: 94300 loss: 0.0018 lr: 0.02\n","iteration: 94350 loss: 0.0017 lr: 0.02\n","iteration: 94400 loss: 0.0018 lr: 0.02\n","iteration: 94450 loss: 0.0018 lr: 0.02\n","iteration: 94500 loss: 0.0017 lr: 0.02\n","iteration: 94550 loss: 0.0017 lr: 0.02\n","iteration: 94600 loss: 0.0017 lr: 0.02\n","iteration: 94650 loss: 0.0018 lr: 0.02\n","iteration: 94700 loss: 0.0018 lr: 0.02\n","iteration: 94750 loss: 0.0017 lr: 0.02\n","iteration: 94800 loss: 0.0018 lr: 0.02\n","iteration: 94850 loss: 0.0017 lr: 0.02\n","iteration: 94900 loss: 0.0018 lr: 0.02\n","iteration: 94950 loss: 0.0019 lr: 0.02\n","iteration: 95000 loss: 0.0018 lr: 0.02\n","iteration: 95050 loss: 0.0018 lr: 0.02\n","iteration: 95100 loss: 0.0018 lr: 0.02\n","iteration: 95150 loss: 0.0018 lr: 0.02\n","iteration: 95200 loss: 0.0017 lr: 0.02\n","iteration: 95250 loss: 0.0019 lr: 0.02\n","iteration: 95300 loss: 0.0018 lr: 0.02\n","iteration: 95350 loss: 0.0017 lr: 0.02\n","iteration: 95400 loss: 0.0018 lr: 0.02\n","iteration: 95450 loss: 0.0017 lr: 0.02\n","iteration: 95500 loss: 0.0017 lr: 0.02\n","iteration: 95550 loss: 0.0018 lr: 0.02\n","iteration: 95600 loss: 0.0019 lr: 0.02\n","iteration: 95650 loss: 0.0018 lr: 0.02\n","iteration: 95700 loss: 0.0018 lr: 0.02\n","iteration: 95750 loss: 0.0018 lr: 0.02\n","iteration: 95800 loss: 0.0018 lr: 0.02\n","iteration: 95850 loss: 0.0018 lr: 0.02\n","iteration: 95900 loss: 0.0018 lr: 0.02\n","iteration: 95950 loss: 0.0017 lr: 0.02\n","iteration: 96000 loss: 0.0017 lr: 0.02\n","iteration: 96050 loss: 0.0016 lr: 0.02\n","iteration: 96100 loss: 0.0020 lr: 0.02\n","iteration: 96150 loss: 0.0017 lr: 0.02\n","iteration: 96200 loss: 0.0016 lr: 0.02\n","iteration: 96250 loss: 0.0019 lr: 0.02\n","iteration: 96300 loss: 0.0017 lr: 0.02\n","iteration: 96350 loss: 0.0018 lr: 0.02\n","iteration: 96400 loss: 0.0018 lr: 0.02\n","iteration: 96450 loss: 0.0018 lr: 0.02\n","iteration: 96500 loss: 0.0017 lr: 0.02\n","iteration: 96550 loss: 0.0017 lr: 0.02\n","iteration: 96600 loss: 0.0018 lr: 0.02\n","iteration: 96650 loss: 0.0018 lr: 0.02\n","iteration: 96700 loss: 0.0016 lr: 0.02\n","iteration: 96750 loss: 0.0017 lr: 0.02\n","iteration: 96800 loss: 0.0017 lr: 0.02\n","iteration: 96850 loss: 0.0017 lr: 0.02\n","iteration: 96900 loss: 0.0017 lr: 0.02\n","iteration: 96950 loss: 0.0017 lr: 0.02\n","iteration: 97000 loss: 0.0017 lr: 0.02\n","iteration: 97050 loss: 0.0017 lr: 0.02\n","iteration: 97100 loss: 0.0019 lr: 0.02\n","iteration: 97150 loss: 0.0018 lr: 0.02\n","iteration: 97200 loss: 0.0017 lr: 0.02\n","iteration: 97250 loss: 0.0017 lr: 0.02\n","iteration: 97300 loss: 0.0018 lr: 0.02\n","iteration: 97350 loss: 0.0017 lr: 0.02\n","iteration: 97400 loss: 0.0017 lr: 0.02\n","iteration: 97450 loss: 0.0017 lr: 0.02\n","iteration: 97500 loss: 0.0016 lr: 0.02\n","iteration: 97550 loss: 0.0017 lr: 0.02\n","iteration: 97600 loss: 0.0017 lr: 0.02\n","iteration: 97650 loss: 0.0016 lr: 0.02\n","iteration: 97700 loss: 0.0018 lr: 0.02\n","iteration: 97750 loss: 0.0018 lr: 0.02\n","iteration: 97800 loss: 0.0017 lr: 0.02\n","iteration: 97850 loss: 0.0018 lr: 0.02\n","iteration: 97900 loss: 0.0018 lr: 0.02\n","iteration: 97950 loss: 0.0017 lr: 0.02\n","iteration: 98000 loss: 0.0017 lr: 0.02\n","iteration: 98050 loss: 0.0020 lr: 0.02\n","iteration: 98100 loss: 0.0017 lr: 0.02\n","iteration: 98150 loss: 0.0017 lr: 0.02\n","iteration: 98200 loss: 0.0017 lr: 0.02\n","iteration: 98250 loss: 0.0017 lr: 0.02\n","iteration: 98300 loss: 0.0018 lr: 0.02\n","iteration: 98350 loss: 0.0019 lr: 0.02\n","iteration: 98400 loss: 0.0018 lr: 0.02\n","iteration: 98450 loss: 0.0017 lr: 0.02\n","iteration: 98500 loss: 0.0017 lr: 0.02\n","iteration: 98550 loss: 0.0017 lr: 0.02\n","iteration: 98600 loss: 0.0018 lr: 0.02\n","iteration: 98650 loss: 0.0019 lr: 0.02\n","iteration: 98700 loss: 0.0018 lr: 0.02\n","iteration: 98750 loss: 0.0019 lr: 0.02\n","iteration: 98800 loss: 0.0017 lr: 0.02\n","iteration: 98850 loss: 0.0018 lr: 0.02\n","iteration: 98900 loss: 0.0019 lr: 0.02\n","iteration: 98950 loss: 0.0018 lr: 0.02\n","iteration: 99000 loss: 0.0019 lr: 0.02\n","iteration: 99050 loss: 0.0018 lr: 0.02\n","iteration: 99100 loss: 0.0018 lr: 0.02\n","iteration: 99150 loss: 0.0016 lr: 0.02\n","iteration: 99200 loss: 0.0016 lr: 0.02\n","iteration: 99250 loss: 0.0017 lr: 0.02\n","iteration: 99300 loss: 0.0018 lr: 0.02\n","iteration: 99350 loss: 0.0017 lr: 0.02\n","iteration: 99400 loss: 0.0018 lr: 0.02\n","iteration: 99450 loss: 0.0017 lr: 0.02\n","iteration: 99500 loss: 0.0018 lr: 0.02\n","iteration: 99550 loss: 0.0018 lr: 0.02\n","iteration: 99600 loss: 0.0018 lr: 0.02\n","iteration: 99650 loss: 0.0016 lr: 0.02\n","iteration: 99700 loss: 0.0017 lr: 0.02\n","iteration: 99750 loss: 0.0017 lr: 0.02\n","iteration: 99800 loss: 0.0017 lr: 0.02\n","iteration: 99850 loss: 0.0016 lr: 0.02\n","iteration: 99900 loss: 0.0017 lr: 0.02\n","iteration: 99950 loss: 0.0018 lr: 0.02\n","iteration: 100000 loss: 0.0016 lr: 0.02\n","iteration: 100050 loss: 0.0017 lr: 0.02\n","iteration: 100100 loss: 0.0019 lr: 0.02\n","iteration: 100150 loss: 0.0018 lr: 0.02\n","iteration: 100200 loss: 0.0018 lr: 0.02\n","iteration: 100250 loss: 0.0017 lr: 0.02\n","iteration: 100300 loss: 0.0017 lr: 0.02\n","iteration: 100350 loss: 0.0017 lr: 0.02\n","iteration: 100400 loss: 0.0017 lr: 0.02\n","iteration: 100450 loss: 0.0016 lr: 0.02\n","iteration: 100500 loss: 0.0017 lr: 0.02\n","iteration: 100550 loss: 0.0017 lr: 0.02\n","iteration: 100600 loss: 0.0018 lr: 0.02\n","iteration: 100650 loss: 0.0018 lr: 0.02\n","iteration: 100700 loss: 0.0017 lr: 0.02\n","iteration: 100750 loss: 0.0018 lr: 0.02\n","iteration: 100800 loss: 0.0018 lr: 0.02\n","iteration: 100850 loss: 0.0018 lr: 0.02\n","iteration: 100900 loss: 0.0018 lr: 0.02\n","iteration: 100950 loss: 0.0017 lr: 0.02\n","iteration: 101000 loss: 0.0016 lr: 0.02\n","iteration: 101050 loss: 0.0016 lr: 0.02\n","iteration: 101100 loss: 0.0017 lr: 0.02\n","iteration: 101150 loss: 0.0017 lr: 0.02\n","iteration: 101200 loss: 0.0016 lr: 0.02\n","iteration: 101250 loss: 0.0017 lr: 0.02\n","iteration: 101300 loss: 0.0017 lr: 0.02\n","iteration: 101350 loss: 0.0017 lr: 0.02\n","iteration: 101400 loss: 0.0016 lr: 0.02\n","iteration: 101450 loss: 0.0017 lr: 0.02\n","iteration: 101500 loss: 0.0018 lr: 0.02\n","iteration: 101550 loss: 0.0017 lr: 0.02\n","iteration: 101600 loss: 0.0017 lr: 0.02\n","iteration: 101650 loss: 0.0016 lr: 0.02\n","iteration: 101700 loss: 0.0017 lr: 0.02\n","iteration: 101750 loss: 0.0016 lr: 0.02\n","iteration: 101800 loss: 0.0016 lr: 0.02\n","iteration: 101850 loss: 0.0017 lr: 0.02\n","iteration: 101900 loss: 0.0017 lr: 0.02\n","iteration: 101950 loss: 0.0017 lr: 0.02\n","iteration: 102000 loss: 0.0017 lr: 0.02\n","iteration: 102050 loss: 0.0018 lr: 0.02\n","iteration: 102100 loss: 0.0018 lr: 0.02\n","iteration: 102150 loss: 0.0016 lr: 0.02\n","iteration: 102200 loss: 0.0018 lr: 0.02\n","iteration: 102250 loss: 0.0017 lr: 0.02\n","iteration: 102300 loss: 0.0017 lr: 0.02\n","iteration: 102350 loss: 0.0017 lr: 0.02\n","iteration: 102400 loss: 0.0018 lr: 0.02\n","iteration: 102450 loss: 0.0016 lr: 0.02\n","iteration: 102500 loss: 0.0016 lr: 0.02\n","iteration: 102550 loss: 0.0017 lr: 0.02\n","iteration: 102600 loss: 0.0018 lr: 0.02\n","iteration: 102650 loss: 0.0017 lr: 0.02\n","iteration: 102700 loss: 0.0017 lr: 0.02\n","iteration: 102750 loss: 0.0017 lr: 0.02\n","iteration: 102800 loss: 0.0016 lr: 0.02\n","iteration: 102850 loss: 0.0016 lr: 0.02\n","iteration: 102900 loss: 0.0016 lr: 0.02\n","iteration: 102950 loss: 0.0017 lr: 0.02\n","iteration: 103000 loss: 0.0017 lr: 0.02\n","iteration: 103050 loss: 0.0017 lr: 0.02\n","iteration: 103100 loss: 0.0018 lr: 0.02\n","iteration: 103150 loss: 0.0018 lr: 0.02\n","iteration: 103200 loss: 0.0019 lr: 0.02\n","iteration: 103250 loss: 0.0018 lr: 0.02\n","iteration: 103300 loss: 0.0018 lr: 0.02\n","iteration: 103350 loss: 0.0017 lr: 0.02\n","iteration: 103400 loss: 0.0017 lr: 0.02\n","iteration: 103450 loss: 0.0017 lr: 0.02\n","iteration: 103500 loss: 0.0017 lr: 0.02\n","iteration: 103550 loss: 0.0018 lr: 0.02\n","iteration: 103600 loss: 0.0017 lr: 0.02\n","iteration: 103650 loss: 0.0018 lr: 0.02\n","iteration: 103700 loss: 0.0019 lr: 0.02\n","iteration: 103750 loss: 0.0017 lr: 0.02\n","iteration: 103800 loss: 0.0016 lr: 0.02\n","iteration: 103850 loss: 0.0017 lr: 0.02\n","iteration: 103900 loss: 0.0017 lr: 0.02\n","iteration: 103950 loss: 0.0016 lr: 0.02\n","iteration: 104000 loss: 0.0018 lr: 0.02\n","iteration: 104050 loss: 0.0017 lr: 0.02\n","iteration: 104100 loss: 0.0017 lr: 0.02\n","iteration: 104150 loss: 0.0016 lr: 0.02\n","iteration: 104200 loss: 0.0018 lr: 0.02\n","iteration: 104250 loss: 0.0018 lr: 0.02\n","iteration: 104300 loss: 0.0017 lr: 0.02\n","iteration: 104350 loss: 0.0016 lr: 0.02\n","iteration: 104400 loss: 0.0017 lr: 0.02\n","iteration: 104450 loss: 0.0017 lr: 0.02\n","iteration: 104500 loss: 0.0016 lr: 0.02\n","iteration: 104550 loss: 0.0017 lr: 0.02\n","iteration: 104600 loss: 0.0016 lr: 0.02\n","iteration: 104650 loss: 0.0017 lr: 0.02\n","iteration: 104700 loss: 0.0017 lr: 0.02\n","iteration: 104750 loss: 0.0018 lr: 0.02\n","iteration: 104800 loss: 0.0016 lr: 0.02\n","iteration: 104850 loss: 0.0017 lr: 0.02\n","iteration: 104900 loss: 0.0017 lr: 0.02\n","iteration: 104950 loss: 0.0017 lr: 0.02\n","iteration: 105000 loss: 0.0017 lr: 0.02\n","iteration: 105050 loss: 0.0016 lr: 0.02\n","iteration: 105100 loss: 0.0017 lr: 0.02\n","iteration: 105150 loss: 0.0017 lr: 0.02\n","iteration: 105200 loss: 0.0017 lr: 0.02\n","iteration: 105250 loss: 0.0017 lr: 0.02\n","iteration: 105300 loss: 0.0017 lr: 0.02\n","iteration: 105350 loss: 0.0016 lr: 0.02\n","iteration: 105400 loss: 0.0017 lr: 0.02\n","iteration: 105450 loss: 0.0016 lr: 0.02\n","iteration: 105500 loss: 0.0017 lr: 0.02\n","iteration: 105550 loss: 0.0017 lr: 0.02\n","iteration: 105600 loss: 0.0017 lr: 0.02\n","iteration: 105650 loss: 0.0017 lr: 0.02\n","iteration: 105700 loss: 0.0017 lr: 0.02\n","iteration: 105750 loss: 0.0017 lr: 0.02\n","iteration: 105800 loss: 0.0017 lr: 0.02\n","iteration: 105850 loss: 0.0017 lr: 0.02\n","iteration: 105900 loss: 0.0018 lr: 0.02\n","iteration: 105950 loss: 0.0017 lr: 0.02\n","iteration: 106000 loss: 0.0019 lr: 0.02\n","iteration: 106050 loss: 0.0016 lr: 0.02\n","iteration: 106100 loss: 0.0017 lr: 0.02\n","iteration: 106150 loss: 0.0017 lr: 0.02\n","iteration: 106200 loss: 0.0019 lr: 0.02\n","iteration: 106250 loss: 0.0018 lr: 0.02\n","iteration: 106300 loss: 0.0017 lr: 0.02\n","iteration: 106350 loss: 0.0016 lr: 0.02\n","iteration: 106400 loss: 0.0016 lr: 0.02\n","iteration: 106450 loss: 0.0017 lr: 0.02\n","iteration: 106500 loss: 0.0017 lr: 0.02\n","iteration: 106550 loss: 0.0017 lr: 0.02\n","iteration: 106600 loss: 0.0017 lr: 0.02\n","iteration: 106650 loss: 0.0017 lr: 0.02\n","iteration: 106700 loss: 0.0019 lr: 0.02\n","iteration: 106750 loss: 0.0017 lr: 0.02\n","iteration: 106800 loss: 0.0017 lr: 0.02\n","iteration: 106850 loss: 0.0017 lr: 0.02\n","iteration: 106900 loss: 0.0015 lr: 0.02\n","iteration: 106950 loss: 0.0017 lr: 0.02\n","iteration: 107000 loss: 0.0017 lr: 0.02\n","iteration: 107050 loss: 0.0017 lr: 0.02\n","iteration: 107100 loss: 0.0017 lr: 0.02\n","iteration: 107150 loss: 0.0016 lr: 0.02\n","iteration: 107200 loss: 0.0019 lr: 0.02\n","iteration: 107250 loss: 0.0018 lr: 0.02\n","iteration: 107300 loss: 0.0016 lr: 0.02\n","iteration: 107350 loss: 0.0017 lr: 0.02\n","iteration: 107400 loss: 0.0018 lr: 0.02\n","iteration: 107450 loss: 0.0017 lr: 0.02\n","iteration: 107500 loss: 0.0018 lr: 0.02\n","iteration: 107550 loss: 0.0017 lr: 0.02\n","iteration: 107600 loss: 0.0017 lr: 0.02\n","iteration: 107650 loss: 0.0018 lr: 0.02\n","iteration: 107700 loss: 0.0018 lr: 0.02\n","iteration: 107750 loss: 0.0017 lr: 0.02\n","iteration: 107800 loss: 0.0016 lr: 0.02\n","iteration: 107850 loss: 0.0018 lr: 0.02\n","iteration: 107900 loss: 0.0016 lr: 0.02\n","iteration: 107950 loss: 0.0017 lr: 0.02\n","iteration: 108000 loss: 0.0017 lr: 0.02\n","iteration: 108050 loss: 0.0016 lr: 0.02\n","iteration: 108100 loss: 0.0016 lr: 0.02\n","iteration: 108150 loss: 0.0016 lr: 0.02\n","iteration: 108200 loss: 0.0016 lr: 0.02\n","iteration: 108250 loss: 0.0016 lr: 0.02\n","iteration: 108300 loss: 0.0017 lr: 0.02\n","iteration: 108350 loss: 0.0017 lr: 0.02\n","iteration: 108400 loss: 0.0018 lr: 0.02\n","iteration: 108450 loss: 0.0016 lr: 0.02\n","iteration: 108500 loss: 0.0016 lr: 0.02\n","iteration: 108550 loss: 0.0017 lr: 0.02\n","iteration: 108600 loss: 0.0017 lr: 0.02\n","iteration: 108650 loss: 0.0018 lr: 0.02\n","iteration: 108700 loss: 0.0016 lr: 0.02\n","iteration: 108750 loss: 0.0019 lr: 0.02\n","iteration: 108800 loss: 0.0016 lr: 0.02\n","iteration: 108850 loss: 0.0017 lr: 0.02\n","iteration: 108900 loss: 0.0016 lr: 0.02\n","iteration: 108950 loss: 0.0017 lr: 0.02\n","iteration: 109000 loss: 0.0018 lr: 0.02\n","iteration: 109050 loss: 0.0017 lr: 0.02\n","iteration: 109100 loss: 0.0016 lr: 0.02\n","iteration: 109150 loss: 0.0017 lr: 0.02\n","iteration: 109200 loss: 0.0017 lr: 0.02\n","iteration: 109250 loss: 0.0017 lr: 0.02\n","iteration: 109300 loss: 0.0016 lr: 0.02\n","iteration: 109350 loss: 0.0017 lr: 0.02\n","iteration: 109400 loss: 0.0015 lr: 0.02\n","iteration: 109450 loss: 0.0017 lr: 0.02\n","iteration: 109500 loss: 0.0016 lr: 0.02\n","iteration: 109550 loss: 0.0016 lr: 0.02\n","iteration: 109600 loss: 0.0015 lr: 0.02\n","iteration: 109650 loss: 0.0017 lr: 0.02\n","iteration: 109700 loss: 0.0017 lr: 0.02\n","iteration: 109750 loss: 0.0018 lr: 0.02\n","iteration: 109800 loss: 0.0016 lr: 0.02\n","iteration: 109850 loss: 0.0016 lr: 0.02\n","iteration: 109900 loss: 0.0016 lr: 0.02\n","iteration: 109950 loss: 0.0016 lr: 0.02\n","iteration: 110000 loss: 0.0016 lr: 0.02\n","iteration: 110050 loss: 0.0017 lr: 0.02\n","iteration: 110100 loss: 0.0016 lr: 0.02\n","iteration: 110150 loss: 0.0017 lr: 0.02\n","iteration: 110200 loss: 0.0016 lr: 0.02\n","iteration: 110250 loss: 0.0017 lr: 0.02\n","iteration: 110300 loss: 0.0018 lr: 0.02\n","iteration: 110350 loss: 0.0017 lr: 0.02\n","iteration: 110400 loss: 0.0017 lr: 0.02\n","iteration: 110450 loss: 0.0016 lr: 0.02\n","iteration: 110500 loss: 0.0016 lr: 0.02\n","iteration: 110550 loss: 0.0016 lr: 0.02\n","iteration: 110600 loss: 0.0017 lr: 0.02\n","iteration: 110650 loss: 0.0017 lr: 0.02\n","iteration: 110700 loss: 0.0017 lr: 0.02\n","iteration: 110750 loss: 0.0016 lr: 0.02\n","iteration: 110800 loss: 0.0017 lr: 0.02\n","iteration: 110850 loss: 0.0017 lr: 0.02\n","iteration: 110900 loss: 0.0017 lr: 0.02\n","iteration: 110950 loss: 0.0016 lr: 0.02\n","iteration: 111000 loss: 0.0016 lr: 0.02\n","iteration: 111050 loss: 0.0016 lr: 0.02\n","iteration: 111100 loss: 0.0017 lr: 0.02\n","iteration: 111150 loss: 0.0017 lr: 0.02\n","iteration: 111200 loss: 0.0017 lr: 0.02\n","iteration: 111250 loss: 0.0016 lr: 0.02\n","iteration: 111300 loss: 0.0017 lr: 0.02\n","iteration: 111350 loss: 0.0016 lr: 0.02\n","iteration: 111400 loss: 0.0016 lr: 0.02\n","iteration: 111450 loss: 0.0017 lr: 0.02\n","iteration: 111500 loss: 0.0016 lr: 0.02\n","iteration: 111550 loss: 0.0016 lr: 0.02\n","iteration: 111600 loss: 0.0016 lr: 0.02\n","iteration: 111650 loss: 0.0017 lr: 0.02\n","iteration: 111700 loss: 0.0017 lr: 0.02\n","iteration: 111750 loss: 0.0016 lr: 0.02\n","iteration: 111800 loss: 0.0017 lr: 0.02\n","iteration: 111850 loss: 0.0016 lr: 0.02\n","iteration: 111900 loss: 0.0018 lr: 0.02\n","iteration: 111950 loss: 0.0017 lr: 0.02\n","iteration: 112000 loss: 0.0017 lr: 0.02\n","iteration: 112050 loss: 0.0017 lr: 0.02\n","iteration: 112100 loss: 0.0017 lr: 0.02\n","iteration: 112150 loss: 0.0018 lr: 0.02\n","iteration: 112200 loss: 0.0017 lr: 0.02\n","iteration: 112250 loss: 0.0016 lr: 0.02\n","iteration: 112300 loss: 0.0016 lr: 0.02\n","iteration: 112350 loss: 0.0017 lr: 0.02\n","iteration: 112400 loss: 0.0015 lr: 0.02\n","iteration: 112450 loss: 0.0016 lr: 0.02\n","iteration: 112500 loss: 0.0016 lr: 0.02\n","iteration: 112550 loss: 0.0016 lr: 0.02\n","iteration: 112600 loss: 0.0017 lr: 0.02\n","iteration: 112650 loss: 0.0015 lr: 0.02\n","iteration: 112700 loss: 0.0016 lr: 0.02\n","iteration: 112750 loss: 0.0016 lr: 0.02\n","iteration: 112800 loss: 0.0017 lr: 0.02\n","iteration: 112850 loss: 0.0015 lr: 0.02\n","iteration: 112900 loss: 0.0016 lr: 0.02\n","iteration: 112950 loss: 0.0016 lr: 0.02\n","iteration: 113000 loss: 0.0017 lr: 0.02\n","iteration: 113050 loss: 0.0018 lr: 0.02\n","iteration: 113100 loss: 0.0015 lr: 0.02\n","iteration: 113150 loss: 0.0016 lr: 0.02\n","iteration: 113200 loss: 0.0016 lr: 0.02\n","iteration: 113250 loss: 0.0017 lr: 0.02\n","iteration: 113300 loss: 0.0015 lr: 0.02\n","iteration: 113350 loss: 0.0016 lr: 0.02\n","iteration: 113400 loss: 0.0015 lr: 0.02\n","iteration: 113450 loss: 0.0018 lr: 0.02\n","iteration: 113500 loss: 0.0016 lr: 0.02\n","iteration: 113550 loss: 0.0016 lr: 0.02\n","iteration: 113600 loss: 0.0016 lr: 0.02\n","iteration: 113650 loss: 0.0015 lr: 0.02\n","iteration: 113700 loss: 0.0017 lr: 0.02\n","iteration: 113750 loss: 0.0016 lr: 0.02\n","iteration: 113800 loss: 0.0016 lr: 0.02\n","iteration: 113850 loss: 0.0016 lr: 0.02\n","iteration: 113900 loss: 0.0017 lr: 0.02\n","iteration: 113950 loss: 0.0017 lr: 0.02\n","iteration: 114000 loss: 0.0015 lr: 0.02\n","iteration: 114050 loss: 0.0017 lr: 0.02\n","iteration: 114100 loss: 0.0018 lr: 0.02\n","iteration: 114150 loss: 0.0016 lr: 0.02\n","iteration: 114200 loss: 0.0016 lr: 0.02\n","iteration: 114250 loss: 0.0017 lr: 0.02\n","iteration: 114300 loss: 0.0016 lr: 0.02\n","iteration: 114350 loss: 0.0017 lr: 0.02\n","iteration: 114400 loss: 0.0016 lr: 0.02\n","iteration: 114450 loss: 0.0015 lr: 0.02\n","iteration: 114500 loss: 0.0016 lr: 0.02\n","iteration: 114550 loss: 0.0015 lr: 0.02\n","iteration: 114600 loss: 0.0016 lr: 0.02\n","iteration: 114650 loss: 0.0016 lr: 0.02\n","iteration: 114700 loss: 0.0017 lr: 0.02\n","iteration: 114750 loss: 0.0016 lr: 0.02\n","iteration: 114800 loss: 0.0015 lr: 0.02\n","iteration: 114850 loss: 0.0016 lr: 0.02\n","iteration: 114900 loss: 0.0017 lr: 0.02\n","iteration: 114950 loss: 0.0016 lr: 0.02\n","iteration: 115000 loss: 0.0016 lr: 0.02\n","iteration: 115050 loss: 0.0016 lr: 0.02\n","iteration: 115100 loss: 0.0017 lr: 0.02\n","iteration: 115150 loss: 0.0017 lr: 0.02\n","iteration: 115200 loss: 0.0015 lr: 0.02\n","iteration: 115250 loss: 0.0015 lr: 0.02\n","iteration: 115300 loss: 0.0016 lr: 0.02\n","iteration: 115350 loss: 0.0015 lr: 0.02\n","iteration: 115400 loss: 0.0016 lr: 0.02\n","iteration: 115450 loss: 0.0016 lr: 0.02\n","iteration: 115500 loss: 0.0017 lr: 0.02\n","iteration: 115550 loss: 0.0016 lr: 0.02\n","iteration: 115600 loss: 0.0016 lr: 0.02\n","iteration: 115650 loss: 0.0017 lr: 0.02\n","iteration: 115700 loss: 0.0016 lr: 0.02\n","iteration: 115750 loss: 0.0017 lr: 0.02\n","iteration: 115800 loss: 0.0016 lr: 0.02\n","iteration: 115850 loss: 0.0015 lr: 0.02\n","iteration: 115900 loss: 0.0016 lr: 0.02\n","iteration: 115950 loss: 0.0017 lr: 0.02\n","iteration: 116000 loss: 0.0016 lr: 0.02\n","iteration: 116050 loss: 0.0016 lr: 0.02\n","iteration: 116100 loss: 0.0016 lr: 0.02\n","iteration: 116150 loss: 0.0017 lr: 0.02\n","iteration: 116200 loss: 0.0016 lr: 0.02\n","iteration: 116250 loss: 0.0016 lr: 0.02\n","iteration: 116300 loss: 0.0015 lr: 0.02\n","iteration: 116350 loss: 0.0015 lr: 0.02\n","iteration: 116400 loss: 0.0016 lr: 0.02\n","iteration: 116450 loss: 0.0016 lr: 0.02\n","iteration: 116500 loss: 0.0016 lr: 0.02\n","iteration: 116550 loss: 0.0017 lr: 0.02\n","iteration: 116600 loss: 0.0016 lr: 0.02\n","iteration: 116650 loss: 0.0016 lr: 0.02\n","iteration: 116700 loss: 0.0016 lr: 0.02\n","iteration: 116750 loss: 0.0016 lr: 0.02\n","iteration: 116800 loss: 0.0015 lr: 0.02\n","iteration: 116850 loss: 0.0015 lr: 0.02\n","iteration: 116900 loss: 0.0016 lr: 0.02\n","iteration: 116950 loss: 0.0015 lr: 0.02\n","iteration: 117000 loss: 0.0015 lr: 0.02\n","iteration: 117050 loss: 0.0016 lr: 0.02\n","iteration: 117100 loss: 0.0017 lr: 0.02\n","iteration: 117150 loss: 0.0016 lr: 0.02\n","iteration: 117200 loss: 0.0016 lr: 0.02\n","iteration: 117250 loss: 0.0015 lr: 0.02\n","iteration: 117300 loss: 0.0016 lr: 0.02\n","iteration: 117350 loss: 0.0016 lr: 0.02\n","iteration: 117400 loss: 0.0017 lr: 0.02\n","iteration: 117450 loss: 0.0016 lr: 0.02\n","iteration: 117500 loss: 0.0015 lr: 0.02\n","iteration: 117550 loss: 0.0016 lr: 0.02\n","iteration: 117600 loss: 0.0016 lr: 0.02\n","iteration: 117650 loss: 0.0016 lr: 0.02\n","iteration: 117700 loss: 0.0016 lr: 0.02\n","iteration: 117750 loss: 0.0017 lr: 0.02\n","iteration: 117800 loss: 0.0016 lr: 0.02\n","iteration: 117850 loss: 0.0015 lr: 0.02\n","iteration: 117900 loss: 0.0015 lr: 0.02\n","iteration: 117950 loss: 0.0016 lr: 0.02\n","iteration: 118000 loss: 0.0016 lr: 0.02\n","iteration: 118050 loss: 0.0018 lr: 0.02\n","iteration: 118100 loss: 0.0016 lr: 0.02\n","iteration: 118150 loss: 0.0015 lr: 0.02\n","iteration: 118200 loss: 0.0016 lr: 0.02\n","iteration: 118250 loss: 0.0016 lr: 0.02\n","iteration: 118300 loss: 0.0016 lr: 0.02\n","iteration: 118350 loss: 0.0016 lr: 0.02\n","iteration: 118400 loss: 0.0016 lr: 0.02\n","iteration: 118450 loss: 0.0015 lr: 0.02\n","iteration: 118500 loss: 0.0016 lr: 0.02\n","iteration: 118550 loss: 0.0017 lr: 0.02\n","iteration: 118600 loss: 0.0017 lr: 0.02\n","iteration: 118650 loss: 0.0017 lr: 0.02\n","iteration: 118700 loss: 0.0016 lr: 0.02\n","iteration: 118750 loss: 0.0015 lr: 0.02\n","iteration: 118800 loss: 0.0015 lr: 0.02\n","iteration: 118850 loss: 0.0016 lr: 0.02\n","iteration: 118900 loss: 0.0017 lr: 0.02\n","iteration: 118950 loss: 0.0016 lr: 0.02\n","iteration: 119000 loss: 0.0016 lr: 0.02\n","iteration: 119050 loss: 0.0016 lr: 0.02\n","iteration: 119100 loss: 0.0016 lr: 0.02\n","iteration: 119150 loss: 0.0016 lr: 0.02\n","iteration: 119200 loss: 0.0015 lr: 0.02\n","iteration: 119250 loss: 0.0016 lr: 0.02\n","iteration: 119300 loss: 0.0016 lr: 0.02\n","iteration: 119350 loss: 0.0017 lr: 0.02\n","iteration: 119400 loss: 0.0016 lr: 0.02\n","iteration: 119450 loss: 0.0017 lr: 0.02\n","iteration: 119500 loss: 0.0016 lr: 0.02\n","iteration: 119550 loss: 0.0016 lr: 0.02\n","iteration: 119600 loss: 0.0016 lr: 0.02\n","iteration: 119650 loss: 0.0016 lr: 0.02\n","iteration: 119700 loss: 0.0016 lr: 0.02\n","iteration: 119750 loss: 0.0016 lr: 0.02\n","iteration: 119800 loss: 0.0015 lr: 0.02\n","iteration: 119850 loss: 0.0015 lr: 0.02\n","iteration: 119900 loss: 0.0016 lr: 0.02\n","iteration: 119950 loss: 0.0017 lr: 0.02\n","iteration: 120000 loss: 0.0017 lr: 0.02\n","iteration: 120050 loss: 0.0015 lr: 0.02\n","iteration: 120100 loss: 0.0016 lr: 0.02\n","iteration: 120150 loss: 0.0015 lr: 0.02\n","iteration: 120200 loss: 0.0016 lr: 0.02\n","iteration: 120250 loss: 0.0016 lr: 0.02\n","iteration: 120300 loss: 0.0017 lr: 0.02\n","iteration: 120350 loss: 0.0017 lr: 0.02\n","iteration: 120400 loss: 0.0018 lr: 0.02\n","iteration: 120450 loss: 0.0016 lr: 0.02\n","iteration: 120500 loss: 0.0016 lr: 0.02\n","iteration: 120550 loss: 0.0015 lr: 0.02\n","iteration: 120600 loss: 0.0016 lr: 0.02\n","iteration: 120650 loss: 0.0016 lr: 0.02\n","iteration: 120700 loss: 0.0015 lr: 0.02\n","iteration: 120750 loss: 0.0016 lr: 0.02\n","iteration: 120800 loss: 0.0017 lr: 0.02\n","iteration: 120850 loss: 0.0017 lr: 0.02\n","iteration: 120900 loss: 0.0016 lr: 0.02\n","iteration: 120950 loss: 0.0017 lr: 0.02\n","iteration: 121000 loss: 0.0016 lr: 0.02\n","iteration: 121050 loss: 0.0015 lr: 0.02\n","iteration: 121100 loss: 0.0015 lr: 0.02\n","iteration: 121150 loss: 0.0016 lr: 0.02\n","iteration: 121200 loss: 0.0016 lr: 0.02\n","iteration: 121250 loss: 0.0017 lr: 0.02\n","iteration: 121300 loss: 0.0016 lr: 0.02\n","iteration: 121350 loss: 0.0016 lr: 0.02\n","iteration: 121400 loss: 0.0015 lr: 0.02\n","iteration: 121450 loss: 0.0016 lr: 0.02\n","iteration: 121500 loss: 0.0016 lr: 0.02\n","iteration: 121550 loss: 0.0015 lr: 0.02\n","iteration: 121600 loss: 0.0017 lr: 0.02\n","iteration: 121650 loss: 0.0017 lr: 0.02\n","iteration: 121700 loss: 0.0017 lr: 0.02\n","iteration: 121750 loss: 0.0015 lr: 0.02\n","iteration: 121800 loss: 0.0016 lr: 0.02\n","iteration: 121850 loss: 0.0016 lr: 0.02\n","iteration: 121900 loss: 0.0016 lr: 0.02\n","iteration: 121950 loss: 0.0015 lr: 0.02\n","iteration: 122000 loss: 0.0017 lr: 0.02\n","iteration: 122050 loss: 0.0016 lr: 0.02\n","iteration: 122100 loss: 0.0017 lr: 0.02\n","iteration: 122150 loss: 0.0015 lr: 0.02\n","iteration: 122200 loss: 0.0015 lr: 0.02\n","iteration: 122250 loss: 0.0016 lr: 0.02\n","iteration: 122300 loss: 0.0016 lr: 0.02\n","iteration: 122350 loss: 0.0016 lr: 0.02\n","iteration: 122400 loss: 0.0016 lr: 0.02\n","iteration: 122450 loss: 0.0016 lr: 0.02\n","iteration: 122500 loss: 0.0015 lr: 0.02\n","iteration: 122550 loss: 0.0016 lr: 0.02\n","iteration: 122600 loss: 0.0015 lr: 0.02\n","iteration: 122650 loss: 0.0016 lr: 0.02\n","iteration: 122700 loss: 0.0016 lr: 0.02\n","iteration: 122750 loss: 0.0016 lr: 0.02\n","iteration: 122800 loss: 0.0016 lr: 0.02\n","iteration: 122850 loss: 0.0016 lr: 0.02\n","iteration: 122900 loss: 0.0015 lr: 0.02\n","iteration: 122950 loss: 0.0014 lr: 0.02\n","iteration: 123000 loss: 0.0016 lr: 0.02\n","iteration: 123050 loss: 0.0017 lr: 0.02\n","iteration: 123100 loss: 0.0016 lr: 0.02\n","iteration: 123150 loss: 0.0016 lr: 0.02\n","iteration: 123200 loss: 0.0015 lr: 0.02\n","iteration: 123250 loss: 0.0017 lr: 0.02\n","iteration: 123300 loss: 0.0016 lr: 0.02\n","iteration: 123350 loss: 0.0015 lr: 0.02\n","iteration: 123400 loss: 0.0015 lr: 0.02\n","iteration: 123450 loss: 0.0015 lr: 0.02\n","iteration: 123500 loss: 0.0016 lr: 0.02\n","iteration: 123550 loss: 0.0015 lr: 0.02\n","iteration: 123600 loss: 0.0016 lr: 0.02\n","iteration: 123650 loss: 0.0016 lr: 0.02\n","iteration: 123700 loss: 0.0016 lr: 0.02\n","iteration: 123750 loss: 0.0016 lr: 0.02\n","iteration: 123800 loss: 0.0015 lr: 0.02\n","iteration: 123850 loss: 0.0015 lr: 0.02\n","iteration: 123900 loss: 0.0015 lr: 0.02\n","iteration: 123950 loss: 0.0015 lr: 0.02\n","iteration: 124000 loss: 0.0015 lr: 0.02\n","iteration: 124050 loss: 0.0015 lr: 0.02\n","iteration: 124100 loss: 0.0015 lr: 0.02\n","iteration: 124150 loss: 0.0016 lr: 0.02\n","iteration: 124200 loss: 0.0016 lr: 0.02\n","iteration: 124250 loss: 0.0015 lr: 0.02\n","iteration: 124300 loss: 0.0016 lr: 0.02\n","iteration: 124350 loss: 0.0016 lr: 0.02\n","iteration: 124400 loss: 0.0015 lr: 0.02\n","iteration: 124450 loss: 0.0015 lr: 0.02\n","iteration: 124500 loss: 0.0015 lr: 0.02\n","iteration: 124550 loss: 0.0016 lr: 0.02\n","iteration: 124600 loss: 0.0016 lr: 0.02\n","iteration: 124650 loss: 0.0015 lr: 0.02\n","iteration: 124700 loss: 0.0015 lr: 0.02\n","iteration: 124750 loss: 0.0016 lr: 0.02\n","iteration: 124800 loss: 0.0015 lr: 0.02\n","iteration: 124850 loss: 0.0015 lr: 0.02\n","iteration: 124900 loss: 0.0016 lr: 0.02\n","iteration: 124950 loss: 0.0015 lr: 0.02\n","iteration: 125000 loss: 0.0015 lr: 0.02\n","iteration: 125050 loss: 0.0016 lr: 0.02\n","iteration: 125100 loss: 0.0015 lr: 0.02\n","iteration: 125150 loss: 0.0016 lr: 0.02\n","iteration: 125200 loss: 0.0014 lr: 0.02\n","iteration: 125250 loss: 0.0016 lr: 0.02\n","iteration: 125300 loss: 0.0015 lr: 0.02\n","iteration: 125350 loss: 0.0015 lr: 0.02\n","iteration: 125400 loss: 0.0016 lr: 0.02\n","iteration: 125450 loss: 0.0016 lr: 0.02\n","iteration: 125500 loss: 0.0015 lr: 0.02\n","iteration: 125550 loss: 0.0016 lr: 0.02\n","iteration: 125600 loss: 0.0017 lr: 0.02\n","iteration: 125650 loss: 0.0016 lr: 0.02\n","iteration: 125700 loss: 0.0016 lr: 0.02\n","iteration: 125750 loss: 0.0016 lr: 0.02\n","iteration: 125800 loss: 0.0016 lr: 0.02\n","iteration: 125850 loss: 0.0015 lr: 0.02\n","iteration: 125900 loss: 0.0016 lr: 0.02\n","iteration: 125950 loss: 0.0016 lr: 0.02\n","iteration: 126000 loss: 0.0015 lr: 0.02\n","iteration: 126050 loss: 0.0016 lr: 0.02\n","iteration: 126100 loss: 0.0015 lr: 0.02\n","iteration: 126150 loss: 0.0015 lr: 0.02\n","iteration: 126200 loss: 0.0015 lr: 0.02\n","iteration: 126250 loss: 0.0017 lr: 0.02\n","iteration: 126300 loss: 0.0016 lr: 0.02\n","iteration: 126350 loss: 0.0016 lr: 0.02\n","iteration: 126400 loss: 0.0016 lr: 0.02\n","iteration: 126450 loss: 0.0015 lr: 0.02\n","iteration: 126500 loss: 0.0015 lr: 0.02\n","iteration: 126550 loss: 0.0016 lr: 0.02\n","iteration: 126600 loss: 0.0015 lr: 0.02\n","iteration: 126650 loss: 0.0016 lr: 0.02\n","iteration: 126700 loss: 0.0015 lr: 0.02\n","iteration: 126750 loss: 0.0016 lr: 0.02\n","iteration: 126800 loss: 0.0016 lr: 0.02\n","iteration: 126850 loss: 0.0014 lr: 0.02\n","iteration: 126900 loss: 0.0016 lr: 0.02\n","iteration: 126950 loss: 0.0015 lr: 0.02\n","iteration: 127000 loss: 0.0016 lr: 0.02\n","iteration: 127050 loss: 0.0015 lr: 0.02\n","iteration: 127100 loss: 0.0015 lr: 0.02\n","iteration: 127150 loss: 0.0016 lr: 0.02\n","iteration: 127200 loss: 0.0015 lr: 0.02\n","iteration: 127250 loss: 0.0016 lr: 0.02\n","iteration: 127300 loss: 0.0018 lr: 0.02\n","iteration: 127350 loss: 0.0016 lr: 0.02\n","iteration: 127400 loss: 0.0017 lr: 0.02\n","iteration: 127450 loss: 0.0016 lr: 0.02\n","iteration: 127500 loss: 0.0016 lr: 0.02\n","iteration: 127550 loss: 0.0016 lr: 0.02\n","iteration: 127600 loss: 0.0017 lr: 0.02\n","iteration: 127650 loss: 0.0016 lr: 0.02\n","iteration: 127700 loss: 0.0016 lr: 0.02\n","iteration: 127750 loss: 0.0016 lr: 0.02\n","iteration: 127800 loss: 0.0015 lr: 0.02\n","iteration: 127850 loss: 0.0016 lr: 0.02\n","iteration: 127900 loss: 0.0016 lr: 0.02\n","iteration: 127950 loss: 0.0015 lr: 0.02\n","iteration: 128000 loss: 0.0015 lr: 0.02\n","iteration: 128050 loss: 0.0016 lr: 0.02\n","iteration: 128100 loss: 0.0015 lr: 0.02\n","iteration: 128150 loss: 0.0016 lr: 0.02\n","iteration: 128200 loss: 0.0015 lr: 0.02\n","iteration: 128250 loss: 0.0015 lr: 0.02\n","iteration: 128300 loss: 0.0017 lr: 0.02\n","iteration: 128350 loss: 0.0016 lr: 0.02\n","iteration: 128400 loss: 0.0016 lr: 0.02\n","iteration: 128450 loss: 0.0015 lr: 0.02\n","iteration: 128500 loss: 0.0016 lr: 0.02\n","iteration: 128550 loss: 0.0015 lr: 0.02\n","iteration: 128600 loss: 0.0016 lr: 0.02\n","iteration: 128650 loss: 0.0015 lr: 0.02\n","iteration: 128700 loss: 0.0015 lr: 0.02\n","iteration: 128750 loss: 0.0015 lr: 0.02\n","iteration: 128800 loss: 0.0016 lr: 0.02\n","iteration: 128850 loss: 0.0016 lr: 0.02\n","iteration: 128900 loss: 0.0015 lr: 0.02\n","iteration: 128950 loss: 0.0016 lr: 0.02\n","iteration: 129000 loss: 0.0015 lr: 0.02\n","iteration: 129050 loss: 0.0014 lr: 0.02\n","iteration: 129100 loss: 0.0014 lr: 0.02\n","iteration: 129150 loss: 0.0015 lr: 0.02\n","iteration: 129200 loss: 0.0015 lr: 0.02\n","iteration: 129250 loss: 0.0015 lr: 0.02\n","iteration: 129300 loss: 0.0015 lr: 0.02\n","iteration: 129350 loss: 0.0015 lr: 0.02\n","iteration: 129400 loss: 0.0016 lr: 0.02\n","iteration: 129450 loss: 0.0015 lr: 0.02\n","iteration: 129500 loss: 0.0015 lr: 0.02\n","iteration: 129550 loss: 0.0016 lr: 0.02\n","iteration: 129600 loss: 0.0014 lr: 0.02\n","iteration: 129650 loss: 0.0014 lr: 0.02\n","iteration: 129700 loss: 0.0016 lr: 0.02\n","iteration: 129750 loss: 0.0017 lr: 0.02\n","iteration: 129800 loss: 0.0015 lr: 0.02\n","iteration: 129850 loss: 0.0016 lr: 0.02\n","iteration: 129900 loss: 0.0015 lr: 0.02\n","iteration: 129950 loss: 0.0015 lr: 0.02\n","iteration: 130000 loss: 0.0015 lr: 0.02\n","iteration: 130050 loss: 0.0015 lr: 0.02\n","iteration: 130100 loss: 0.0015 lr: 0.02\n","iteration: 130150 loss: 0.0015 lr: 0.02\n","iteration: 130200 loss: 0.0016 lr: 0.02\n","iteration: 130250 loss: 0.0016 lr: 0.02\n","iteration: 130300 loss: 0.0015 lr: 0.02\n","iteration: 130350 loss: 0.0015 lr: 0.02\n","iteration: 130400 loss: 0.0015 lr: 0.02\n","iteration: 130450 loss: 0.0016 lr: 0.02\n","iteration: 130500 loss: 0.0015 lr: 0.02\n","iteration: 130550 loss: 0.0016 lr: 0.02\n","iteration: 130600 loss: 0.0016 lr: 0.02\n","iteration: 130650 loss: 0.0015 lr: 0.02\n","iteration: 130700 loss: 0.0016 lr: 0.02\n","iteration: 130750 loss: 0.0015 lr: 0.02\n","iteration: 130800 loss: 0.0016 lr: 0.02\n","iteration: 130850 loss: 0.0016 lr: 0.02\n","iteration: 130900 loss: 0.0016 lr: 0.02\n","iteration: 130950 loss: 0.0016 lr: 0.02\n","iteration: 131000 loss: 0.0015 lr: 0.02\n","iteration: 131050 loss: 0.0016 lr: 0.02\n","iteration: 131100 loss: 0.0017 lr: 0.02\n","iteration: 131150 loss: 0.0014 lr: 0.02\n","iteration: 131200 loss: 0.0017 lr: 0.02\n","iteration: 131250 loss: 0.0015 lr: 0.02\n","iteration: 131300 loss: 0.0015 lr: 0.02\n","iteration: 131350 loss: 0.0016 lr: 0.02\n","iteration: 131400 loss: 0.0015 lr: 0.02\n","iteration: 131450 loss: 0.0016 lr: 0.02\n","iteration: 131500 loss: 0.0015 lr: 0.02\n","iteration: 131550 loss: 0.0015 lr: 0.02\n","iteration: 131600 loss: 0.0015 lr: 0.02\n","iteration: 131650 loss: 0.0015 lr: 0.02\n","iteration: 131700 loss: 0.0015 lr: 0.02\n","iteration: 131750 loss: 0.0015 lr: 0.02\n","iteration: 131800 loss: 0.0014 lr: 0.02\n","iteration: 131850 loss: 0.0015 lr: 0.02\n","iteration: 131900 loss: 0.0015 lr: 0.02\n","iteration: 131950 loss: 0.0016 lr: 0.02\n","iteration: 132000 loss: 0.0016 lr: 0.02\n","iteration: 132050 loss: 0.0014 lr: 0.02\n","iteration: 132100 loss: 0.0016 lr: 0.02\n","iteration: 132150 loss: 0.0015 lr: 0.02\n","iteration: 132200 loss: 0.0016 lr: 0.02\n","iteration: 132250 loss: 0.0017 lr: 0.02\n","iteration: 132300 loss: 0.0015 lr: 0.02\n","iteration: 132350 loss: 0.0014 lr: 0.02\n","iteration: 132400 loss: 0.0015 lr: 0.02\n","iteration: 132450 loss: 0.0014 lr: 0.02\n","iteration: 132500 loss: 0.0015 lr: 0.02\n","iteration: 132550 loss: 0.0016 lr: 0.02\n","iteration: 132600 loss: 0.0015 lr: 0.02\n","iteration: 132650 loss: 0.0015 lr: 0.02\n","iteration: 132700 loss: 0.0016 lr: 0.02\n","iteration: 132750 loss: 0.0015 lr: 0.02\n","iteration: 132800 loss: 0.0017 lr: 0.02\n","iteration: 132850 loss: 0.0016 lr: 0.02\n","iteration: 132900 loss: 0.0016 lr: 0.02\n","iteration: 132950 loss: 0.0016 lr: 0.02\n","iteration: 133000 loss: 0.0016 lr: 0.02\n","iteration: 133050 loss: 0.0016 lr: 0.02\n","iteration: 133100 loss: 0.0015 lr: 0.02\n","iteration: 133150 loss: 0.0015 lr: 0.02\n","iteration: 133200 loss: 0.0014 lr: 0.02\n","iteration: 133250 loss: 0.0015 lr: 0.02\n","iteration: 133300 loss: 0.0015 lr: 0.02\n","iteration: 133350 loss: 0.0016 lr: 0.02\n","iteration: 133400 loss: 0.0015 lr: 0.02\n","iteration: 133450 loss: 0.0015 lr: 0.02\n","iteration: 133500 loss: 0.0015 lr: 0.02\n","iteration: 133550 loss: 0.0015 lr: 0.02\n","iteration: 133600 loss: 0.0015 lr: 0.02\n","iteration: 133650 loss: 0.0018 lr: 0.02\n","iteration: 133700 loss: 0.0015 lr: 0.02\n","iteration: 133750 loss: 0.0016 lr: 0.02\n","iteration: 133800 loss: 0.0015 lr: 0.02\n","iteration: 133850 loss: 0.0015 lr: 0.02\n","iteration: 133900 loss: 0.0016 lr: 0.02\n","iteration: 133950 loss: 0.0015 lr: 0.02\n","iteration: 134000 loss: 0.0015 lr: 0.02\n","iteration: 134050 loss: 0.0016 lr: 0.02\n","iteration: 134100 loss: 0.0014 lr: 0.02\n","iteration: 134150 loss: 0.0013 lr: 0.02\n","iteration: 134200 loss: 0.0015 lr: 0.02\n","iteration: 134250 loss: 0.0016 lr: 0.02\n","iteration: 134300 loss: 0.0015 lr: 0.02\n","iteration: 134350 loss: 0.0015 lr: 0.02\n","iteration: 134400 loss: 0.0015 lr: 0.02\n","iteration: 134450 loss: 0.0015 lr: 0.02\n","iteration: 134500 loss: 0.0015 lr: 0.02\n","iteration: 134550 loss: 0.0015 lr: 0.02\n","iteration: 134600 loss: 0.0016 lr: 0.02\n","iteration: 134650 loss: 0.0015 lr: 0.02\n","iteration: 134700 loss: 0.0015 lr: 0.02\n","iteration: 134750 loss: 0.0015 lr: 0.02\n","iteration: 134800 loss: 0.0015 lr: 0.02\n","iteration: 134850 loss: 0.0015 lr: 0.02\n","iteration: 134900 loss: 0.0015 lr: 0.02\n","iteration: 134950 loss: 0.0016 lr: 0.02\n","iteration: 135000 loss: 0.0015 lr: 0.02\n","iteration: 135050 loss: 0.0016 lr: 0.02\n","iteration: 135100 loss: 0.0015 lr: 0.02\n","iteration: 135150 loss: 0.0015 lr: 0.02\n","iteration: 135200 loss: 0.0016 lr: 0.02\n","iteration: 135250 loss: 0.0015 lr: 0.02\n","iteration: 135300 loss: 0.0015 lr: 0.02\n","iteration: 135350 loss: 0.0015 lr: 0.02\n","iteration: 135400 loss: 0.0015 lr: 0.02\n","iteration: 135450 loss: 0.0015 lr: 0.02\n","iteration: 135500 loss: 0.0015 lr: 0.02\n","iteration: 135550 loss: 0.0016 lr: 0.02\n","iteration: 135600 loss: 0.0015 lr: 0.02\n","iteration: 135650 loss: 0.0014 lr: 0.02\n","iteration: 135700 loss: 0.0015 lr: 0.02\n","iteration: 135750 loss: 0.0015 lr: 0.02\n","iteration: 135800 loss: 0.0015 lr: 0.02\n","iteration: 135850 loss: 0.0015 lr: 0.02\n","iteration: 135900 loss: 0.0015 lr: 0.02\n","iteration: 135950 loss: 0.0014 lr: 0.02\n","iteration: 136000 loss: 0.0016 lr: 0.02\n","iteration: 136050 loss: 0.0015 lr: 0.02\n","iteration: 136100 loss: 0.0015 lr: 0.02\n","iteration: 136150 loss: 0.0014 lr: 0.02\n","iteration: 136200 loss: 0.0016 lr: 0.02\n","iteration: 136250 loss: 0.0015 lr: 0.02\n","iteration: 136300 loss: 0.0015 lr: 0.02\n","iteration: 136350 loss: 0.0015 lr: 0.02\n","iteration: 136400 loss: 0.0016 lr: 0.02\n","iteration: 136450 loss: 0.0015 lr: 0.02\n","iteration: 136500 loss: 0.0015 lr: 0.02\n","iteration: 136550 loss: 0.0015 lr: 0.02\n","iteration: 136600 loss: 0.0015 lr: 0.02\n","iteration: 136650 loss: 0.0015 lr: 0.02\n","iteration: 136700 loss: 0.0015 lr: 0.02\n","iteration: 136750 loss: 0.0015 lr: 0.02\n","iteration: 136800 loss: 0.0015 lr: 0.02\n","iteration: 136850 loss: 0.0014 lr: 0.02\n","iteration: 136900 loss: 0.0014 lr: 0.02\n","iteration: 136950 loss: 0.0015 lr: 0.02\n","iteration: 137000 loss: 0.0015 lr: 0.02\n","iteration: 137050 loss: 0.0014 lr: 0.02\n","iteration: 137100 loss: 0.0015 lr: 0.02\n","iteration: 137150 loss: 0.0015 lr: 0.02\n","iteration: 137200 loss: 0.0014 lr: 0.02\n","iteration: 137250 loss: 0.0015 lr: 0.02\n","iteration: 137300 loss: 0.0015 lr: 0.02\n","iteration: 137350 loss: 0.0015 lr: 0.02\n","iteration: 137400 loss: 0.0015 lr: 0.02\n","iteration: 137450 loss: 0.0015 lr: 0.02\n","iteration: 137500 loss: 0.0014 lr: 0.02\n","iteration: 137550 loss: 0.0017 lr: 0.02\n","iteration: 137600 loss: 0.0016 lr: 0.02\n","iteration: 137650 loss: 0.0015 lr: 0.02\n","iteration: 137700 loss: 0.0015 lr: 0.02\n","iteration: 137750 loss: 0.0014 lr: 0.02\n","iteration: 137800 loss: 0.0015 lr: 0.02\n","iteration: 137850 loss: 0.0016 lr: 0.02\n","iteration: 137900 loss: 0.0016 lr: 0.02\n","iteration: 137950 loss: 0.0016 lr: 0.02\n","iteration: 138000 loss: 0.0014 lr: 0.02\n","iteration: 138050 loss: 0.0015 lr: 0.02\n","iteration: 138100 loss: 0.0016 lr: 0.02\n","iteration: 138150 loss: 0.0015 lr: 0.02\n","iteration: 138200 loss: 0.0014 lr: 0.02\n","iteration: 138250 loss: 0.0014 lr: 0.02\n","iteration: 138300 loss: 0.0015 lr: 0.02\n","iteration: 138350 loss: 0.0014 lr: 0.02\n","iteration: 138400 loss: 0.0016 lr: 0.02\n","iteration: 138450 loss: 0.0015 lr: 0.02\n","iteration: 138500 loss: 0.0016 lr: 0.02\n","iteration: 138550 loss: 0.0015 lr: 0.02\n","iteration: 138600 loss: 0.0015 lr: 0.02\n","iteration: 138650 loss: 0.0014 lr: 0.02\n","iteration: 138700 loss: 0.0015 lr: 0.02\n","iteration: 138750 loss: 0.0014 lr: 0.02\n","iteration: 138800 loss: 0.0014 lr: 0.02\n","iteration: 138850 loss: 0.0015 lr: 0.02\n","iteration: 138900 loss: 0.0015 lr: 0.02\n","iteration: 138950 loss: 0.0015 lr: 0.02\n","iteration: 139000 loss: 0.0014 lr: 0.02\n","iteration: 139050 loss: 0.0014 lr: 0.02\n","iteration: 139100 loss: 0.0015 lr: 0.02\n","iteration: 139150 loss: 0.0014 lr: 0.02\n","iteration: 139200 loss: 0.0016 lr: 0.02\n","iteration: 139250 loss: 0.0015 lr: 0.02\n","iteration: 139300 loss: 0.0015 lr: 0.02\n","iteration: 139350 loss: 0.0015 lr: 0.02\n","iteration: 139400 loss: 0.0014 lr: 0.02\n","iteration: 139450 loss: 0.0014 lr: 0.02\n","iteration: 139500 loss: 0.0014 lr: 0.02\n","iteration: 139550 loss: 0.0014 lr: 0.02\n","iteration: 139600 loss: 0.0015 lr: 0.02\n","iteration: 139650 loss: 0.0015 lr: 0.02\n","iteration: 139700 loss: 0.0014 lr: 0.02\n","iteration: 139750 loss: 0.0014 lr: 0.02\n","iteration: 139800 loss: 0.0015 lr: 0.02\n","iteration: 139850 loss: 0.0016 lr: 0.02\n","iteration: 139900 loss: 0.0014 lr: 0.02\n","iteration: 139950 loss: 0.0015 lr: 0.02\n","iteration: 140000 loss: 0.0015 lr: 0.02\n","iteration: 140050 loss: 0.0015 lr: 0.02\n","iteration: 140100 loss: 0.0015 lr: 0.02\n","iteration: 140150 loss: 0.0015 lr: 0.02\n","iteration: 140200 loss: 0.0014 lr: 0.02\n","iteration: 140250 loss: 0.0015 lr: 0.02\n","iteration: 140300 loss: 0.0015 lr: 0.02\n","iteration: 140350 loss: 0.0015 lr: 0.02\n","iteration: 140400 loss: 0.0015 lr: 0.02\n","iteration: 140450 loss: 0.0014 lr: 0.02\n","iteration: 140500 loss: 0.0015 lr: 0.02\n","iteration: 140550 loss: 0.0015 lr: 0.02\n","iteration: 140600 loss: 0.0014 lr: 0.02\n","iteration: 140650 loss: 0.0017 lr: 0.02\n","iteration: 140700 loss: 0.0014 lr: 0.02\n","iteration: 140750 loss: 0.0015 lr: 0.02\n","iteration: 140800 loss: 0.0015 lr: 0.02\n","iteration: 140850 loss: 0.0015 lr: 0.02\n","iteration: 140900 loss: 0.0014 lr: 0.02\n","iteration: 140950 loss: 0.0014 lr: 0.02\n","iteration: 141000 loss: 0.0016 lr: 0.02\n","iteration: 141050 loss: 0.0015 lr: 0.02\n","iteration: 141100 loss: 0.0014 lr: 0.02\n","iteration: 141150 loss: 0.0015 lr: 0.02\n","iteration: 141200 loss: 0.0014 lr: 0.02\n","iteration: 141250 loss: 0.0014 lr: 0.02\n","iteration: 141300 loss: 0.0015 lr: 0.02\n","iteration: 141350 loss: 0.0014 lr: 0.02\n","iteration: 141400 loss: 0.0015 lr: 0.02\n","iteration: 141450 loss: 0.0015 lr: 0.02\n","iteration: 141500 loss: 0.0014 lr: 0.02\n","iteration: 141550 loss: 0.0014 lr: 0.02\n","iteration: 141600 loss: 0.0014 lr: 0.02\n","iteration: 141650 loss: 0.0014 lr: 0.02\n","iteration: 141700 loss: 0.0015 lr: 0.02\n","iteration: 141750 loss: 0.0015 lr: 0.02\n","iteration: 141800 loss: 0.0015 lr: 0.02\n","iteration: 141850 loss: 0.0016 lr: 0.02\n","iteration: 141900 loss: 0.0015 lr: 0.02\n","iteration: 141950 loss: 0.0015 lr: 0.02\n","iteration: 142000 loss: 0.0015 lr: 0.02\n","iteration: 142050 loss: 0.0014 lr: 0.02\n","iteration: 142100 loss: 0.0015 lr: 0.02\n","iteration: 142150 loss: 0.0015 lr: 0.02\n","iteration: 142200 loss: 0.0015 lr: 0.02\n","iteration: 142250 loss: 0.0015 lr: 0.02\n","iteration: 142300 loss: 0.0014 lr: 0.02\n","iteration: 142350 loss: 0.0016 lr: 0.02\n","iteration: 142400 loss: 0.0014 lr: 0.02\n","iteration: 142450 loss: 0.0015 lr: 0.02\n","iteration: 142500 loss: 0.0016 lr: 0.02\n","iteration: 142550 loss: 0.0016 lr: 0.02\n","iteration: 142600 loss: 0.0015 lr: 0.02\n","iteration: 142650 loss: 0.0014 lr: 0.02\n","iteration: 142700 loss: 0.0015 lr: 0.02\n","iteration: 142750 loss: 0.0015 lr: 0.02\n","iteration: 142800 loss: 0.0015 lr: 0.02\n","iteration: 142850 loss: 0.0014 lr: 0.02\n","iteration: 142900 loss: 0.0015 lr: 0.02\n","iteration: 142950 loss: 0.0015 lr: 0.02\n","iteration: 143000 loss: 0.0015 lr: 0.02\n","iteration: 143050 loss: 0.0015 lr: 0.02\n","iteration: 143100 loss: 0.0014 lr: 0.02\n","iteration: 143150 loss: 0.0015 lr: 0.02\n","iteration: 143200 loss: 0.0015 lr: 0.02\n","iteration: 143250 loss: 0.0016 lr: 0.02\n","iteration: 143300 loss: 0.0015 lr: 0.02\n","iteration: 143350 loss: 0.0014 lr: 0.02\n","iteration: 143400 loss: 0.0014 lr: 0.02\n","iteration: 143450 loss: 0.0015 lr: 0.02\n","iteration: 143500 loss: 0.0015 lr: 0.02\n","iteration: 143550 loss: 0.0015 lr: 0.02\n","iteration: 143600 loss: 0.0015 lr: 0.02\n","iteration: 143650 loss: 0.0014 lr: 0.02\n","iteration: 143700 loss: 0.0014 lr: 0.02\n","iteration: 143750 loss: 0.0014 lr: 0.02\n","iteration: 143800 loss: 0.0014 lr: 0.02\n","iteration: 143850 loss: 0.0015 lr: 0.02\n","iteration: 143900 loss: 0.0014 lr: 0.02\n","iteration: 143950 loss: 0.0015 lr: 0.02\n","iteration: 144000 loss: 0.0014 lr: 0.02\n","iteration: 144050 loss: 0.0014 lr: 0.02\n","iteration: 144100 loss: 0.0015 lr: 0.02\n","iteration: 144150 loss: 0.0015 lr: 0.02\n","iteration: 144200 loss: 0.0014 lr: 0.02\n","iteration: 144250 loss: 0.0016 lr: 0.02\n","iteration: 144300 loss: 0.0016 lr: 0.02\n","iteration: 144350 loss: 0.0015 lr: 0.02\n","iteration: 144400 loss: 0.0014 lr: 0.02\n","iteration: 144450 loss: 0.0014 lr: 0.02\n","iteration: 144500 loss: 0.0015 lr: 0.02\n","iteration: 144550 loss: 0.0014 lr: 0.02\n","iteration: 144600 loss: 0.0015 lr: 0.02\n","iteration: 144650 loss: 0.0015 lr: 0.02\n","iteration: 144700 loss: 0.0015 lr: 0.02\n","iteration: 144750 loss: 0.0014 lr: 0.02\n","iteration: 144800 loss: 0.0014 lr: 0.02\n","iteration: 144850 loss: 0.0014 lr: 0.02\n","iteration: 144900 loss: 0.0015 lr: 0.02\n","iteration: 144950 loss: 0.0014 lr: 0.02\n","iteration: 145000 loss: 0.0015 lr: 0.02\n","iteration: 145050 loss: 0.0014 lr: 0.02\n","iteration: 145100 loss: 0.0014 lr: 0.02\n","iteration: 145150 loss: 0.0014 lr: 0.02\n","iteration: 145200 loss: 0.0014 lr: 0.02\n","iteration: 145250 loss: 0.0014 lr: 0.02\n","iteration: 145300 loss: 0.0016 lr: 0.02\n","iteration: 145350 loss: 0.0014 lr: 0.02\n","iteration: 145400 loss: 0.0014 lr: 0.02\n","iteration: 145450 loss: 0.0014 lr: 0.02\n","iteration: 145500 loss: 0.0015 lr: 0.02\n","iteration: 145550 loss: 0.0014 lr: 0.02\n","iteration: 145600 loss: 0.0016 lr: 0.02\n","iteration: 145650 loss: 0.0014 lr: 0.02\n","iteration: 145700 loss: 0.0014 lr: 0.02\n","iteration: 145750 loss: 0.0013 lr: 0.02\n","iteration: 145800 loss: 0.0015 lr: 0.02\n","iteration: 145850 loss: 0.0014 lr: 0.02\n","iteration: 145900 loss: 0.0014 lr: 0.02\n","iteration: 145950 loss: 0.0015 lr: 0.02\n","iteration: 146000 loss: 0.0015 lr: 0.02\n","iteration: 146050 loss: 0.0014 lr: 0.02\n","iteration: 146100 loss: 0.0015 lr: 0.02\n","iteration: 146150 loss: 0.0015 lr: 0.02\n","iteration: 146200 loss: 0.0015 lr: 0.02\n","iteration: 146250 loss: 0.0014 lr: 0.02\n","iteration: 146300 loss: 0.0015 lr: 0.02\n","iteration: 146350 loss: 0.0014 lr: 0.02\n","iteration: 146400 loss: 0.0014 lr: 0.02\n","iteration: 146450 loss: 0.0015 lr: 0.02\n","iteration: 146500 loss: 0.0014 lr: 0.02\n","iteration: 146550 loss: 0.0014 lr: 0.02\n","iteration: 146600 loss: 0.0014 lr: 0.02\n","iteration: 146650 loss: 0.0015 lr: 0.02\n","iteration: 146700 loss: 0.0014 lr: 0.02\n","iteration: 146750 loss: 0.0014 lr: 0.02\n","iteration: 146800 loss: 0.0014 lr: 0.02\n","iteration: 146850 loss: 0.0015 lr: 0.02\n","iteration: 146900 loss: 0.0014 lr: 0.02\n","iteration: 146950 loss: 0.0015 lr: 0.02\n","iteration: 147000 loss: 0.0014 lr: 0.02\n","iteration: 147050 loss: 0.0015 lr: 0.02\n","iteration: 147100 loss: 0.0014 lr: 0.02\n","iteration: 147150 loss: 0.0015 lr: 0.02\n","iteration: 147200 loss: 0.0014 lr: 0.02\n","iteration: 147250 loss: 0.0014 lr: 0.02\n","iteration: 147300 loss: 0.0014 lr: 0.02\n","iteration: 147350 loss: 0.0014 lr: 0.02\n","iteration: 147400 loss: 0.0014 lr: 0.02\n","iteration: 147450 loss: 0.0014 lr: 0.02\n","iteration: 147500 loss: 0.0015 lr: 0.02\n","iteration: 147550 loss: 0.0014 lr: 0.02\n","iteration: 147600 loss: 0.0015 lr: 0.02\n","iteration: 147650 loss: 0.0015 lr: 0.02\n","iteration: 147700 loss: 0.0015 lr: 0.02\n","iteration: 147750 loss: 0.0015 lr: 0.02\n","iteration: 147800 loss: 0.0015 lr: 0.02\n","iteration: 147850 loss: 0.0014 lr: 0.02\n","iteration: 147900 loss: 0.0014 lr: 0.02\n","iteration: 147950 loss: 0.0014 lr: 0.02\n","iteration: 148000 loss: 0.0015 lr: 0.02\n","iteration: 148050 loss: 0.0015 lr: 0.02\n","iteration: 148100 loss: 0.0014 lr: 0.02\n","iteration: 148150 loss: 0.0014 lr: 0.02\n","iteration: 148200 loss: 0.0014 lr: 0.02\n","iteration: 148250 loss: 0.0013 lr: 0.02\n","iteration: 148300 loss: 0.0014 lr: 0.02\n","iteration: 148350 loss: 0.0014 lr: 0.02\n","iteration: 148400 loss: 0.0015 lr: 0.02\n","iteration: 148450 loss: 0.0015 lr: 0.02\n","iteration: 148500 loss: 0.0015 lr: 0.02\n","iteration: 148550 loss: 0.0014 lr: 0.02\n","iteration: 148600 loss: 0.0015 lr: 0.02\n","iteration: 148650 loss: 0.0015 lr: 0.02\n","iteration: 148700 loss: 0.0015 lr: 0.02\n","iteration: 148750 loss: 0.0015 lr: 0.02\n","iteration: 148800 loss: 0.0015 lr: 0.02\n","iteration: 148850 loss: 0.0015 lr: 0.02\n","iteration: 148900 loss: 0.0015 lr: 0.02\n","iteration: 148950 loss: 0.0015 lr: 0.02\n","iteration: 149000 loss: 0.0016 lr: 0.02\n","iteration: 149050 loss: 0.0014 lr: 0.02\n","iteration: 149100 loss: 0.0015 lr: 0.02\n","iteration: 149150 loss: 0.0014 lr: 0.02\n","iteration: 149200 loss: 0.0015 lr: 0.02\n","iteration: 149250 loss: 0.0015 lr: 0.02\n","iteration: 149300 loss: 0.0015 lr: 0.02\n","iteration: 149350 loss: 0.0014 lr: 0.02\n","iteration: 149400 loss: 0.0013 lr: 0.02\n","iteration: 149450 loss: 0.0014 lr: 0.02\n","iteration: 149500 loss: 0.0015 lr: 0.02\n","iteration: 149550 loss: 0.0014 lr: 0.02\n","iteration: 149600 loss: 0.0015 lr: 0.02\n","iteration: 149650 loss: 0.0014 lr: 0.02\n","iteration: 149700 loss: 0.0015 lr: 0.02\n","iteration: 149750 loss: 0.0014 lr: 0.02\n","iteration: 149800 loss: 0.0015 lr: 0.02\n","iteration: 149850 loss: 0.0015 lr: 0.02\n","iteration: 149900 loss: 0.0015 lr: 0.02\n","iteration: 149950 loss: 0.0016 lr: 0.02\n","iteration: 150000 loss: 0.0014 lr: 0.02\n","iteration: 150050 loss: 0.0014 lr: 0.02\n","iteration: 150100 loss: 0.0014 lr: 0.02\n","iteration: 150150 loss: 0.0015 lr: 0.02\n","iteration: 150200 loss: 0.0015 lr: 0.02\n","iteration: 150250 loss: 0.0014 lr: 0.02\n","iteration: 150300 loss: 0.0013 lr: 0.02\n","iteration: 150350 loss: 0.0014 lr: 0.02\n","iteration: 150400 loss: 0.0014 lr: 0.02\n","iteration: 150450 loss: 0.0014 lr: 0.02\n","Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","\n","Unfortunately, your original traceback can not be constructed.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-3-df25ed38d10b>\", line 27, in <module>\n","    model.dlc.train_network(config_path, saveiters=10000,displayiters=50,maxiters=300000,max_snapshots_to_keep=15, allow_growth=True)\n","  File \"/content/drive/My Drive/Development/DeepLabCut/deeplabcut/pose_estimation_tensorflow/training.py\", line 134, in train_network\n","    raise e\n","  File \"/content/drive/My Drive/Development/DeepLabCut/deeplabcut/pose_estimation_tensorflow/training.py\", line 132, in train_network\n","    train(str(poseconfigfile),displayiters,saveiters,maxiters,max_to_keep=max_snapshots_to_keep,keepdeconvweights=keepdeconvweights,allow_growth=allow_growth) #pass on path and file name for pose_cfg.yaml!\n","  File \"/content/drive/My Drive/Development/DeepLabCut/deeplabcut/pose_estimation_tensorflow/train.py\", line 190, in train\n","    feed_dict={learning_rate: current_lr})\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 956, in run\n","    run_metadata_ptr)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1180, in _run\n","    feed_dict_tensor, options, run_metadata)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1359, in _do_run\n","    run_metadata)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1365, in _do_call\n","    return fn(*args)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\n","    target_list, run_metadata)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun\n","    run_metadata)\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.6/inspect.py\", line 1452, in getframeinfo\n","    lines, lnum = findsource(frame)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 182, in findsource\n","    lines = linecache.getlines(file, globals_dict)\n","  File \"/usr/lib/python3.6/linecache.py\", line 47, in getlines\n","    return updatecache(filename, module_globals)\n","  File \"/usr/lib/python3.6/linecache.py\", line 136, in updatecache\n","    with tokenize.open(fullname) as fp:\n","  File \"/usr/lib/python3.6/tokenize.py\", line 454, in open\n","    encoding, lines = detect_encoding(buffer.readline)\n","  File \"/usr/lib/python3.6/tokenize.py\", line 423, in detect_encoding\n","    first = read_or_stop()\n","  File \"/usr/lib/python3.6/tokenize.py\", line 381, in read_or_stop\n","    return readline()\n","KeyboardInterrupt\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"]}]},{"cell_type":"code","metadata":{"id":"PO29RI0BjLvW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1596640526824,"user_tz":240,"elapsed":441104,"user":{"displayName":"Phil Fahn-Lai","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgaLRzhwWuE2_ZNOkUUkb6lkk-noGiIsb7ucOzW=s64","userId":"13631433551768862211"}},"outputId":"004037b8-ea64-48d1-efb9-d7a52d43f5b5"},"source":["model.evaluateAndAnalyze()\n","# model.getOutliers(20, outlier_algo='jump') \n","alert_done()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 576x288 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["Config:\n","{'all_joints': [[0],\n","                [1],\n","                [2],\n","                [3],\n","                [4],\n","                [5],\n","                [6],\n","                [7],\n","                [8],\n","                [9],\n","                [10],\n","                [11],\n","                [12],\n","                [13],\n","                [14],\n","                [15],\n","                [16],\n","                [17],\n","                [18],\n","                [19],\n","                [20],\n","                [21],\n","                [22],\n","                [23],\n","                [24],\n","                [25],\n","                [26],\n","                [27],\n","                [28],\n","                [29],\n","                [30],\n","                [31],\n","                [32],\n","                [33],\n","                [34],\n","                [35],\n","                [36],\n","                [37],\n","                [38],\n","                [39],\n","                [40],\n","                [41],\n","                [42],\n","                [43]],\n"," 'all_joints_names': ['Body_ds1_crn_cam1',\n","                      'Body_ds1_crn_cam2',\n","                      'Body_ds2_int_cam1',\n","                      'Body_ds2_int_cam2',\n","                      'Body_ds3_cdl_cam1',\n","                      'Body_ds3_cdl_cam2',\n","                      'Body_vn1_crn_cam1',\n","                      'Body_vn1_crn_cam2',\n","                      'Body_vn2_int_cam1',\n","                      'Body_vn2_int_cam2',\n","                      'Body_vn3_cdl_cam1',\n","                      'Body_vn3_cdl_cam2',\n","                      'Scapula_acr_cam1',\n","                      'Scapula_acr_cam2',\n","                      'Scapula_spi_cam1',\n","                      'Scapula_spi_cam2',\n","                      'Scapula_vtb_cam1',\n","                      'Scapula_vtb_cam2',\n","                      'Humerus_dpc_cam1',\n","                      'Humerus_dpc_cam2',\n","                      'Humerus_ent_cam1',\n","                      'Humerus_ent_cam2',\n","                      'Humerus_ect_cam1',\n","                      'Humerus_ect_cam2',\n","                      'Ulna_olc_cam1',\n","                      'Ulna_olc_cam2',\n","                      'Ulna_int_cam1',\n","                      'Ulna_int_cam2',\n","                      'Ulna_dst_cam1',\n","                      'Ulna_dst_cam2',\n","                      'Radius_prx_cam1',\n","                      'Radius_prx_cam2',\n","                      'Radius_int_cam1',\n","                      'Radius_int_cam2',\n","                      'Radius_dst_cam1',\n","                      'Radius_dst_cam2',\n","                      'Teres_maj_prx_cam1',\n","                      'Teres_maj_prx_cam2',\n","                      'Teres_maj_dst_cam1',\n","                      'Teres_maj_dst_cam2',\n","                      'Biceps_prx_cam1',\n","                      'Biceps_prx_cam2',\n","                      'Biceps_dst_cam1',\n","                      'Biceps_dst_cam2'],\n"," 'batch_size': 8,\n"," 'bottomheight': 400,\n"," 'crop': True,\n"," 'crop_pad': 0,\n"," 'cropratio': 0.4,\n"," 'dataset': 'training-datasets/iteration-4/UnaugmentedDataSet_possum101_11AprApr13/possum101_11Apr_Phil95shuffle1.mat',\n"," 'dataset_type': 'imgaug',\n"," 'deconvolutionstride': 2,\n"," 'deterministic': False,\n"," 'display_iters': 1000,\n"," 'fg_fraction': 0.25,\n"," 'global_scale': 0.8,\n"," 'init_weights': '/content/drive/My '\n","                 'Drive/Development/DeepLabCut/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',\n"," 'intermediate_supervision': False,\n"," 'intermediate_supervision_layer': 12,\n"," 'leftwidth': 400,\n"," 'location_refinement': True,\n"," 'locref_huber_loss': True,\n"," 'locref_loss_weight': 0.05,\n"," 'locref_stdev': 7.2801,\n"," 'log_dir': 'log',\n"," 'max_input_size': 1500,\n"," 'mean_pixel': [123.68, 116.779, 103.939],\n"," 'metadataset': 'training-datasets/iteration-4/UnaugmentedDataSet_possum101_11AprApr13/Documentation_data-possum101_11Apr_95shuffle1.pickle',\n"," 'min_input_size': 64,\n"," 'minsize': 100,\n"," 'mirror': False,\n"," 'multi_step': [[0.005, 10000],\n","                [0.02, 430000],\n","                [0.002, 730000],\n","                [0.001, 1030000]],\n"," 'net_type': 'resnet_50',\n"," 'num_joints': 44,\n"," 'num_outputs': 1,\n"," 'optimizer': 'sgd',\n"," 'output_stride': 16,\n"," 'pos_dist_thresh': 17,\n"," 'project_path': '/content/drive/My '\n","                 'Drive/Development/DeepLabCut/dev/possum101_11Apr-Phil-2020-04-13-diff',\n"," 'regularize': False,\n"," 'rightwidth': 400,\n"," 'save_iters': 50000,\n"," 'scale_jitter_lo': 0.5,\n"," 'scale_jitter_up': 1.25,\n"," 'scoremap_dir': 'test',\n"," 'shuffle': True,\n"," 'snapshot_prefix': '/content/drive/My '\n","                    'Drive/Development/DeepLabCut/dev/possum101_11Apr-Phil-2020-04-13-diff/dlc-models/iteration-4/possum101_11AprApr13-trainset95shuffle1/test/snapshot',\n"," 'stride': 8.0,\n"," 'topheight': 400,\n"," 'weigh_negatives': False,\n"," 'weigh_only_present_joints': False,\n"," 'weigh_part_predictions': False,\n"," 'weight_decay': 0.0001}\n"],"name":"stderr"},{"output_type":"stream","text":["/content/drive/My Drive/Development/DeepLabCut/dev/possum101_11Apr-Phil-2020-04-13-diff/evaluation-results/  already exists!\n","/content/drive/My Drive/Development/DeepLabCut/dev/possum101_11Apr-Phil-2020-04-13-diff/evaluation-results/iteration-4/possum101_11AprApr13-trainset95shuffle1  already exists!\n","Running  DLC_resnet50_possum101_11AprApr13shuffle1_150000  with # of trainingiterations: 150000\n","This net has already been evaluated!\n"],"name":"stdout"},{"output_type":"stream","text":["\n","  0%|          | 0/75 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Plotting...(attention scale might be inconsistent in comparison to when data was analyzed; i.e. if you used rescale)\n","/content/drive/My Drive/Development/DeepLabCut/dev/possum101_11Apr-Phil-2020-04-13-diff/evaluation-results/iteration-4/possum101_11AprApr13-trainset95shuffle1/LabeledImages_DLC_resnet50_possum101_11AprApr13shuffle1_150000_snapshot-150000  already exists!\n"],"name":"stdout"},{"output_type":"stream","text":["\n","  1%|▏         | 1/75 [00:00<00:43,  1.69it/s]\u001b[A\n","  3%|▎         | 2/75 [00:01<00:52,  1.39it/s]\u001b[A\n","  4%|▍         | 3/75 [00:02<00:58,  1.24it/s]\u001b[A\n","  5%|▌         | 4/75 [00:03<01:01,  1.15it/s]\u001b[A\n","  7%|▋         | 5/75 [00:04<01:04,  1.08it/s]\u001b[A\n","  8%|▊         | 6/75 [00:05<01:06,  1.03it/s]\u001b[A\n","  9%|▉         | 7/75 [00:07<01:24,  1.25s/it]\u001b[A\n"," 11%|█         | 8/75 [00:08<01:19,  1.19s/it]\u001b[A\n"," 12%|█▏        | 9/75 [00:09<01:15,  1.14s/it]\u001b[A\n"," 13%|█▎        | 10/75 [00:10<01:12,  1.11s/it]\u001b[A\n"," 15%|█▍        | 11/75 [00:11<01:09,  1.09s/it]\u001b[A\n"," 16%|█▌        | 12/75 [00:12<01:07,  1.08s/it]\u001b[A\n"," 17%|█▋        | 13/75 [00:13<01:06,  1.07s/it]\u001b[A\n"," 19%|█▊        | 14/75 [00:14<01:05,  1.07s/it]\u001b[A\n"," 20%|██        | 15/75 [00:16<01:03,  1.06s/it]\u001b[A\n"," 21%|██▏       | 16/75 [00:17<01:02,  1.05s/it]\u001b[A\n"," 23%|██▎       | 17/75 [00:18<01:00,  1.04s/it]\u001b[A\n"," 24%|██▍       | 18/75 [00:19<00:59,  1.04s/it]\u001b[A\n"," 25%|██▌       | 19/75 [00:20<01:00,  1.08s/it]\u001b[A\n"," 27%|██▋       | 20/75 [00:21<00:58,  1.06s/it]\u001b[A\n"," 28%|██▊       | 21/75 [00:22<01:00,  1.11s/it]\u001b[A\n"," 29%|██▉       | 22/75 [00:23<00:57,  1.08s/it]\u001b[A\n"," 31%|███       | 23/75 [00:24<00:55,  1.06s/it]\u001b[A\n"," 32%|███▏      | 24/75 [00:25<00:53,  1.06s/it]\u001b[A\n"," 33%|███▎      | 25/75 [00:26<00:52,  1.05s/it]\u001b[A\n"," 35%|███▍      | 26/75 [00:27<00:44,  1.10it/s]\u001b[A\n"," 36%|███▌      | 27/75 [00:28<00:45,  1.07it/s]\u001b[A\n"," 37%|███▋      | 28/75 [00:29<00:45,  1.04it/s]\u001b[A\n"," 39%|███▊      | 29/75 [00:30<00:44,  1.03it/s]\u001b[A\n"," 40%|████      | 30/75 [00:31<00:44,  1.00it/s]\u001b[A\n"," 41%|████▏     | 31/75 [00:32<00:44,  1.00s/it]\u001b[A\n"," 43%|████▎     | 32/75 [00:33<00:43,  1.01s/it]\u001b[A\n"," 44%|████▍     | 33/75 [00:34<00:42,  1.00s/it]\u001b[A\n"," 45%|████▌     | 34/75 [00:35<00:41,  1.01s/it]\u001b[A\n"," 47%|████▋     | 35/75 [00:36<00:40,  1.01s/it]\u001b[A\n"," 48%|████▊     | 36/75 [00:37<00:39,  1.01s/it]\u001b[A\n"," 49%|████▉     | 37/75 [00:38<00:38,  1.01s/it]\u001b[A\n"," 51%|█████     | 38/75 [00:39<00:37,  1.01s/it]\u001b[A\n"," 52%|█████▏    | 39/75 [00:40<00:39,  1.10s/it]\u001b[A\n"," 53%|█████▎    | 40/75 [00:41<00:37,  1.07s/it]\u001b[A\n"," 55%|█████▍    | 41/75 [00:42<00:36,  1.06s/it]\u001b[A\n"," 56%|█████▌    | 42/75 [00:43<00:34,  1.04s/it]\u001b[A\n"," 57%|█████▋    | 43/75 [00:44<00:33,  1.04s/it]\u001b[A\n"," 59%|█████▊    | 44/75 [00:45<00:31,  1.03s/it]\u001b[A\n"," 60%|██████    | 45/75 [00:46<00:30,  1.02s/it]\u001b[A\n"," 61%|██████▏   | 46/75 [00:47<00:29,  1.02s/it]\u001b[A\n"," 63%|██████▎   | 47/75 [00:48<00:28,  1.02s/it]\u001b[A\n"," 64%|██████▍   | 48/75 [00:49<00:27,  1.01s/it]\u001b[A\n"," 65%|██████▌   | 49/75 [00:50<00:26,  1.01s/it]\u001b[A\n"," 67%|██████▋   | 50/75 [00:51<00:25,  1.01s/it]\u001b[A\n"," 68%|██████▊   | 51/75 [00:52<00:24,  1.01s/it]\u001b[A\n"," 69%|██████▉   | 52/75 [00:53<00:24,  1.05s/it]\u001b[A\n"," 71%|███████   | 53/75 [00:54<00:22,  1.04s/it]\u001b[A\n"," 72%|███████▏  | 54/75 [00:56<00:21,  1.03s/it]\u001b[A\n"," 73%|███████▎  | 55/75 [00:57<00:20,  1.02s/it]\u001b[A\n"," 75%|███████▍  | 56/75 [00:58<00:19,  1.02s/it]\u001b[A\n"," 76%|███████▌  | 57/75 [00:59<00:18,  1.01s/it]\u001b[A\n"," 77%|███████▋  | 58/75 [01:00<00:17,  1.01s/it]\u001b[A\n"," 79%|███████▊  | 59/75 [01:01<00:16,  1.02s/it]\u001b[A\n"," 80%|████████  | 60/75 [01:02<00:15,  1.01s/it]\u001b[A\n"," 81%|████████▏ | 61/75 [01:03<00:15,  1.12s/it]\u001b[A\n"," 83%|████████▎ | 62/75 [01:04<00:14,  1.08s/it]\u001b[A\n"," 84%|████████▍ | 63/75 [01:05<00:12,  1.08s/it]\u001b[A\n"," 85%|████████▌ | 64/75 [01:06<00:11,  1.06s/it]\u001b[A\n"," 87%|████████▋ | 65/75 [01:07<00:10,  1.04s/it]\u001b[A\n"," 88%|████████▊ | 66/75 [01:08<00:09,  1.03s/it]\u001b[A\n"," 89%|████████▉ | 67/75 [01:09<00:08,  1.03s/it]\u001b[A\n"," 91%|█████████ | 68/75 [01:10<00:07,  1.03s/it]\u001b[A\n"," 92%|█████████▏| 69/75 [01:11<00:06,  1.02s/it]\u001b[A\n"," 93%|█████████▎| 70/75 [01:12<00:05,  1.02s/it]\u001b[A\n"," 95%|█████████▍| 71/75 [01:13<00:04,  1.02s/it]\u001b[A\n"," 96%|█████████▌| 72/75 [01:14<00:03,  1.02s/it]\u001b[A\n"," 97%|█████████▋| 73/75 [01:15<00:02,  1.06s/it]\u001b[A\n"," 99%|█████████▊| 74/75 [01:16<00:01,  1.05s/it]\u001b[A\n","100%|██████████| 75/75 [01:17<00:00,  1.04s/it]\n","/content/drive/My Drive/Development/DeepLabCut/deadROMM/possumPolish.py:261: UnsafeLoaderWarning: \n","The default 'Loader' for 'load(stream)' without further arguments can be unsafe.\n","Use 'load(stream, Loader=ruamel.yaml.Loader)' explicitly if that is OK.\n","Alternatively include the following in your code:\n","\n","  import warnings\n","  warnings.simplefilter('ignore', ruamel.yaml.error.UnsafeLoaderWarning)\n","\n","In most other cases you should consider using 'safe_load(stream)'\n","  self.config = ruamel.yaml.load(open(self.yaml))\n"],"name":"stderr"},{"output_type":"stream","text":["Function <function evaluate_network at 0x7f7486e09400> created 0 new files in directory ./dev/possum101_11Apr-Phil-2020-04-13-diff/evaluation-results\n"],"name":"stdout"},{"output_type":"stream","text":["Config:\n","{'all_joints': [[0],\n","                [1],\n","                [2],\n","                [3],\n","                [4],\n","                [5],\n","                [6],\n","                [7],\n","                [8],\n","                [9],\n","                [10],\n","                [11],\n","                [12],\n","                [13],\n","                [14],\n","                [15],\n","                [16],\n","                [17],\n","                [18],\n","                [19],\n","                [20],\n","                [21],\n","                [22],\n","                [23],\n","                [24],\n","                [25],\n","                [26],\n","                [27],\n","                [28],\n","                [29],\n","                [30],\n","                [31],\n","                [32],\n","                [33],\n","                [34],\n","                [35],\n","                [36],\n","                [37],\n","                [38],\n","                [39],\n","                [40],\n","                [41],\n","                [42],\n","                [43]],\n"," 'all_joints_names': ['Body_ds1_crn_cam1',\n","                      'Body_ds1_crn_cam2',\n","                      'Body_ds2_int_cam1',\n","                      'Body_ds2_int_cam2',\n","                      'Body_ds3_cdl_cam1',\n","                      'Body_ds3_cdl_cam2',\n","                      'Body_vn1_crn_cam1',\n","                      'Body_vn1_crn_cam2',\n","                      'Body_vn2_int_cam1',\n","                      'Body_vn2_int_cam2',\n","                      'Body_vn3_cdl_cam1',\n","                      'Body_vn3_cdl_cam2',\n","                      'Scapula_acr_cam1',\n","                      'Scapula_acr_cam2',\n","                      'Scapula_spi_cam1',\n","                      'Scapula_spi_cam2',\n","                      'Scapula_vtb_cam1',\n","                      'Scapula_vtb_cam2',\n","                      'Humerus_dpc_cam1',\n","                      'Humerus_dpc_cam2',\n","                      'Humerus_ent_cam1',\n","                      'Humerus_ent_cam2',\n","                      'Humerus_ect_cam1',\n","                      'Humerus_ect_cam2',\n","                      'Ulna_olc_cam1',\n","                      'Ulna_olc_cam2',\n","                      'Ulna_int_cam1',\n","                      'Ulna_int_cam2',\n","                      'Ulna_dst_cam1',\n","                      'Ulna_dst_cam2',\n","                      'Radius_prx_cam1',\n","                      'Radius_prx_cam2',\n","                      'Radius_int_cam1',\n","                      'Radius_int_cam2',\n","                      'Radius_dst_cam1',\n","                      'Radius_dst_cam2',\n","                      'Teres_maj_prx_cam1',\n","                      'Teres_maj_prx_cam2',\n","                      'Teres_maj_dst_cam1',\n","                      'Teres_maj_dst_cam2',\n","                      'Biceps_prx_cam1',\n","                      'Biceps_prx_cam2',\n","                      'Biceps_dst_cam1',\n","                      'Biceps_dst_cam2'],\n"," 'batch_size': 1,\n"," 'bottomheight': 400,\n"," 'crop': True,\n"," 'crop_pad': 0,\n"," 'cropratio': 0.4,\n"," 'dataset': 'training-datasets/iteration-4/UnaugmentedDataSet_possum101_11AprApr13/possum101_11Apr_Phil95shuffle1.mat',\n"," 'dataset_type': 'imgaug',\n"," 'deconvolutionstride': 2,\n"," 'deterministic': False,\n"," 'display_iters': 1000,\n"," 'fg_fraction': 0.25,\n"," 'global_scale': 0.8,\n"," 'init_weights': '/content/drive/My '\n","                 'Drive/Development/DeepLabCut/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',\n"," 'intermediate_supervision': False,\n"," 'intermediate_supervision_layer': 12,\n"," 'leftwidth': 400,\n"," 'location_refinement': True,\n"," 'locref_huber_loss': True,\n"," 'locref_loss_weight': 0.05,\n"," 'locref_stdev': 7.2801,\n"," 'log_dir': 'log',\n"," 'max_input_size': 1500,\n"," 'mean_pixel': [123.68, 116.779, 103.939],\n"," 'metadataset': 'training-datasets/iteration-4/UnaugmentedDataSet_possum101_11AprApr13/Documentation_data-possum101_11Apr_95shuffle1.pickle',\n"," 'min_input_size': 64,\n"," 'minsize': 100,\n"," 'mirror': False,\n"," 'multi_step': [[0.005, 10000],\n","                [0.02, 430000],\n","                [0.002, 730000],\n","                [0.001, 1030000]],\n"," 'net_type': 'resnet_50',\n"," 'num_joints': 44,\n"," 'num_outputs': 1,\n"," 'optimizer': 'sgd',\n"," 'output_stride': 16,\n"," 'pos_dist_thresh': 17,\n"," 'project_path': '/content/drive/My '\n","                 'Drive/Development/DeepLabCut/dev/possum101_11Apr-Phil-2020-04-13-diff',\n"," 'regularize': False,\n"," 'rightwidth': 400,\n"," 'save_iters': 50000,\n"," 'scale_jitter_lo': 0.5,\n"," 'scale_jitter_up': 1.25,\n"," 'scoremap_dir': 'test',\n"," 'shuffle': True,\n"," 'snapshot_prefix': '/content/drive/My '\n","                    'Drive/Development/DeepLabCut/dev/possum101_11Apr-Phil-2020-04-13-diff/dlc-models/iteration-4/possum101_11AprApr13-trainset95shuffle1/test/snapshot',\n"," 'stride': 8.0,\n"," 'topheight': 400,\n"," 'weigh_negatives': False,\n"," 'weigh_only_present_joints': False,\n"," 'weigh_part_predictions': False,\n"," 'weight_decay': 0.0001}\n"],"name":"stderr"},{"output_type":"stream","text":["Updated config.yaml with event evaluation at 05Aug20_15h08m06s\n","Using snapshot-150000 for model /content/drive/My Drive/Development/DeepLabCut/dev/possum101_11Apr-Phil-2020-04-13-diff/dlc-models/iteration-4/possum101_11AprApr13-trainset95shuffle1\n","Initializing ResNet\n","INFO:tensorflow:Restoring parameters from /content/drive/My Drive/Development/DeepLabCut/dev/possum101_11Apr-Phil-2020-04-13-diff/dlc-models/iteration-4/possum101_11AprApr13-trainset95shuffle1/train/snapshot-150000\n"],"name":"stdout"},{"output_type":"stream","text":["\n","  0%|          | 0/7904 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Starting to analyze %  ./videos/11Apr_diff.mp4\n","Loading  ./videos/11Apr_diff.mp4\n","Duration of video [s]:  263.47 , recorded with  30.0 fps!\n","Overall # of frames:  7904  found with (before cropping) frame dimensions:  1024 1024\n","Starting to extract posture\n"],"name":"stdout"},{"output_type":"stream","text":["\n","  2%|▏         | 158/7904 [00:04<03:42, 34.82it/s]\u001b[A\n","  3%|▎         | 237/7904 [00:08<04:15, 30.02it/s]\u001b[A\n","  4%|▍         | 316/7904 [00:11<04:39, 27.11it/s]\u001b[A\n","  5%|▍         | 395/7904 [00:15<04:56, 25.31it/s]\u001b[A\n","  6%|▌         | 474/7904 [00:18<05:07, 24.18it/s]\u001b[A\n","  7%|▋         | 553/7904 [00:22<05:13, 23.47it/s]\u001b[A\n","  8%|▊         | 632/7904 [00:26<05:16, 22.98it/s]\u001b[A\n","  9%|▉         | 711/7904 [00:29<05:17, 22.63it/s]\u001b[A\n"," 10%|▉         | 790/7904 [00:32<05:08, 23.07it/s]\u001b[A\n"," 11%|█         | 869/7904 [00:36<05:09, 22.72it/s]\u001b[A\n"," 12%|█▏        | 948/7904 [00:40<05:09, 22.48it/s]\u001b[A\n"," 13%|█▎        | 1027/7904 [00:43<05:08, 22.27it/s]\u001b[A\n"," 14%|█▍        | 1106/7904 [00:47<05:07, 22.11it/s]\u001b[A\n"," 15%|█▍        | 1185/7904 [00:50<05:04, 22.07it/s]\u001b[A\n"," 16%|█▌        | 1264/7904 [00:54<05:01, 22.00it/s]\u001b[A\n"," 17%|█▋        | 1343/7904 [00:58<04:58, 21.97it/s]\u001b[A\n"," 18%|█▊        | 1422/7904 [01:01<04:47, 22.58it/s]\u001b[A\n"," 19%|█▉        | 1501/7904 [01:05<04:46, 22.34it/s]\u001b[A\n"," 20%|█▉        | 1580/7904 [01:08<04:44, 22.21it/s]\u001b[A\n"," 21%|██        | 1659/7904 [01:12<04:42, 22.12it/s]\u001b[A\n"," 22%|██▏       | 1738/7904 [01:15<04:39, 22.07it/s]\u001b[A\n"," 23%|██▎       | 1817/7904 [01:19<04:36, 21.99it/s]\u001b[A\n"," 24%|██▍       | 1896/7904 [01:23<04:33, 21.95it/s]\u001b[A\n"," 25%|██▍       | 1975/7904 [01:26<04:30, 21.93it/s]\u001b[A\n"," 26%|██▌       | 2054/7904 [01:29<04:19, 22.55it/s]\u001b[A\n"," 27%|██▋       | 2133/7904 [01:33<04:17, 22.37it/s]\u001b[A\n"," 28%|██▊       | 2212/7904 [01:37<04:16, 22.23it/s]\u001b[A\n"," 29%|██▉       | 2291/7904 [01:40<04:13, 22.14it/s]\u001b[A\n"," 30%|██▉       | 2370/7904 [01:44<04:10, 22.08it/s]\u001b[A\n"," 31%|███       | 2449/7904 [01:47<04:07, 22.05it/s]\u001b[A\n"," 32%|███▏      | 2528/7904 [01:51<04:03, 22.05it/s]\u001b[A\n"," 33%|███▎      | 2607/7904 [01:55<04:00, 22.04it/s]\u001b[A\n"," 34%|███▍      | 2686/7904 [01:58<03:50, 22.66it/s]\u001b[A\n"," 35%|███▍      | 2765/7904 [02:02<03:49, 22.44it/s]\u001b[A\n"," 36%|███▌      | 2844/7904 [02:05<03:46, 22.32it/s]\u001b[A\n"," 37%|███▋      | 2923/7904 [02:09<03:44, 22.23it/s]\u001b[A\n"," 38%|███▊      | 3002/7904 [02:12<03:40, 22.19it/s]\u001b[A\n"," 39%|███▉      | 3081/7904 [02:16<03:38, 22.12it/s]\u001b[A\n"," 40%|███▉      | 3160/7904 [02:19<03:34, 22.11it/s]\u001b[A\n"," 41%|████      | 3239/7904 [02:23<03:30, 22.13it/s]\u001b[A\n"," 42%|████▏     | 3318/7904 [02:26<03:21, 22.74it/s]\u001b[A\n"," 43%|████▎     | 3397/7904 [02:30<03:20, 22.48it/s]\u001b[A\n"," 44%|████▍     | 3476/7904 [02:33<03:18, 22.35it/s]\u001b[A\n"," 45%|████▍     | 3555/7904 [02:37<03:15, 22.25it/s]\u001b[A\n"," 46%|████▌     | 3634/7904 [02:41<03:12, 22.16it/s]\u001b[A\n"," 47%|████▋     | 3713/7904 [02:44<03:09, 22.14it/s]\u001b[A\n"," 48%|████▊     | 3792/7904 [02:48<03:05, 22.11it/s]\u001b[A\n"," 49%|████▉     | 3871/7904 [02:51<03:02, 22.08it/s]\u001b[A\n"," 50%|████▉     | 3950/7904 [02:55<02:54, 22.67it/s]\u001b[A\n"," 51%|█████     | 4029/7904 [02:58<02:52, 22.47it/s]\u001b[A\n"," 52%|█████▏    | 4108/7904 [03:02<02:50, 22.31it/s]\u001b[A\n"," 53%|█████▎    | 4187/7904 [03:05<02:47, 22.23it/s]\u001b[A\n"," 54%|█████▍    | 4266/7904 [03:09<02:44, 22.16it/s]\u001b[A\n"," 55%|█████▍    | 4345/7904 [03:13<02:41, 22.09it/s]\u001b[A\n"," 56%|█████▌    | 4424/7904 [03:16<02:37, 22.06it/s]\u001b[A\n"," 57%|█████▋    | 4503/7904 [03:20<02:34, 22.06it/s]\u001b[A\n"," 58%|█████▊    | 4582/7904 [03:23<02:26, 22.66it/s]\u001b[A\n"," 59%|█████▉    | 4661/7904 [03:27<02:24, 22.48it/s]\u001b[A\n"," 60%|█████▉    | 4740/7904 [03:30<02:21, 22.34it/s]\u001b[A\n"," 61%|██████    | 4819/7904 [03:34<02:18, 22.27it/s]\u001b[A\n"," 62%|██████▏   | 4898/7904 [03:37<02:15, 22.20it/s]\u001b[A\n"," 63%|██████▎   | 4977/7904 [03:41<02:12, 22.12it/s]\u001b[A\n"," 64%|██████▍   | 5056/7904 [03:45<02:08, 22.11it/s]\u001b[A\n"," 65%|██████▍   | 5135/7904 [03:48<02:05, 22.11it/s]\u001b[A\n"," 66%|██████▌   | 5214/7904 [03:51<01:58, 22.72it/s]\u001b[A\n"," 67%|██████▋   | 5293/7904 [03:55<01:56, 22.49it/s]\u001b[A\n"," 68%|██████▊   | 5372/7904 [03:59<01:53, 22.31it/s]\u001b[A\n"," 69%|██████▉   | 5451/7904 [04:02<01:50, 22.21it/s]\u001b[A\n"," 70%|██████▉   | 5530/7904 [04:06<01:47, 22.16it/s]\u001b[A\n"," 71%|███████   | 5609/7904 [04:09<01:43, 22.13it/s]\u001b[A\n"," 72%|███████▏  | 5688/7904 [04:13<01:40, 22.08it/s]\u001b[A\n"," 73%|███████▎  | 5767/7904 [04:17<01:36, 22.05it/s]\u001b[A\n"," 74%|███████▍  | 5846/7904 [04:20<01:30, 22.64it/s]\u001b[A\n"," 75%|███████▍  | 5925/7904 [04:23<01:28, 22.48it/s]\u001b[A\n"," 76%|███████▌  | 6004/7904 [04:27<01:25, 22.34it/s]\u001b[A\n"," 77%|███████▋  | 6083/7904 [04:31<01:21, 22.25it/s]\u001b[A\n"," 78%|███████▊  | 6162/7904 [04:34<01:18, 22.19it/s]\u001b[A\n"," 79%|███████▉  | 6241/7904 [04:38<01:15, 22.16it/s]\u001b[A\n"," 80%|███████▉  | 6320/7904 [04:41<01:11, 22.10it/s]\u001b[A\n"," 81%|████████  | 6399/7904 [04:45<01:08, 22.10it/s]\u001b[A\n"," 82%|████████▏ | 6478/7904 [04:48<01:02, 22.72it/s]\u001b[A\n"," 83%|████████▎ | 6557/7904 [04:52<00:59, 22.49it/s]\u001b[A\n"," 84%|████████▍ | 6636/7904 [04:55<00:56, 22.36it/s]\u001b[A\n"," 85%|████████▍ | 6715/7904 [04:59<00:53, 22.25it/s]\u001b[A\n"," 86%|████████▌ | 6794/7904 [05:02<00:50, 22.19it/s]\u001b[A\n"," 87%|████████▋ | 6873/7904 [05:06<00:46, 22.12it/s]\u001b[A\n"," 88%|████████▊ | 6952/7904 [05:10<00:43, 22.09it/s]\u001b[A\n"," 89%|████████▉ | 7031/7904 [05:13<00:39, 22.09it/s]\u001b[A\n"," 90%|████████▉ | 7110/7904 [05:17<00:35, 22.68it/s]\u001b[A\n"," 91%|█████████ | 7189/7904 [05:20<00:31, 22.49it/s]\u001b[A\n"," 92%|█████████▏| 7268/7904 [05:24<00:28, 22.34it/s]\u001b[A\n"," 93%|█████████▎| 7347/7904 [05:27<00:25, 22.24it/s]\u001b[A\n"," 94%|█████████▍| 7426/7904 [05:31<00:21, 22.17it/s]\u001b[A\n"," 95%|█████████▍| 7505/7904 [05:34<00:18, 22.11it/s]\u001b[A\n"," 96%|█████████▌| 7584/7904 [05:38<00:14, 22.06it/s]\u001b[A\n"," 97%|█████████▋| 7663/7904 [05:42<00:10, 22.04it/s]\u001b[A\n"," 98%|█████████▊| 7742/7904 [05:45<00:07, 22.66it/s]\u001b[A\n"," 99%|█████████▉| 7821/7904 [05:48<00:03, 22.48it/s]\u001b[A\n","100%|█████████▉| 7900/7904 [05:52<00:00, 22.36it/s]\u001b[A\n","7979it [05:56, 22.38it/s]"],"name":"stderr"},{"output_type":"stream","text":["Detected frames:  7904\n","Saving results in videos...\n","Saving csv poses!\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["The videos are analyzed. Now your research can truly start! \n"," You can create labeled videos with 'create_labeled_video'.\n","If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract any outlier frames!\n","Function <function analyze_videos at 0x7f7486e096a8> created 3 new files in directory ./videos\n","Updated config.yaml with event analysis at 05Aug20_15h09m24s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VUgo9ZGTguCq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1596510296941,"user_tz":240,"elapsed":70902,"user":{"displayName":"Phil Fahn-Lai","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgaLRzhwWuE2_ZNOkUUkb6lkk-noGiIsb7ucOzW=s64","userId":"13631433551768862211"}},"outputId":"34b4ccbe-a06e-42d1-939e-90578bca89c5"},"source":["model.dlc.check_labels(model.yaml)\n","alert_done()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Creating images with labels by Phil.\n","/content/drive/My Drive/Development/DeepLabCut/dev/possum101_11Apr-Phil-2020-04-13-diff/labeled-data/11Apr_diff_labeled  already exists!\n","They are stored in the following folder: /content/drive/My Drive/Development/DeepLabCut/dev/possum101_11Apr-Phil-2020-04-13-diff/labeled-data/11Apr_diff_labeled.\n","If all the labels are ok, then use the function 'create_training_dataset' to create the training dataset!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Tn6ww5NZaXSh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"executionInfo":{"status":"ok","timestamp":1590503914935,"user_tz":240,"elapsed":134848,"user":{"displayName":"Phil Fahn-Lai","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgaLRzhwWuE2_ZNOkUUkb6lkk-noGiIsb7ucOzW=s64","userId":"13631433551768862211"}},"outputId":"3a9e9402-a874-476f-f9d3-c7438e7e05a4"},"source":["model.dlc.create_labeled_video(config_path,['/content/drive/My Drive/Development/DeadROMM/possum101_11Apr-Phil-2020-04-13-diff/videos/11Apr_diff.mp4'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Starting %  /content/drive/My Drive/Development/DeadROMM/possum101_11Apr-Phil-2020-04-13-diff/videos ['/content/drive/My Drive/Development/DeadROMM/possum101_11Apr-Phil-2020-04-13-diff/videos/11Apr_diff.mp4']\n","Loading  /content/drive/My Drive/Development/DeadROMM/possum101_11Apr-Phil-2020-04-13-diff/videos/11Apr_diff.mp4 and data.\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 13/7904 [00:00<01:05, 120.12it/s]"],"name":"stderr"},{"output_type":"stream","text":["7904\n","Duration of video [s]:  263.47 , recorded with  30.0 fps!\n","Overall # of frames:  7904 with cropped frame dimensions:  1024 1024\n","Generating frames and creating video.\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7904/7904 [02:13<00:00, 59.14it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"dZWt2jyJbozo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":207},"executionInfo":{"status":"ok","timestamp":1590504272902,"user_tz":240,"elapsed":139908,"user":{"displayName":"Phil Fahn-Lai","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgaLRzhwWuE2_ZNOkUUkb6lkk-noGiIsb7ucOzW=s64","userId":"13631433551768862211"}},"outputId":"1ba643be-2519-405c-9b47-fb63ec0d106f"},"source":["\n","model.dlc.filterpredictions(config_path,['/content/drive/My Drive/Development/DeadROMM/possum101_11Apr-Phil-2020-04-13-diff/videos/11Apr_diff.mp4'], filtertype=\"spline\",windowlength=17)\n","model.dlc.create_labeled_video(config_path,['/content/drive/My Drive/Development/DeadROMM/possum101_11Apr-Phil-2020-04-13-diff/videos/11Apr_diff.mp4'],filtered=True)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["4it [00:00, 39.50it/s]"],"name":"stderr"},{"output_type":"stream","text":["Filtering with spline model /content/drive/My Drive/Development/DeadROMM/possum101_11Apr-Phil-2020-04-13-diff/videos/11Apr_diff.mp4\n"],"name":"stdout"},{"output_type":"stream","text":["44it [00:01, 38.13it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Saving filtered csv poses!\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/7904 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Starting %  /content/drive/My Drive/Development/DeadROMM/possum101_11Apr-Phil-2020-04-13-diff/videos ['/content/drive/My Drive/Development/DeadROMM/possum101_11Apr-Phil-2020-04-13-diff/videos/11Apr_diff.mp4']\n","Loading  /content/drive/My Drive/Development/DeadROMM/possum101_11Apr-Phil-2020-04-13-diff/videos/11Apr_diff.mp4 and data.\n","7904\n","Duration of video [s]:  263.47 , recorded with  30.0 fps!\n","Overall # of frames:  7904 with cropped frame dimensions:  1024 1024\n","Generating frames and creating video.\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7904/7904 [02:16<00:00, 57.86it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"R-1ytuoWudYB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":258},"executionInfo":{"status":"ok","timestamp":1590444349382,"user_tz":240,"elapsed":35624,"user":{"displayName":"Phil Fahn-Lai","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgaLRzhwWuE2_ZNOkUUkb6lkk-noGiIsb7ucOzW=s64","userId":"13631433551768862211"}},"outputId":"02d782c0-1783-48e3-f10a-5941d5f56cfb"},"source":["model.dlc.create_training_dataset(model.yaml,windows2linux=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Development/DeadROMM/possum101_11Apr-Phil-2020-04-13-diff/training-datasets/iteration-3/UnaugmentedDataSet_possum101_11AprApr13  already exists!\n","Annotation data converted to unix format...\n","/content/drive/My Drive/Development/DeadROMM/possum101_11Apr-Phil-2020-04-13-diff/dlc-models/iteration-3/possum101_11AprApr13-trainset95shuffle1  already exists!\n","/content/drive/My Drive/Development/DeadROMM/possum101_11Apr-Phil-2020-04-13-diff/dlc-models/iteration-3/possum101_11AprApr13-trainset95shuffle1/train  already exists!\n","/content/drive/My Drive/Development/DeadROMM/possum101_11Apr-Phil-2020-04-13-diff/dlc-models/iteration-3/possum101_11AprApr13-trainset95shuffle1/test  already exists!\n","The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[(0.95,\n","  1,\n","  (array([57, 62, 69, 32, 70,  1, 35, 29,  6, 66, 48, 79, 22, 44, 40, 36, 21,\n","           7, 10,  5, 43, 63,  8, 31, 55, 38, 33, 65, 41,  2, 73, 50, 24, 77,\n","          52, 53, 16, 64, 78, 67, 30, 72,  9, 71, 47, 75, 59, 23, 37, 19, 15,\n","          25, 76, 58, 20, 42, 14, 68, 74, 39, 49, 12, 56,  0, 46,  4, 27, 60,\n","          26, 34, 13, 54, 51, 45, 11, 28]), array([ 3, 17, 18, 61])))]"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"swrP1i3l2OFH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":289},"executionInfo":{"status":"ok","timestamp":1590444878322,"user_tz":240,"elapsed":1208,"user":{"displayName":"Phil Fahn-Lai","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgaLRzhwWuE2_ZNOkUUkb6lkk-noGiIsb7ucOzW=s64","userId":"13631433551768862211"}},"outputId":"25629f38-f360-4b99-bcd7-138d1c857544"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mon May 25 22:14:37 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   44C    P0    34W / 250W |  15767MiB / 16280MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]}]}