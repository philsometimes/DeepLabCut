{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RK255E7YoEIt"
   },
   "source": [
    "# DeepLabCut Toolbox\n",
    "https://github.com/AlexEMG/DeepLabCut\n",
    "\n",
    "Nath\\*, Mathis\\* et al. Using DeepLabCut for 3D markerless pose estimation during behavior across species.\n",
    "\n",
    "pre-print: https://www.biorxiv.org/content/10.1101/476531v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jqLZhp7EoEI0"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import deeplabcut\n",
    "import pandas as pd\n",
    "import ruamel.yaml\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "wd=r\"E:\\Users\\Phil\\DeepLabCut\\dev\"\n",
    "vid_directory=r\"Z:\\lab\\NSF forelimb project\\Phil_lab\\C-arm\\Ex\\23Apr18.LaiRegnault.SEP85.LS.biceps_teres_lat\\run5flexextLAR_90-35-4ms\"\n",
    "cam_prefix = \"robo_CLAHE_CameraNo.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "markerlist = ['Body_ds1_crn',\n",
    "              'Body_ds2_int',\n",
    "              'Body_ds3_cdl',\n",
    "              'Body_vn1_crn',\n",
    "              'Body_vn2_int',\n",
    "              'Body_vn3_cdl',\n",
    "              'Body_acc',\n",
    "              'Scapula_acr',\n",
    "              'Scapula_spi',\n",
    "              'Scapula_vtb',\n",
    "              'Scapula_acc',\n",
    "              'Humerus_dpc',\n",
    "              'Humerus_ent',\n",
    "              'Humerus_ect',\n",
    "              'Humerus_acc',\n",
    "              'Ulna_olc',\n",
    "              'Ulna_int',\n",
    "              'Ulna_dst',\n",
    "              'Ulna_acc',\n",
    "              'Radius_prx',\n",
    "              'Radius_int',\n",
    "              'Radius_dst',\n",
    "              'Radius_acc',\n",
    "              'Teres_maj_prx',\n",
    "              'Teres_maj_dst',\n",
    "              'Biceps_prx',\n",
    "              'Biceps_dst']\n",
    "dotsize = 5\n",
    "corner2move2 = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_config_file=r\"E:\\Users\\Phil\\DeepLabCut\\dev\\23Apr18-Phil-2019-09-20\\config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c9DjG55FoEI7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created \"E:\\Users\\Phil\\DeepLabCut\\dev\\23Apr18CLAHE-Phil-2019-09-24\\videos\"\n",
      "Created \"E:\\Users\\Phil\\DeepLabCut\\dev\\23Apr18CLAHE-Phil-2019-09-24\\labeled-data\"\n",
      "Created \"E:\\Users\\Phil\\DeepLabCut\\dev\\23Apr18CLAHE-Phil-2019-09-24\\training-datasets\"\n",
      "Created \"E:\\Users\\Phil\\DeepLabCut\\dev\\23Apr18CLAHE-Phil-2019-09-24\\dlc-models\"\n",
      "Creating the symbolic link of the video\n",
      "Created the symlink of Z:\\lab\\NSF forelimb project\\Phil_lab\\C-arm\\Ex\\23Apr18.LaiRegnault.SEP85.LS.biceps_teres_lat\\run5flexextLAR_90-35-4ms\\robo_CLAHE_CameraNo.1.avi to E:\\Users\\Phil\\DeepLabCut\\dev\\23Apr18CLAHE-Phil-2019-09-24\\videos\\robo_CLAHE_CameraNo.1.avi\n",
      "Created the symlink of Z:\\lab\\NSF forelimb project\\Phil_lab\\C-arm\\Ex\\23Apr18.LaiRegnault.SEP85.LS.biceps_teres_lat\\run5flexextLAR_90-35-4ms\\robo_CLAHE_CameraNo.2.avi to E:\\Users\\Phil\\DeepLabCut\\dev\\23Apr18CLAHE-Phil-2019-09-24\\videos\\robo_CLAHE_CameraNo.2.avi\n",
      "E:\\Users\\Phil\\DeepLabCut\\dev\\23Apr18CLAHE-Phil-2019-09-24\\videos\\robo_CLAHE_CameraNo.1.avi\n",
      "E:\\Users\\Phil\\DeepLabCut\\dev\\23Apr18CLAHE-Phil-2019-09-24\\videos\\robo_CLAHE_CameraNo.2.avi\n",
      "Generated \"E:\\Users\\Phil\\DeepLabCut\\dev\\23Apr18CLAHE-Phil-2019-09-24\\config.yaml\"\n",
      "\n",
      "A new project with name 23Apr18CLAHE-Phil-2019-09-24 is created at E:\\Users\\Phil\\DeepLabCut\\dev and a configurable file (config.yaml) is stored there. Change the parameters in this file to adapt to your project's needs.\n",
      " Once you have changed the configuration file, use the function 'extract_frames' to select frames for labeling.\n",
      ". [OPTIONAL] Use the function 'add_new_videos' to add new videos to your project (at any stage).\n"
     ]
    }
   ],
   "source": [
    "task='23Apr18CLAHE'\n",
    "experimenter='Phil'\n",
    "video=[vid_directory+r'\\robo_CLAHE_CameraNo.1.avi',\n",
    "       vid_directory+r'\\robo_CLAHE_CameraNo.2.avi'] # Enter the paths of your videos you want to grab frames from.\n",
    "path_config_file=deeplabcut.create_new_project(task,experimenter,video, working_directory=wd,copy_videos=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "## substitute markers from 23apr18 experiment ##\n",
    "config = ruamel.yaml.load(open(path_config_file))\n",
    "config['bodyparts']=markerlist\n",
    "config['dotsize']=dotsize\n",
    "config['corner2move2']=[corner2move2,corner2move2]\n",
    "ruamel.yaml.round_trip_dump(config, sys.stdout)\n",
    "with open(path_config_file, 'w') as fp:\n",
    "    ruamel.yaml.round_trip_dump(config, fp)\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t1ulumCuoEJC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file read successfully.\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 26.67  seconds.\n",
      "Extracting and downsampling... 800  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [00:05, 140.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 26.67  seconds.\n",
      "Extracting and downsampling... 800  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [00:06, 125.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "\n",
      "Frames were selected.\n",
      "You can now label the frames using the function 'label_frames' (if you extracted enough frames for all videos).\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "deeplabcut.extract_frames(path_config_file, userfeedback=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Users\\Phil\\DeepLabCut\\dev\\23Apr18CLAHE-Phil-2019-09-24\\labeled-data\\robo_CLAHE_CameraNo.1\\CollectedData_Phil.h5\n",
      "E:\\Users\\Phil\\DeepLabCut\\dev\\23Apr18CLAHE-Phil-2019-09-24\\labeled-data\\robo_CLAHE_CameraNo.2\\CollectedData_Phil.h5\n"
     ]
    }
   ],
   "source": [
    "#### convert xmalab exports to training format ####\n",
    "# no spaces in marker names, otherwise 2D export fails (more header columns than data)\n",
    "##!!!! CHECK FOR FLIPPING!!!!\n",
    "xma_2Dcsv_path = r\"E:\\Users\\Phil\\XROMM\\23Apr18\\run5flexextLAR_90-35-4ms\\2D_run5flexextLAR_90-35-4ms.csv\"\n",
    "labeled_data_path=path_config_file.split('config.yaml')[0]+\"labeled-data\\\\\"\n",
    "\n",
    "## import XMAlab 2D exports\n",
    "df = pd.read_csv(xma_2Dcsv_path, sep=',', header=0)\n",
    "\n",
    "## coerce data into DeepLabCut hierarchical format\n",
    "df['frame_index']=df.index\n",
    "df['scorer']=experimenter\n",
    "df = df.melt(id_vars=['frame_index','scorer'])\n",
    "new = df['variable'].str.rsplit(\"_\",n=2,expand=True)\n",
    "df['variable'],df['cam'],df['coords'] = new[0], new[1], new[2]\n",
    "df=df.rename(columns={'variable':'bodyparts'})\n",
    "df['coords']=df['coords'].str.rstrip(\" \").str.lower()\n",
    "df['bodyparts']=df['bodyparts'].str.lstrip(\" \").astype(\"category\")\n",
    "df['bodyparts'].cat.set_categories(markerlist,inplace=True)\n",
    "df['frame_index'] = ['labeled-data\\\\' + cam_prefix+x[-1] + '\\\\img' + (f\"{y:03d}\") + '.png' for x, y in zip(df['cam'], df['frame_index'])]\n",
    "newdf = df.pivot_table(columns=['scorer', 'bodyparts', 'coords'],index='frame_index',values='value',aggfunc='first')\n",
    "newdf.index.name=None\n",
    "newdf = newdf.dropna()\n",
    "newdf = newdf.astype(np.float64)\n",
    "\n",
    "## go into frame folders and get frame index ##\n",
    "extracted_frames = []\n",
    "for root, dirs, files in os.walk(os.path.join(os.path.dirname(path_config_file),'labeled-data')):\n",
    "    for name in files:\n",
    "        if name.endswith(\".png\"):\n",
    "            camera_id = root.split(' ')[-1][-1]\n",
    "            frame_no = int(name.split('.')[0].replace('img',''))\n",
    "            new_name = 'labeled-data\\\\'+cam_prefix+camera_id+'\\\\img' + (f\"{frame_no:03d}\") + '.png'\n",
    "            extracted_frames.append(new_name)\n",
    "        \n",
    "## filter by list of extracted frames\n",
    "df_extracted = newdf.filter(items=pd.Index(extracted_frames),axis=0)\n",
    "\n",
    "## split new df into cams 1 and 2\n",
    "df1 = df_extracted.filter(like=cam_prefix+\"1\",axis=0)\n",
    "df2 = df_extracted.filter(like=cam_prefix+\"2\",axis=0)\n",
    "\n",
    "## split new df into cams 1 and 2, export as h5 and csv\n",
    "for x in [1,2]:\n",
    "    cam_name = cam_prefix+str(x)\n",
    "    dfx = df_extracted.filter(like=cam_name,axis=0)\n",
    "    data_name = labeled_data_path+cam_name+\"\\\\CollectedData_\"+experimenter+\".h5\"\n",
    "    dfx.to_hdf(data_name, 'df_with_missing', format='table', mode='w')\n",
    "    dfx.to_csv(data_name.split('.h5')[0]+'.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iyROSOiEoEJI"
   },
   "outputs": [],
   "source": [
    "%gui wx\n",
    "deeplabcut.label_frames(path_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NwvgPJouPP2O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating images with labels by Phil.\n",
      "They are stored in the following folder: E:\\Users\\Phil\\DeepLabCut\\dev\\23Apr18-Phil-2019-09-20\\labeled-data\\Camera No.1_labeled.\n",
      "They are stored in the following folder: E:\\Users\\Phil\\DeepLabCut\\dev\\23Apr18-Phil-2019-09-20\\labeled-data\\Camera No.2_labeled.\n",
      "If all the labels are ok, then use the function 'create_training_dataset' to create the training dataset!\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.check_labels(path_config_file) #this creates a subdirectory with the frames + your labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xNi9s1dboEJN"
   },
   "source": [
    "## Create a training dataset\n",
    "\n",
    "This function generates the training data information for network training based on the pandas dataframes that hold label information. The user can set the fraction of the training set size (from all labeled image in the hd5 file) in the config.yaml file. While creating the dataset, the user can create multiple shuffles if they want to benchmark the performance (typcailly, 1 is what you will set, so you pass nothing!). \n",
    "\n",
    "After running this script the training dataset is created and saved in the project directory under the subdirectory **'training-datasets'**\n",
    "\n",
    "This function also creates new subdirectories under **dlc-models** and appends the project config.yaml file with the correct path to the training and testing pose configuration file. These files hold the parameters for training the network. Such an example file is provided with the toolbox and named as **pose_cfg.yaml**. For most all use cases we have seen, the defaults are perfectly fine.\n",
    "\n",
    "Now it is the time to start training the network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMeUwgxPoEJP",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Users\\Phil\\DeepLabCut\\dev\\23Apr18-Phil-2019-09-20\\training-datasets\\iteration-0\\UnaugmentedDataSet_23Apr18Sep20  already exists!\n",
      "E:\\Users\\Phil\\DeepLabCut\\dev\\23Apr18-Phil-2019-09-20\\dlc-models\\iteration-0\\23Apr18Sep20-trainset95shuffle1  already exists!\n",
      "E:\\Users\\Phil\\DeepLabCut\\dev\\23Apr18-Phil-2019-09-20\\dlc-models\\iteration-0\\23Apr18Sep20-trainset95shuffle1//train  already exists!\n",
      "E:\\Users\\Phil\\DeepLabCut\\dev\\23Apr18-Phil-2019-09-20\\dlc-models\\iteration-0\\23Apr18Sep20-trainset95shuffle1//test  already exists!\n",
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.create_training_dataset(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c4FczXGDoEJU"
   },
   "source": [
    "## Start training:\n",
    "\n",
    "This function trains the network for a specific shuffle of the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_pOvDq_2oEJW"
   },
   "outputs": [],
   "source": [
    "deeplabcut.train_network(path_config_file, displayiters=10,saveiters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nv4zlbrnoEJg"
   },
   "outputs": [],
   "source": [
    "deeplabcut.evaluate_network(path_config_file, plotting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y_LZiS_0oEJl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0],\n",
      "                [1],\n",
      "                [2],\n",
      "                [3],\n",
      "                [4],\n",
      "                [5],\n",
      "                [6],\n",
      "                [7],\n",
      "                [8],\n",
      "                [9],\n",
      "                [10],\n",
      "                [11],\n",
      "                [12],\n",
      "                [13],\n",
      "                [14],\n",
      "                [15],\n",
      "                [16],\n",
      "                [17],\n",
      "                [18],\n",
      "                [19],\n",
      "                [20],\n",
      "                [21],\n",
      "                [22],\n",
      "                [23],\n",
      "                [24],\n",
      "                [25],\n",
      "                [26]],\n",
      " 'all_joints_names': ['Body_ds1_crn',\n",
      "                      'Body_ds2_int',\n",
      "                      'Body_ds3_cdl',\n",
      "                      'Body_vn1_crn',\n",
      "                      'Body_vn2_int',\n",
      "                      'Body_vn3_cdl',\n",
      "                      'Body_acc',\n",
      "                      'Scapula_acr',\n",
      "                      'Scapula_spi',\n",
      "                      'Scapula_vtb',\n",
      "                      'Scapula_acc',\n",
      "                      'Humerus_dpc',\n",
      "                      'Humerus_ent',\n",
      "                      'Humerus_ect',\n",
      "                      'Humerus_acc',\n",
      "                      'Ulna_olc',\n",
      "                      'Ulna_int',\n",
      "                      'Ulna_dst',\n",
      "                      'Ulna_acc',\n",
      "                      'Radius_prx',\n",
      "                      'Radius_int',\n",
      "                      'Radius_dst',\n",
      "                      'Radius_acc',\n",
      "                      'Teres_maj_prx',\n",
      "                      'Teres_maj_dst',\n",
      "                      'Biceps_prx',\n",
      "                      'Biceps_dst'],\n",
      " 'batch_size': 8,\n",
      " 'bottomheight': 400,\n",
      " 'crop': True,\n",
      " 'crop_pad': 0,\n",
      " 'cropratio': 0.4,\n",
      " 'dataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_23Apr18Sep20\\\\23Apr18_Phil95shuffle1.mat',\n",
      " 'dataset_type': 'default',\n",
      " 'deterministic': False,\n",
      " 'display_iters': 1000,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': 'C:\\\\Users\\\\LabAdmin\\\\.conda\\\\envs\\\\dlc-windowsGPU\\\\lib\\\\site-packages\\\\deeplabcut\\\\pose_estimation_tensorflow\\\\models\\\\pretrained\\\\resnet_v1_50.ckpt',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'leftwidth': 400,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 0.05,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'max_input_size': 1500,\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'metadataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_23Apr18Sep20\\\\Documentation_data-23Apr18_95shuffle1.pickle',\n",
      " 'min_input_size': 64,\n",
      " 'minsize': 100,\n",
      " 'mirror': False,\n",
      " 'multi_step': [[0.005, 10000],\n",
      "                [0.02, 430000],\n",
      "                [0.002, 730000],\n",
      "                [0.001, 1030000]],\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 27,\n",
      " 'num_outputs': 1,\n",
      " 'optimizer': 'sgd',\n",
      " 'pos_dist_thresh': 17,\n",
      " 'project_path': 'E:\\\\Users\\\\Phil\\\\DeepLabCut\\\\dev\\\\23Apr18-Phil-2019-09-20',\n",
      " 'regularize': False,\n",
      " 'rightwidth': 400,\n",
      " 'save_iters': 50000,\n",
      " 'scale_jitter_lo': 0.5,\n",
      " 'scale_jitter_up': 1.25,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': 'E:\\\\Users\\\\Phil\\\\DeepLabCut\\\\dev\\\\23Apr18-Phil-2019-09-20\\\\dlc-models\\\\iteration-0\\\\23Apr18Sep20-trainset95shuffle1\\\\test\\\\snapshot',\n",
      " 'stride': 8.0,\n",
      " 'topheight': 400,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using snapshot-1030000 for model E:\\Users\\Phil\\DeepLabCut\\dev\\23Apr18-Phil-2019-09-20\\dlc-models\\iteration-0\\23Apr18Sep20-trainset95shuffle1\n",
      "num_outputs =  1\n",
      "INFO:tensorflow:Restoring parameters from E:\\Users\\Phil\\DeepLabCut\\dev\\23Apr18-Phil-2019-09-20\\dlc-models\\iteration-0\\23Apr18Sep20-trainset95shuffle1\\train\\snapshot-1030000\n",
      "Analyzing all the videos in the directory\n",
      "Starting to analyze %  Camera No.2.avi\n",
      "Loading  Camera No.2.avi\n",
      "Duration of video [s]:  26.67 , recorded with  30.0 fps!\n",
      "Overall # of frames:  800  found with (before cropping) frame dimensions:  1024 1024\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "810it [01:14, 11.05it/s]                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "810it [01:14, 10.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in ....\n",
      "Starting to analyze %  Camera No.1.avi\n",
      "Loading  Camera No.1.avi\n",
      "Duration of video [s]:  26.67 , recorded with  30.0 fps!\n",
      "Overall # of frames:  800  found with (before cropping) frame dimensions:  1024 1024\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "810it [00:47, 15.43it/s]                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "810it [00:47, 17.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in ....\n",
      "The videos are analyzed. Now your research can truly start! \n",
      " You can create labeled videos with 'create_labeled_video'.\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract any outlier frames!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'DeepCut_resnet50_23Apr18Sep20shuffle1_1030000'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videofile_path = [r'Z:\\lab\\NSF forelimb project\\Phil_lab\\C-arm\\Ex\\23Apr18.LaiRegnault.SEP85.LS.biceps_teres_lat\\run5flexextLAR_90-35-4ms'] #Enter a folder OR a list of videos to analyze.\n",
    "\n",
    "deeplabcut.analyze_videos(path_config_file,videofile_path, videotype='.avi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing all the videos in the directory\n",
      "Starting %  . ['Z:\\\\lab\\\\NSF forelimb project\\\\Phil_lab\\\\C-arm\\\\Ex\\\\23Apr18.LaiRegnault.SEP85.LS.biceps_teres_lat\\\\run5flexextLAR_90-35-4ms']\n",
      "Loading  Camera No.1.avi and data.\n",
      "False 0 1024 0 1024\n",
      "800\n",
      "Duration of video [s]:  26.67 , recorded with  30.0 fps!\n",
      "Overall # of frames:  800 with cropped frame dimensions:  1024 1024\n",
      "Generating frames and creating video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 800/800 [00:13<00:00, 60.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting %  . ['Z:\\\\lab\\\\NSF forelimb project\\\\Phil_lab\\\\C-arm\\\\Ex\\\\23Apr18.LaiRegnault.SEP85.LS.biceps_teres_lat\\\\run5flexextLAR_90-35-4ms']\n",
      "Loading  Camera No.2.avi and data.\n",
      "False 0 1024 0 1024\n",
      "800\n",
      "Duration of video [s]:  26.67 , recorded with  30.0 fps!\n",
      "Overall # of frames:  800 with cropped frame dimensions:  1024 1024\n",
      "Generating frames and creating video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 800/800 [00:13<00:00, 59.72it/s]\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.create_labeled_video(path_config_file,[r'Z:\\lab\\NSF forelimb project\\Phil_lab\\C-arm\\Ex\\23Apr18.LaiRegnault.SEP85.LS.biceps_teres_lat\\run5flexextLAR_90-35-4ms'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iGu_PdTWoEJr"
   },
   "source": [
    "## Extract outlier frames [optional step]\n",
    "\n",
    "This is an optional step and is used only when the evaluation results are poor i.e. the labels are incorrectly predicted. In such a case, the user can use the following function to extract frames where the labels are incorrectly predicted. This step has many options, so please look at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.extract_outlier_frames?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mdeeplabcut\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_labeled_video\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvideos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvideotype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'avi'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainingsetindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_frames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFrames2plot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelete\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisplayedbodyparts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'all'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcodec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mp4v'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputframerate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestfolder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdraw_skeleton\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrailpoints\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisplaycropped\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "    Labels the bodyparts in a video. Make sure the video is already analyzed by the function 'analyze_video'\n",
       "\n",
       "    Parameters\n",
       "    ----------\n",
       "    config : string\n",
       "        Full path of the config.yaml file as a string.\n",
       "\n",
       "    videos : list\n",
       "        A list of strings containing the full paths to videos for analysis or a path to the directory, where all the videos with same extension are stored.\n",
       "    \n",
       "    videotype: string, optional\n",
       "        Checks for the extension of the video in case the input to the video is a directory.\n",
       " Only videos with this extension are analyzed. The default is ``.avi``\n",
       "\n",
       "    shuffle : int, optional\n",
       "        Number of shuffles of training dataset. Default is set to 1.\n",
       "    \n",
       "    trainingsetindex: int, optional\n",
       "        Integer specifying which TrainingsetFraction to use. By default the first (note that TrainingFraction is a list in config.yaml).\n",
       "     \n",
       "    filtered: bool, default false\n",
       "        Boolean variable indicating if filtered output should be plotted rather than frame-by-frame predictions. Filtered version can be calculated with deeplabcut.filterpredictions\n",
       "    \n",
       "    videotype: string, optional\n",
       "        Checks for the extension of the video in case the input is a directory.\n",
       "Only videos with this extension are analyzed. The default is ``.avi``\n",
       "\n",
       "    save_frames: bool\n",
       "        If true creates each frame individual and then combines into a video. This variant is relatively slow as\n",
       "        it stores all individual frames. However, it uses matplotlib to create the frames and is therefore much more flexible (one can set transparency of markers, crop, and easily customize).\n",
       "\n",
       "    Frames2plot: List of indices\n",
       "        If not None & save_frames=True then the frames corresponding to the index will be plotted. For example, Frames2plot=[0,11] will plot the first and the 12th frame.\n",
       "        \n",
       "    delete: bool\n",
       "        If true then the individual frames created during the video generation will be deleted.\n",
       "\n",
       "    displayedbodyparts: list of strings, optional\n",
       "        This select the body parts that are plotted in the video. Either ``all``, then all body parts\n",
       "        from config.yaml are used orr a list of strings that are a subset of the full list.\n",
       "        E.g. ['hand','Joystick'] for the demo Reaching-Mackenzie-2018-08-30/config.yaml to select only these two body parts.\n",
       "\n",
       "    codec: codec for labeled video. Options see http://www.fourcc.org/codecs.php [depends on your ffmpeg installation.]\n",
       "    \n",
       "    outputframerate: positive number, output frame rate for labeled video (only available for the mode with saving frames.) By default: None, which results in the original video rate.\n",
       "    \n",
       "    destfolder: string, optional\n",
       "        Specifies the destination folder that was used for storing analysis data (default is the path of the video).\n",
       "        \n",
       "    draw_skeleton: bool\n",
       "        If ``True`` adds a line connecting the body parts making a skeleton on on each frame. The body parts to be connected and the color of these connecting lines are specified in the config file. By default: ``False``\n",
       "    \n",
       "    trailpoints: int\n",
       "        Number of revious frames whose body parts are plotted in a frame (for displaying history). Default is set to 0.\n",
       "    \n",
       "    displaycropped: bool, optional\n",
       "        Specifies whether only cropped frame is displayed (with labels analyzed therein), or the original frame with the labels analyzed in the cropped subset.\n",
       "    \n",
       "    Examples\n",
       "    --------\n",
       "    If you want to create the labeled video for only 1 video\n",
       "    >>> deeplabcut.create_labeled_video('/analysis/project/reaching-task/config.yaml',['/analysis/project/videos/reachingvideo1.avi'])\n",
       "    --------\n",
       "\n",
       "    If you want to create the labeled video for only 1 video and store the individual frames\n",
       "    >>> deeplabcut.create_labeled_video('/analysis/project/reaching-task/config.yaml',['/analysis/project/videos/reachingvideo1.avi'],save_frames=True)\n",
       "    --------\n",
       "\n",
       "    If you want to create the labeled video for multiple videos\n",
       "    >>> deeplabcut.create_labeled_video('/analysis/project/reaching-task/config.yaml',['/analysis/project/videos/reachingvideo1.avi','/analysis/project/videos/reachingvideo2.avi'])\n",
       "    --------\n",
       "\n",
       "    If you want to create the labeled video for all the videos (as .avi extension) in a directory.\n",
       "    >>> deeplabcut.create_labeled_video('/analysis/project/reaching-task/config.yaml',['/analysis/project/videos/'])\n",
       "\n",
       "    --------\n",
       "    If you want to create the labeled video for all the videos (as .mp4 extension) in a directory.\n",
       "    >>> deeplabcut.create_labeled_video('/analysis/project/reaching-task/config.yaml',['/analysis/project/videos/'],videotype='mp4')\n",
       "\n",
       "    --------\n",
       "\n",
       "    \n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\labadmin\\.conda\\envs\\dlc-windowsgpu\\lib\\site-packages\\deeplabcut\\utils\\make_labeled_video.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deeplabcut.create_labeled_video?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gkbaBOJVoEJs"
   },
   "outputs": [],
   "source": [
    "deeplabcut.extract_outlier_frames(path_config_file,['/videos/video3.avi']) #pass a specific video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ib0uvhaoEJx"
   },
   "source": [
    "## Refine Labels [optional step]\n",
    "Following the extraction of outlier frames, the user can use the following function to move the predicted labels to the correct location. Thus augmenting the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n_FpEXtyoEJy"
   },
   "outputs": [],
   "source": [
    "%gui wx\n",
    "deeplabcut.refine_labels(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Afterwards, if you want to look at the adjusted frames, you can load them in the main GUI by running: ``deeplabcut.label_frames(path_config_file)``\n",
    "\n",
    "(you can add a new \"cell\" below to add this code!)\n",
    "\n",
    "#### Once all folders are relabeled, check the labels again! If you are not happy, adjust them in the main GUI:\n",
    "\n",
    "``deeplabcut.label_frames(path_config_file)``\n",
    "\n",
    "Check Labels:\n",
    "\n",
    "``deeplabcut.check_labels(path_config_file)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CHzstWr8oEJ2"
   },
   "outputs": [],
   "source": [
    "#NOW, merge this with your original data:\n",
    "\n",
    "deeplabcut.merge_datasets(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QCHj7qyboEJ6"
   },
   "source": [
    "## Create a new iteration of training dataset [optional step]\n",
    "Following the refinement of labels and appending them to the original dataset, this creates a new iteration of training dataset. This is automatically set in the config.yaml file, so let's get training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ytQoxIldoEJ7"
   },
   "outputs": [],
   "source": [
    "deeplabcut.create_training_dataset(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pCrUvQIvoEKD"
   },
   "source": [
    "## Create labeled video\n",
    "This funtion is for visualiztion purpose and can be used to create a video in .mp4 format with labels predicted by the network. This video is saved in the same directory where the original video resides. \n",
    "\n",
    "THIS HAS MANY FUN OPTIONS! \n",
    "\n",
    "``deeplabcut.create_labeled_video(config, videos, videotype='avi', shuffle=1, trainingsetindex=0, filtered=False, save_frames=False, Frames2plot=None, delete=False, displayedbodyparts='all', codec='mp4v', outputframerate=None, destfolder=None, draw_skeleton=False, trailpoints=0, displaycropped=False)``\n",
    "\n",
    "So please check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.create_labeled_video?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6aDF7Q7KoEKE"
   },
   "outputs": [],
   "source": [
    "deeplabcut.create_labeled_video(path_config_file,videofile_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8GTiuJESoEKH"
   },
   "source": [
    "## Plot the trajectories of the analyzed videos\n",
    "This function plots the trajectories of all the body parts across the entire video. Each body part is identified by a unique color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gX21zZbXoEKJ"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook #for making interactive plots.\n",
    "deeplabcut.plot_trajectories(path_config_file,videofile_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Demo-yourowndata.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
