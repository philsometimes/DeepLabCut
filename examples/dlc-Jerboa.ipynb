{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RK255E7YoEIt"
   },
   "source": [
    "# DeepLabCut Toolbox\n",
    "https://github.com/AlexEMG/DeepLabCut\n",
    "\n",
    "Nath\\*, Mathis\\* et al. Using DeepLabCut for 3D markerless pose estimation during behavior across species.\n",
    "\n",
    "pre-print: https://www.biorxiv.org/content/10.1101/476531v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Setup and parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jqLZhp7EoEI0"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import deeplabcut\n",
    "import pandas as pd\n",
    "import ruamel.yaml\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "wd=r\"E:\\Users\\Phil\\DeepLabCut\\dev\"\n",
    "vid_directory=r\"E:\\Users\\Phil\\Jerboa\"\n",
    "experimenter='Andy'\n",
    "markerlist = ['COM','cal1','cal2']\n",
    "dotsize = 5\n",
    "corner2move2 = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create new project (1 per animal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c9DjG55FoEI7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created \"E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-12-06\\videos\"\n",
      "Created \"E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-12-06\\labeled-data\"\n",
      "Created \"E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-12-06\\training-datasets\"\n",
      "Created \"E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-12-06\\dlc-models\"\n",
      "Creating the symbolic link of the video\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'mklink E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-12-06\\videos\\0426_0221F_4-1.mp4 E:\\Users\\Phil\\Jerboa\\0426_0221F_4-1.mp4' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\create_project\\new.py\u001b[0m in \u001b[0;36mcreate_new_project\u001b[1;34m(project, experimenter, videos, working_directory, copy_videos, videotype)\u001b[0m\n\u001b[0;32m    117\u001b[0m                 \u001b[0mdst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: symbolic link privilege not held",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-af7e7ecc56d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m vid_list=[ vid_directory+r'\\0426_0221F_4-1.mp4',\n\u001b[0;32m      3\u001b[0m          vid_directory+r'\\0426_0221F_C1_1.mp4']\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpath_config_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeeplabcut\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_new_project\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mexperimenter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvid_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworking_directory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcopy_videos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\create_project\\new.py\u001b[0m in \u001b[0;36mcreate_new_project\u001b[1;34m(project, experimenter, videos, working_directory, copy_videos, videotype)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m                 \u001b[1;32mimport\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m                 \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mklink %s %s'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Created the symlink of {} to {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[0mvideos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcheck_call\u001b[1;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcmd\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpopenargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command 'mklink E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-12-06\\videos\\0426_0221F_4-1.mp4 E:\\Users\\Phil\\Jerboa\\0426_0221F_4-1.mp4' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "task='arena_chase'\n",
    "vid_list=[ vid_directory+r'\\0426_0221F_4-1.mp4',\n",
    "         vid_directory+r'\\0426_0221F_C1_1.mp4']\n",
    "path_config_file=deeplabcut.create_new_project(task,experimenter,vid_list, working_directory=wd,copy_videos=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_config_file=wd+r'\\arena_chase-Andy-2019-10-21\\config.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Overwrite default bodyparts with XROMM marker list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "config = ruamel.yaml.load(open(path_config_file))\n",
    "config['bodyparts']=markerlist\n",
    "config['dotsize']=dotsize\n",
    "config['corner2move2']=[corner2move2,corner2move2]\n",
    "ruamel.yaml.round_trip_dump(config, sys.stdout)\n",
    "with open(path_config_file, 'w') as fp:\n",
    "    ruamel.yaml.round_trip_dump(config, fp)\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Extract frames from vid_list for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t1ulumCuoEJC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file read successfully.\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 57.17  seconds.\n",
      "Extracting and downsampling... 6853  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6853it [00:14, 457.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 101.54  seconds.\n",
      "Extracting and downsampling... 12173  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12173it [00:26, 457.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "\n",
      "Frames were selected.\n",
      "You can now label the frames using the function 'label_frames' (if you extracted enough frames for all videos).\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "deeplabcut.extract_frames(path_config_file, userfeedback=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found new frames..\n",
      "You can now check the labels, using 'check_labels' before proceeding. Then, you can use the function 'create_training_dataset' to create the training dataset.\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.label_frames(path_config_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Create training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMeUwgxPoEJP",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-10-21\\training-datasets\\iteration-0\\UnaugmentedDataSet_arena_chaseOct21  already exists!\n",
      "E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-10-21\\dlc-models\\iteration-0\\arena_chaseOct21-trainset95shuffle1  already exists!\n",
      "E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-10-21\\dlc-models\\iteration-0\\arena_chaseOct21-trainset95shuffle1//train  already exists!\n",
      "E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-10-21\\dlc-models\\iteration-0\\arena_chaseOct21-trainset95shuffle1//test  already exists!\n",
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.create_training_dataset(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c4FczXGDoEJU"
   },
   "source": [
    "#### 9. Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_pOvDq_2oEJW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0], [1], [2]],\n",
      " 'all_joints_names': ['COM', 'cal1', 'cal2'],\n",
      " 'batch_size': 1,\n",
      " 'bottomheight': 400,\n",
      " 'crop': True,\n",
      " 'crop_pad': 0,\n",
      " 'cropratio': 0.4,\n",
      " 'dataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_arena_chaseOct21\\\\arena_chase_Andy95shuffle1.mat',\n",
      " 'dataset_type': 'default',\n",
      " 'deterministic': False,\n",
      " 'display_iters': 1000,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': 'C:\\\\Users\\\\LabAdmin\\\\.conda\\\\envs\\\\dlc-windowsGPU\\\\lib\\\\site-packages\\\\deeplabcut\\\\pose_estimation_tensorflow\\\\models\\\\pretrained\\\\resnet_v1_50.ckpt',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'leftwidth': 400,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 0.05,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'max_input_size': 1500,\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'metadataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_arena_chaseOct21\\\\Documentation_data-arena_chase_95shuffle1.pickle',\n",
      " 'min_input_size': 64,\n",
      " 'minsize': 100,\n",
      " 'mirror': False,\n",
      " 'multi_step': [[0.005, 10000],\n",
      "                [0.02, 430000],\n",
      "                [0.002, 730000],\n",
      "                [0.001, 1030000]],\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 3,\n",
      " 'optimizer': 'sgd',\n",
      " 'pos_dist_thresh': 17,\n",
      " 'project_path': 'E:\\\\Users\\\\Phil\\\\DeepLabCut\\\\dev\\\\arena_chase-Andy-2019-10-21',\n",
      " 'regularize': False,\n",
      " 'rightwidth': 400,\n",
      " 'save_iters': 50000,\n",
      " 'scale_jitter_lo': 0.5,\n",
      " 'scale_jitter_up': 1.25,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': 'E:\\\\Users\\\\Phil\\\\DeepLabCut\\\\dev\\\\arena_chase-Andy-2019-10-21\\\\dlc-models\\\\iteration-0\\\\arena_chaseOct21-trainset95shuffle1\\\\train\\\\snapshot',\n",
      " 'stride': 8.0,\n",
      " 'topheight': 400,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with standard pose-dataset loader.\n",
      "WARNING:tensorflow:From C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt\n",
      "Max_iters overwritten as 100000\n",
      "Display_iters overwritten as 50\n",
      "Training parameter:\n",
      "{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'weigh_only_present_joints': False, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': 'E:\\\\Users\\\\Phil\\\\DeepLabCut\\\\dev\\\\arena_chase-Andy-2019-10-21\\\\dlc-models\\\\iteration-0\\\\arena_chaseOct21-trainset95shuffle1\\\\train\\\\snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'mirror': False, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 1, 'dataset_type': 'default', 'deterministic': False, 'crop': True, 'cropratio': 0.4, 'minsize': 100, 'leftwidth': 400, 'rightwidth': 400, 'topheight': 400, 'bottomheight': 400, 'all_joints': [[0], [1], [2]], 'all_joints_names': ['COM', 'cal1', 'cal2'], 'dataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_arena_chaseOct21\\\\arena_chase_Andy95shuffle1.mat', 'display_iters': 1000, 'init_weights': 'C:\\\\Users\\\\LabAdmin\\\\.conda\\\\envs\\\\dlc-windowsGPU\\\\lib\\\\site-packages\\\\deeplabcut\\\\pose_estimation_tensorflow\\\\models\\\\pretrained\\\\resnet_v1_50.ckpt', 'max_input_size': 1500, 'metadataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_arena_chaseOct21\\\\Documentation_data-arena_chase_95shuffle1.pickle', 'min_input_size': 64, 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'net_type': 'resnet_50', 'num_joints': 3, 'pos_dist_thresh': 17, 'project_path': 'E:\\\\Users\\\\Phil\\\\DeepLabCut\\\\dev\\\\arena_chase-Andy-2019-10-21', 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25}\n",
      "Starting training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 50 loss: 0.0923 lr: 0.005\n",
      "iteration: 100 loss: 0.0253 lr: 0.005\n",
      "iteration: 150 loss: 0.0205 lr: 0.005\n",
      "iteration: 200 loss: 0.0204 lr: 0.005\n",
      "iteration: 250 loss: 0.0181 lr: 0.005\n",
      "iteration: 300 loss: 0.0150 lr: 0.005\n",
      "iteration: 350 loss: 0.0158 lr: 0.005\n",
      "iteration: 400 loss: 0.0131 lr: 0.005\n",
      "iteration: 450 loss: 0.0139 lr: 0.005\n",
      "iteration: 500 loss: 0.0129 lr: 0.005\n",
      "iteration: 550 loss: 0.0141 lr: 0.005\n",
      "iteration: 600 loss: 0.0109 lr: 0.005\n",
      "iteration: 650 loss: 0.0120 lr: 0.005\n",
      "iteration: 700 loss: 0.0122 lr: 0.005\n",
      "iteration: 750 loss: 0.0109 lr: 0.005\n",
      "iteration: 800 loss: 0.0088 lr: 0.005\n",
      "iteration: 850 loss: 0.0101 lr: 0.005\n",
      "iteration: 900 loss: 0.0095 lr: 0.005\n",
      "iteration: 950 loss: 0.0093 lr: 0.005\n",
      "iteration: 1000 loss: 0.0084 lr: 0.005\n",
      "iteration: 1050 loss: 0.0073 lr: 0.005\n",
      "iteration: 1100 loss: 0.0094 lr: 0.005\n",
      "iteration: 1150 loss: 0.0082 lr: 0.005\n",
      "iteration: 1200 loss: 0.0080 lr: 0.005\n",
      "iteration: 1250 loss: 0.0066 lr: 0.005\n",
      "iteration: 1300 loss: 0.0068 lr: 0.005\n",
      "iteration: 1350 loss: 0.0068 lr: 0.005\n",
      "iteration: 1400 loss: 0.0072 lr: 0.005\n",
      "iteration: 1450 loss: 0.0079 lr: 0.005\n",
      "iteration: 1500 loss: 0.0058 lr: 0.005\n",
      "iteration: 1550 loss: 0.0063 lr: 0.005\n",
      "iteration: 1600 loss: 0.0073 lr: 0.005\n",
      "iteration: 1650 loss: 0.0064 lr: 0.005\n",
      "iteration: 1700 loss: 0.0078 lr: 0.005\n",
      "iteration: 1750 loss: 0.0070 lr: 0.005\n",
      "iteration: 1800 loss: 0.0059 lr: 0.005\n",
      "iteration: 1850 loss: 0.0074 lr: 0.005\n",
      "iteration: 1900 loss: 0.0059 lr: 0.005\n",
      "iteration: 1950 loss: 0.0064 lr: 0.005\n",
      "iteration: 2000 loss: 0.0050 lr: 0.005\n",
      "iteration: 2050 loss: 0.0061 lr: 0.005\n",
      "iteration: 2100 loss: 0.0053 lr: 0.005\n",
      "iteration: 2150 loss: 0.0048 lr: 0.005\n",
      "iteration: 2200 loss: 0.0059 lr: 0.005\n",
      "iteration: 2250 loss: 0.0046 lr: 0.005\n",
      "iteration: 2300 loss: 0.0051 lr: 0.005\n",
      "iteration: 2350 loss: 0.0047 lr: 0.005\n",
      "iteration: 2400 loss: 0.0053 lr: 0.005\n",
      "iteration: 2450 loss: 0.0056 lr: 0.005\n",
      "iteration: 2500 loss: 0.0050 lr: 0.005\n",
      "iteration: 2550 loss: 0.0053 lr: 0.005\n",
      "iteration: 2600 loss: 0.0047 lr: 0.005\n",
      "iteration: 2650 loss: 0.0048 lr: 0.005\n",
      "iteration: 2700 loss: 0.0046 lr: 0.005\n",
      "iteration: 2750 loss: 0.0043 lr: 0.005\n",
      "iteration: 2800 loss: 0.0042 lr: 0.005\n",
      "iteration: 2850 loss: 0.0050 lr: 0.005\n",
      "iteration: 2900 loss: 0.0054 lr: 0.005\n",
      "iteration: 2950 loss: 0.0040 lr: 0.005\n",
      "iteration: 3000 loss: 0.0046 lr: 0.005\n",
      "iteration: 3050 loss: 0.0045 lr: 0.005\n",
      "iteration: 3100 loss: 0.0047 lr: 0.005\n",
      "iteration: 3150 loss: 0.0048 lr: 0.005\n",
      "iteration: 3200 loss: 0.0051 lr: 0.005\n",
      "iteration: 3250 loss: 0.0051 lr: 0.005\n",
      "iteration: 3300 loss: 0.0044 lr: 0.005\n",
      "iteration: 3350 loss: 0.0047 lr: 0.005\n",
      "iteration: 3400 loss: 0.0042 lr: 0.005\n",
      "iteration: 3450 loss: 0.0044 lr: 0.005\n",
      "iteration: 3500 loss: 0.0042 lr: 0.005\n",
      "iteration: 3550 loss: 0.0040 lr: 0.005\n",
      "iteration: 3600 loss: 0.0045 lr: 0.005\n",
      "iteration: 3650 loss: 0.0040 lr: 0.005\n",
      "iteration: 3700 loss: 0.0039 lr: 0.005\n",
      "iteration: 3750 loss: 0.0044 lr: 0.005\n",
      "iteration: 3800 loss: 0.0038 lr: 0.005\n",
      "iteration: 3850 loss: 0.0037 lr: 0.005\n",
      "iteration: 3900 loss: 0.0040 lr: 0.005\n",
      "iteration: 3950 loss: 0.0039 lr: 0.005\n",
      "iteration: 4000 loss: 0.0041 lr: 0.005\n",
      "iteration: 4050 loss: 0.0040 lr: 0.005\n",
      "iteration: 4100 loss: 0.0040 lr: 0.005\n",
      "iteration: 4150 loss: 0.0048 lr: 0.005\n",
      "iteration: 4200 loss: 0.0038 lr: 0.005\n",
      "iteration: 4250 loss: 0.0040 lr: 0.005\n",
      "iteration: 4300 loss: 0.0041 lr: 0.005\n",
      "iteration: 4350 loss: 0.0037 lr: 0.005\n",
      "iteration: 4400 loss: 0.0040 lr: 0.005\n",
      "iteration: 4450 loss: 0.0036 lr: 0.005\n",
      "iteration: 4500 loss: 0.0039 lr: 0.005\n",
      "iteration: 4550 loss: 0.0037 lr: 0.005\n",
      "iteration: 4600 loss: 0.0042 lr: 0.005\n",
      "iteration: 4650 loss: 0.0037 lr: 0.005\n",
      "iteration: 4700 loss: 0.0035 lr: 0.005\n",
      "iteration: 4750 loss: 0.0044 lr: 0.005\n",
      "iteration: 4800 loss: 0.0044 lr: 0.005\n",
      "iteration: 4850 loss: 0.0037 lr: 0.005\n",
      "iteration: 4900 loss: 0.0038 lr: 0.005\n",
      "iteration: 4950 loss: 0.0034 lr: 0.005\n",
      "iteration: 5000 loss: 0.0038 lr: 0.005\n",
      "iteration: 5050 loss: 0.0035 lr: 0.005\n",
      "iteration: 5100 loss: 0.0038 lr: 0.005\n",
      "iteration: 5150 loss: 0.0033 lr: 0.005\n",
      "iteration: 5200 loss: 0.0042 lr: 0.005\n",
      "iteration: 5250 loss: 0.0031 lr: 0.005\n",
      "iteration: 5300 loss: 0.0032 lr: 0.005\n",
      "iteration: 5350 loss: 0.0037 lr: 0.005\n",
      "iteration: 5400 loss: 0.0040 lr: 0.005\n",
      "iteration: 5450 loss: 0.0035 lr: 0.005\n",
      "iteration: 5500 loss: 0.0035 lr: 0.005\n",
      "iteration: 5550 loss: 0.0034 lr: 0.005\n",
      "iteration: 5600 loss: 0.0031 lr: 0.005\n",
      "iteration: 5650 loss: 0.0034 lr: 0.005\n",
      "iteration: 5700 loss: 0.0029 lr: 0.005\n",
      "iteration: 5750 loss: 0.0036 lr: 0.005\n",
      "iteration: 5800 loss: 0.0035 lr: 0.005\n",
      "iteration: 5850 loss: 0.0034 lr: 0.005\n",
      "iteration: 5900 loss: 0.0030 lr: 0.005\n",
      "iteration: 5950 loss: 0.0032 lr: 0.005\n",
      "iteration: 6000 loss: 0.0030 lr: 0.005\n",
      "iteration: 6050 loss: 0.0033 lr: 0.005\n",
      "iteration: 6100 loss: 0.0030 lr: 0.005\n",
      "iteration: 6150 loss: 0.0030 lr: 0.005\n",
      "iteration: 6200 loss: 0.0031 lr: 0.005\n",
      "iteration: 6250 loss: 0.0025 lr: 0.005\n",
      "iteration: 6300 loss: 0.0032 lr: 0.005\n",
      "iteration: 6350 loss: 0.0027 lr: 0.005\n",
      "iteration: 6400 loss: 0.0029 lr: 0.005\n",
      "iteration: 6450 loss: 0.0029 lr: 0.005\n",
      "iteration: 6500 loss: 0.0036 lr: 0.005\n",
      "iteration: 6550 loss: 0.0030 lr: 0.005\n",
      "iteration: 6600 loss: 0.0032 lr: 0.005\n",
      "iteration: 6650 loss: 0.0027 lr: 0.005\n",
      "iteration: 6700 loss: 0.0032 lr: 0.005\n",
      "iteration: 6750 loss: 0.0029 lr: 0.005\n",
      "iteration: 6800 loss: 0.0032 lr: 0.005\n",
      "iteration: 6850 loss: 0.0038 lr: 0.005\n",
      "iteration: 6900 loss: 0.0028 lr: 0.005\n",
      "iteration: 6950 loss: 0.0029 lr: 0.005\n",
      "iteration: 7000 loss: 0.0033 lr: 0.005\n",
      "iteration: 7050 loss: 0.0030 lr: 0.005\n",
      "iteration: 7100 loss: 0.0027 lr: 0.005\n",
      "iteration: 7150 loss: 0.0026 lr: 0.005\n",
      "iteration: 7200 loss: 0.0033 lr: 0.005\n",
      "iteration: 7250 loss: 0.0025 lr: 0.005\n",
      "iteration: 7300 loss: 0.0028 lr: 0.005\n",
      "iteration: 7350 loss: 0.0032 lr: 0.005\n",
      "iteration: 7400 loss: 0.0032 lr: 0.005\n",
      "iteration: 7450 loss: 0.0032 lr: 0.005\n",
      "iteration: 7500 loss: 0.0033 lr: 0.005\n",
      "iteration: 7550 loss: 0.0030 lr: 0.005\n",
      "iteration: 7600 loss: 0.0031 lr: 0.005\n",
      "iteration: 7650 loss: 0.0033 lr: 0.005\n",
      "iteration: 7700 loss: 0.0030 lr: 0.005\n",
      "iteration: 7750 loss: 0.0027 lr: 0.005\n",
      "iteration: 7800 loss: 0.0022 lr: 0.005\n",
      "iteration: 7850 loss: 0.0025 lr: 0.005\n",
      "iteration: 7900 loss: 0.0025 lr: 0.005\n",
      "iteration: 7950 loss: 0.0024 lr: 0.005\n",
      "iteration: 8000 loss: 0.0028 lr: 0.005\n",
      "iteration: 8050 loss: 0.0024 lr: 0.005\n",
      "iteration: 8100 loss: 0.0023 lr: 0.005\n",
      "iteration: 8150 loss: 0.0030 lr: 0.005\n",
      "iteration: 8200 loss: 0.0029 lr: 0.005\n",
      "iteration: 8250 loss: 0.0024 lr: 0.005\n",
      "iteration: 8300 loss: 0.0027 lr: 0.005\n",
      "iteration: 8350 loss: 0.0028 lr: 0.005\n",
      "iteration: 8400 loss: 0.0031 lr: 0.005\n",
      "iteration: 8450 loss: 0.0030 lr: 0.005\n",
      "iteration: 8500 loss: 0.0030 lr: 0.005\n",
      "iteration: 8550 loss: 0.0024 lr: 0.005\n",
      "iteration: 8600 loss: 0.0027 lr: 0.005\n",
      "iteration: 8650 loss: 0.0024 lr: 0.005\n",
      "iteration: 8700 loss: 0.0028 lr: 0.005\n",
      "iteration: 8750 loss: 0.0028 lr: 0.005\n",
      "iteration: 8800 loss: 0.0022 lr: 0.005\n",
      "iteration: 8850 loss: 0.0028 lr: 0.005\n",
      "iteration: 8900 loss: 0.0026 lr: 0.005\n",
      "iteration: 8950 loss: 0.0022 lr: 0.005\n",
      "iteration: 9000 loss: 0.0019 lr: 0.005\n",
      "iteration: 9050 loss: 0.0026 lr: 0.005\n",
      "iteration: 9100 loss: 0.0028 lr: 0.005\n",
      "iteration: 9150 loss: 0.0022 lr: 0.005\n",
      "iteration: 9200 loss: 0.0027 lr: 0.005\n",
      "iteration: 9250 loss: 0.0024 lr: 0.005\n",
      "iteration: 9300 loss: 0.0025 lr: 0.005\n",
      "iteration: 9350 loss: 0.0025 lr: 0.005\n",
      "iteration: 9400 loss: 0.0023 lr: 0.005\n",
      "iteration: 9450 loss: 0.0024 lr: 0.005\n",
      "iteration: 9500 loss: 0.0028 lr: 0.005\n",
      "iteration: 9550 loss: 0.0021 lr: 0.005\n",
      "iteration: 9600 loss: 0.0023 lr: 0.005\n",
      "iteration: 9650 loss: 0.0024 lr: 0.005\n",
      "iteration: 9700 loss: 0.0024 lr: 0.005\n",
      "iteration: 9750 loss: 0.0025 lr: 0.005\n",
      "iteration: 9800 loss: 0.0027 lr: 0.005\n",
      "iteration: 9850 loss: 0.0024 lr: 0.005\n",
      "iteration: 9900 loss: 0.0026 lr: 0.005\n",
      "iteration: 9950 loss: 0.0020 lr: 0.005\n",
      "iteration: 10000 loss: 0.0021 lr: 0.005\n",
      "iteration: 10050 loss: 0.0050 lr: 0.02\n",
      "iteration: 10100 loss: 0.0075 lr: 0.02\n",
      "iteration: 10150 loss: 0.0071 lr: 0.02\n",
      "iteration: 10200 loss: 0.0066 lr: 0.02\n",
      "iteration: 10250 loss: 0.0063 lr: 0.02\n",
      "iteration: 10300 loss: 0.0054 lr: 0.02\n",
      "iteration: 10350 loss: 0.0059 lr: 0.02\n",
      "iteration: 10400 loss: 0.0063 lr: 0.02\n",
      "iteration: 10450 loss: 0.0068 lr: 0.02\n",
      "iteration: 10500 loss: 0.0066 lr: 0.02\n",
      "iteration: 10550 loss: 0.0054 lr: 0.02\n",
      "iteration: 10600 loss: 0.0054 lr: 0.02\n",
      "iteration: 10650 loss: 0.0046 lr: 0.02\n",
      "iteration: 10700 loss: 0.0064 lr: 0.02\n",
      "iteration: 10750 loss: 0.0054 lr: 0.02\n",
      "iteration: 10800 loss: 0.0057 lr: 0.02\n",
      "iteration: 10850 loss: 0.0056 lr: 0.02\n",
      "iteration: 10900 loss: 0.0046 lr: 0.02\n",
      "iteration: 10950 loss: 0.0047 lr: 0.02\n",
      "iteration: 11000 loss: 0.0044 lr: 0.02\n",
      "iteration: 11050 loss: 0.0046 lr: 0.02\n",
      "iteration: 11100 loss: 0.0043 lr: 0.02\n",
      "iteration: 11150 loss: 0.0043 lr: 0.02\n",
      "iteration: 11200 loss: 0.0044 lr: 0.02\n",
      "iteration: 11250 loss: 0.0039 lr: 0.02\n",
      "iteration: 11300 loss: 0.0035 lr: 0.02\n",
      "iteration: 11350 loss: 0.0038 lr: 0.02\n",
      "iteration: 11400 loss: 0.0037 lr: 0.02\n",
      "iteration: 11450 loss: 0.0052 lr: 0.02\n",
      "iteration: 11500 loss: 0.0037 lr: 0.02\n",
      "iteration: 11550 loss: 0.0038 lr: 0.02\n",
      "iteration: 11600 loss: 0.0037 lr: 0.02\n",
      "iteration: 11650 loss: 0.0039 lr: 0.02\n",
      "iteration: 11700 loss: 0.0041 lr: 0.02\n",
      "iteration: 11750 loss: 0.0035 lr: 0.02\n",
      "iteration: 11800 loss: 0.0046 lr: 0.02\n",
      "iteration: 11850 loss: 0.0035 lr: 0.02\n",
      "iteration: 11900 loss: 0.0032 lr: 0.02\n",
      "iteration: 11950 loss: 0.0031 lr: 0.02\n",
      "iteration: 12000 loss: 0.0035 lr: 0.02\n",
      "iteration: 12050 loss: 0.0038 lr: 0.02\n",
      "iteration: 12100 loss: 0.0032 lr: 0.02\n",
      "iteration: 12150 loss: 0.0033 lr: 0.02\n",
      "iteration: 12200 loss: 0.0031 lr: 0.02\n",
      "iteration: 12250 loss: 0.0025 lr: 0.02\n",
      "iteration: 12300 loss: 0.0029 lr: 0.02\n",
      "iteration: 12350 loss: 0.0031 lr: 0.02\n",
      "iteration: 12400 loss: 0.0036 lr: 0.02\n",
      "iteration: 12450 loss: 0.0030 lr: 0.02\n",
      "iteration: 12500 loss: 0.0032 lr: 0.02\n",
      "iteration: 12550 loss: 0.0027 lr: 0.02\n",
      "iteration: 12600 loss: 0.0027 lr: 0.02\n",
      "iteration: 12650 loss: 0.0036 lr: 0.02\n",
      "iteration: 12700 loss: 0.0029 lr: 0.02\n",
      "iteration: 12750 loss: 0.0029 lr: 0.02\n",
      "iteration: 12800 loss: 0.0027 lr: 0.02\n",
      "iteration: 12850 loss: 0.0030 lr: 0.02\n",
      "iteration: 12900 loss: 0.0028 lr: 0.02\n",
      "iteration: 12950 loss: 0.0031 lr: 0.02\n",
      "iteration: 13000 loss: 0.0027 lr: 0.02\n",
      "iteration: 13050 loss: 0.0032 lr: 0.02\n",
      "iteration: 13100 loss: 0.0036 lr: 0.02\n",
      "iteration: 13150 loss: 0.0028 lr: 0.02\n",
      "iteration: 13200 loss: 0.0031 lr: 0.02\n",
      "iteration: 13250 loss: 0.0025 lr: 0.02\n",
      "iteration: 13300 loss: 0.0027 lr: 0.02\n",
      "iteration: 13350 loss: 0.0031 lr: 0.02\n",
      "iteration: 13400 loss: 0.0027 lr: 0.02\n",
      "iteration: 13450 loss: 0.0027 lr: 0.02\n",
      "iteration: 13500 loss: 0.0027 lr: 0.02\n",
      "iteration: 13550 loss: 0.0029 lr: 0.02\n",
      "iteration: 13600 loss: 0.0026 lr: 0.02\n",
      "iteration: 13650 loss: 0.0030 lr: 0.02\n",
      "iteration: 13700 loss: 0.0028 lr: 0.02\n",
      "iteration: 13750 loss: 0.0027 lr: 0.02\n",
      "iteration: 13800 loss: 0.0026 lr: 0.02\n",
      "iteration: 13850 loss: 0.0024 lr: 0.02\n",
      "iteration: 13900 loss: 0.0023 lr: 0.02\n",
      "iteration: 13950 loss: 0.0027 lr: 0.02\n",
      "iteration: 14000 loss: 0.0027 lr: 0.02\n",
      "iteration: 14050 loss: 0.0028 lr: 0.02\n",
      "iteration: 14100 loss: 0.0033 lr: 0.02\n",
      "iteration: 14150 loss: 0.0024 lr: 0.02\n",
      "iteration: 14200 loss: 0.0025 lr: 0.02\n",
      "iteration: 14250 loss: 0.0030 lr: 0.02\n",
      "iteration: 14300 loss: 0.0030 lr: 0.02\n",
      "iteration: 14350 loss: 0.0026 lr: 0.02\n",
      "iteration: 14400 loss: 0.0025 lr: 0.02\n",
      "iteration: 14450 loss: 0.0021 lr: 0.02\n",
      "iteration: 14500 loss: 0.0023 lr: 0.02\n",
      "iteration: 14550 loss: 0.0023 lr: 0.02\n",
      "iteration: 14600 loss: 0.0024 lr: 0.02\n",
      "iteration: 14650 loss: 0.0025 lr: 0.02\n",
      "iteration: 14700 loss: 0.0022 lr: 0.02\n",
      "iteration: 14750 loss: 0.0020 lr: 0.02\n",
      "iteration: 14800 loss: 0.0022 lr: 0.02\n",
      "iteration: 14850 loss: 0.0021 lr: 0.02\n",
      "iteration: 14900 loss: 0.0023 lr: 0.02\n",
      "iteration: 14950 loss: 0.0027 lr: 0.02\n",
      "iteration: 15000 loss: 0.0021 lr: 0.02\n",
      "iteration: 15050 loss: 0.0021 lr: 0.02\n",
      "iteration: 15100 loss: 0.0023 lr: 0.02\n",
      "iteration: 15150 loss: 0.0024 lr: 0.02\n",
      "iteration: 15200 loss: 0.0022 lr: 0.02\n",
      "iteration: 15250 loss: 0.0020 lr: 0.02\n",
      "iteration: 15300 loss: 0.0020 lr: 0.02\n",
      "iteration: 15350 loss: 0.0022 lr: 0.02\n",
      "iteration: 15400 loss: 0.0023 lr: 0.02\n",
      "iteration: 15450 loss: 0.0022 lr: 0.02\n",
      "iteration: 15500 loss: 0.0022 lr: 0.02\n",
      "iteration: 15550 loss: 0.0019 lr: 0.02\n",
      "iteration: 15600 loss: 0.0022 lr: 0.02\n",
      "iteration: 15650 loss: 0.0025 lr: 0.02\n",
      "iteration: 15700 loss: 0.0023 lr: 0.02\n",
      "iteration: 15750 loss: 0.0021 lr: 0.02\n",
      "iteration: 15800 loss: 0.0020 lr: 0.02\n",
      "iteration: 15850 loss: 0.0022 lr: 0.02\n",
      "iteration: 15900 loss: 0.0023 lr: 0.02\n",
      "iteration: 15950 loss: 0.0022 lr: 0.02\n",
      "iteration: 16000 loss: 0.0028 lr: 0.02\n",
      "iteration: 16050 loss: 0.0022 lr: 0.02\n",
      "iteration: 16100 loss: 0.0023 lr: 0.02\n",
      "iteration: 16150 loss: 0.0023 lr: 0.02\n",
      "iteration: 16200 loss: 0.0024 lr: 0.02\n",
      "iteration: 16250 loss: 0.0021 lr: 0.02\n",
      "iteration: 16300 loss: 0.0023 lr: 0.02\n",
      "iteration: 16350 loss: 0.0021 lr: 0.02\n",
      "iteration: 16400 loss: 0.0021 lr: 0.02\n",
      "iteration: 16450 loss: 0.0021 lr: 0.02\n",
      "iteration: 16500 loss: 0.0017 lr: 0.02\n",
      "iteration: 16550 loss: 0.0018 lr: 0.02\n",
      "iteration: 16600 loss: 0.0020 lr: 0.02\n",
      "iteration: 16650 loss: 0.0025 lr: 0.02\n",
      "iteration: 16700 loss: 0.0026 lr: 0.02\n",
      "iteration: 16750 loss: 0.0021 lr: 0.02\n",
      "iteration: 16800 loss: 0.0023 lr: 0.02\n",
      "iteration: 16850 loss: 0.0022 lr: 0.02\n",
      "iteration: 16900 loss: 0.0021 lr: 0.02\n",
      "iteration: 16950 loss: 0.0021 lr: 0.02\n",
      "iteration: 17000 loss: 0.0021 lr: 0.02\n",
      "iteration: 17050 loss: 0.0025 lr: 0.02\n",
      "iteration: 17100 loss: 0.0021 lr: 0.02\n",
      "iteration: 17150 loss: 0.0020 lr: 0.02\n",
      "iteration: 17200 loss: 0.0019 lr: 0.02\n",
      "iteration: 17250 loss: 0.0019 lr: 0.02\n",
      "iteration: 17300 loss: 0.0017 lr: 0.02\n",
      "iteration: 17350 loss: 0.0019 lr: 0.02\n",
      "iteration: 17400 loss: 0.0019 lr: 0.02\n",
      "iteration: 17450 loss: 0.0020 lr: 0.02\n",
      "iteration: 17500 loss: 0.0023 lr: 0.02\n",
      "iteration: 17550 loss: 0.0022 lr: 0.02\n",
      "iteration: 17600 loss: 0.0023 lr: 0.02\n",
      "iteration: 17650 loss: 0.0023 lr: 0.02\n",
      "iteration: 17700 loss: 0.0020 lr: 0.02\n",
      "iteration: 17750 loss: 0.0020 lr: 0.02\n",
      "iteration: 17800 loss: 0.0023 lr: 0.02\n",
      "iteration: 17850 loss: 0.0023 lr: 0.02\n",
      "iteration: 17900 loss: 0.0020 lr: 0.02\n",
      "iteration: 17950 loss: 0.0019 lr: 0.02\n",
      "iteration: 18000 loss: 0.0020 lr: 0.02\n",
      "iteration: 18050 loss: 0.0019 lr: 0.02\n",
      "iteration: 18100 loss: 0.0018 lr: 0.02\n",
      "iteration: 18150 loss: 0.0020 lr: 0.02\n",
      "iteration: 18200 loss: 0.0020 lr: 0.02\n",
      "iteration: 18250 loss: 0.0024 lr: 0.02\n",
      "iteration: 18300 loss: 0.0018 lr: 0.02\n",
      "iteration: 18350 loss: 0.0019 lr: 0.02\n",
      "iteration: 18400 loss: 0.0018 lr: 0.02\n",
      "iteration: 18450 loss: 0.0019 lr: 0.02\n",
      "iteration: 18500 loss: 0.0021 lr: 0.02\n",
      "iteration: 18550 loss: 0.0019 lr: 0.02\n",
      "iteration: 18600 loss: 0.0020 lr: 0.02\n",
      "iteration: 18650 loss: 0.0022 lr: 0.02\n",
      "iteration: 18700 loss: 0.0020 lr: 0.02\n",
      "iteration: 18750 loss: 0.0021 lr: 0.02\n",
      "iteration: 18800 loss: 0.0017 lr: 0.02\n",
      "iteration: 18850 loss: 0.0023 lr: 0.02\n",
      "iteration: 18900 loss: 0.0018 lr: 0.02\n",
      "iteration: 18950 loss: 0.0018 lr: 0.02\n",
      "iteration: 19000 loss: 0.0020 lr: 0.02\n",
      "iteration: 19050 loss: 0.0018 lr: 0.02\n",
      "iteration: 19100 loss: 0.0023 lr: 0.02\n",
      "iteration: 19150 loss: 0.0024 lr: 0.02\n",
      "iteration: 19200 loss: 0.0019 lr: 0.02\n",
      "iteration: 19250 loss: 0.0019 lr: 0.02\n",
      "iteration: 19300 loss: 0.0021 lr: 0.02\n",
      "iteration: 19350 loss: 0.0017 lr: 0.02\n",
      "iteration: 19400 loss: 0.0023 lr: 0.02\n",
      "iteration: 19450 loss: 0.0022 lr: 0.02\n",
      "iteration: 19500 loss: 0.0021 lr: 0.02\n",
      "iteration: 19550 loss: 0.0021 lr: 0.02\n",
      "iteration: 19600 loss: 0.0019 lr: 0.02\n",
      "iteration: 19650 loss: 0.0019 lr: 0.02\n",
      "iteration: 19700 loss: 0.0019 lr: 0.02\n",
      "iteration: 19750 loss: 0.0019 lr: 0.02\n",
      "iteration: 19800 loss: 0.0020 lr: 0.02\n",
      "iteration: 19850 loss: 0.0020 lr: 0.02\n",
      "iteration: 19900 loss: 0.0022 lr: 0.02\n",
      "iteration: 19950 loss: 0.0021 lr: 0.02\n",
      "iteration: 20000 loss: 0.0021 lr: 0.02\n",
      "iteration: 20050 loss: 0.0020 lr: 0.02\n",
      "iteration: 20100 loss: 0.0019 lr: 0.02\n",
      "iteration: 20150 loss: 0.0020 lr: 0.02\n",
      "iteration: 20200 loss: 0.0019 lr: 0.02\n",
      "iteration: 20250 loss: 0.0021 lr: 0.02\n",
      "iteration: 20300 loss: 0.0021 lr: 0.02\n",
      "iteration: 20350 loss: 0.0020 lr: 0.02\n",
      "iteration: 20400 loss: 0.0017 lr: 0.02\n",
      "iteration: 20450 loss: 0.0016 lr: 0.02\n",
      "iteration: 20500 loss: 0.0018 lr: 0.02\n",
      "iteration: 20550 loss: 0.0018 lr: 0.02\n",
      "iteration: 20600 loss: 0.0020 lr: 0.02\n",
      "iteration: 20650 loss: 0.0018 lr: 0.02\n",
      "iteration: 20700 loss: 0.0017 lr: 0.02\n",
      "iteration: 20750 loss: 0.0019 lr: 0.02\n",
      "iteration: 20800 loss: 0.0018 lr: 0.02\n",
      "iteration: 20850 loss: 0.0022 lr: 0.02\n",
      "iteration: 20900 loss: 0.0019 lr: 0.02\n",
      "iteration: 20950 loss: 0.0021 lr: 0.02\n",
      "iteration: 21000 loss: 0.0022 lr: 0.02\n",
      "iteration: 21050 loss: 0.0019 lr: 0.02\n",
      "iteration: 21100 loss: 0.0017 lr: 0.02\n",
      "iteration: 21150 loss: 0.0017 lr: 0.02\n",
      "iteration: 21200 loss: 0.0017 lr: 0.02\n",
      "iteration: 21250 loss: 0.0017 lr: 0.02\n",
      "iteration: 21300 loss: 0.0016 lr: 0.02\n",
      "iteration: 21350 loss: 0.0018 lr: 0.02\n",
      "iteration: 21400 loss: 0.0020 lr: 0.02\n",
      "iteration: 21450 loss: 0.0023 lr: 0.02\n",
      "iteration: 21500 loss: 0.0020 lr: 0.02\n",
      "iteration: 21550 loss: 0.0018 lr: 0.02\n",
      "iteration: 21600 loss: 0.0018 lr: 0.02\n",
      "iteration: 21650 loss: 0.0015 lr: 0.02\n",
      "iteration: 21700 loss: 0.0020 lr: 0.02\n",
      "iteration: 21750 loss: 0.0020 lr: 0.02\n",
      "iteration: 21800 loss: 0.0020 lr: 0.02\n",
      "iteration: 21850 loss: 0.0019 lr: 0.02\n",
      "iteration: 21900 loss: 0.0016 lr: 0.02\n",
      "iteration: 21950 loss: 0.0019 lr: 0.02\n",
      "iteration: 22000 loss: 0.0016 lr: 0.02\n",
      "iteration: 22050 loss: 0.0019 lr: 0.02\n",
      "iteration: 22100 loss: 0.0015 lr: 0.02\n",
      "iteration: 22150 loss: 0.0018 lr: 0.02\n",
      "iteration: 22200 loss: 0.0016 lr: 0.02\n",
      "iteration: 22250 loss: 0.0017 lr: 0.02\n",
      "iteration: 22300 loss: 0.0018 lr: 0.02\n",
      "iteration: 22350 loss: 0.0019 lr: 0.02\n",
      "iteration: 22400 loss: 0.0019 lr: 0.02\n",
      "iteration: 22450 loss: 0.0020 lr: 0.02\n",
      "iteration: 22500 loss: 0.0014 lr: 0.02\n",
      "iteration: 22550 loss: 0.0014 lr: 0.02\n",
      "iteration: 22600 loss: 0.0016 lr: 0.02\n",
      "iteration: 22650 loss: 0.0016 lr: 0.02\n",
      "iteration: 22700 loss: 0.0017 lr: 0.02\n",
      "iteration: 22750 loss: 0.0018 lr: 0.02\n",
      "iteration: 22800 loss: 0.0017 lr: 0.02\n",
      "iteration: 22850 loss: 0.0017 lr: 0.02\n",
      "iteration: 22900 loss: 0.0020 lr: 0.02\n",
      "iteration: 22950 loss: 0.0019 lr: 0.02\n",
      "iteration: 23000 loss: 0.0017 lr: 0.02\n",
      "iteration: 23050 loss: 0.0016 lr: 0.02\n",
      "iteration: 23100 loss: 0.0017 lr: 0.02\n",
      "iteration: 23150 loss: 0.0017 lr: 0.02\n",
      "iteration: 23200 loss: 0.0017 lr: 0.02\n",
      "iteration: 23250 loss: 0.0017 lr: 0.02\n",
      "iteration: 23300 loss: 0.0016 lr: 0.02\n",
      "iteration: 23350 loss: 0.0017 lr: 0.02\n",
      "iteration: 23400 loss: 0.0015 lr: 0.02\n",
      "iteration: 23450 loss: 0.0017 lr: 0.02\n",
      "iteration: 23500 loss: 0.0017 lr: 0.02\n",
      "iteration: 23550 loss: 0.0019 lr: 0.02\n",
      "iteration: 23600 loss: 0.0015 lr: 0.02\n",
      "iteration: 23650 loss: 0.0017 lr: 0.02\n",
      "iteration: 23700 loss: 0.0015 lr: 0.02\n",
      "iteration: 23750 loss: 0.0017 lr: 0.02\n",
      "iteration: 23800 loss: 0.0018 lr: 0.02\n",
      "iteration: 23850 loss: 0.0017 lr: 0.02\n",
      "iteration: 23900 loss: 0.0015 lr: 0.02\n",
      "iteration: 23950 loss: 0.0019 lr: 0.02\n",
      "iteration: 24000 loss: 0.0018 lr: 0.02\n",
      "iteration: 24050 loss: 0.0020 lr: 0.02\n",
      "iteration: 24100 loss: 0.0015 lr: 0.02\n",
      "iteration: 24150 loss: 0.0017 lr: 0.02\n",
      "iteration: 24200 loss: 0.0017 lr: 0.02\n",
      "iteration: 24250 loss: 0.0014 lr: 0.02\n",
      "iteration: 24300 loss: 0.0019 lr: 0.02\n",
      "iteration: 24350 loss: 0.0016 lr: 0.02\n",
      "iteration: 24400 loss: 0.0016 lr: 0.02\n",
      "iteration: 24450 loss: 0.0015 lr: 0.02\n",
      "iteration: 24500 loss: 0.0017 lr: 0.02\n",
      "iteration: 24550 loss: 0.0017 lr: 0.02\n",
      "iteration: 24600 loss: 0.0014 lr: 0.02\n",
      "iteration: 24650 loss: 0.0014 lr: 0.02\n",
      "iteration: 24700 loss: 0.0016 lr: 0.02\n",
      "iteration: 24750 loss: 0.0018 lr: 0.02\n",
      "iteration: 24800 loss: 0.0017 lr: 0.02\n",
      "iteration: 24850 loss: 0.0016 lr: 0.02\n",
      "iteration: 24900 loss: 0.0018 lr: 0.02\n",
      "iteration: 24950 loss: 0.0016 lr: 0.02\n",
      "iteration: 25000 loss: 0.0015 lr: 0.02\n",
      "iteration: 25050 loss: 0.0018 lr: 0.02\n",
      "iteration: 25100 loss: 0.0025 lr: 0.02\n",
      "iteration: 25150 loss: 0.0016 lr: 0.02\n",
      "iteration: 25200 loss: 0.0016 lr: 0.02\n",
      "iteration: 25250 loss: 0.0017 lr: 0.02\n",
      "iteration: 25300 loss: 0.0015 lr: 0.02\n",
      "iteration: 25350 loss: 0.0018 lr: 0.02\n",
      "iteration: 25400 loss: 0.0015 lr: 0.02\n",
      "iteration: 25450 loss: 0.0017 lr: 0.02\n",
      "iteration: 25500 loss: 0.0016 lr: 0.02\n",
      "iteration: 25550 loss: 0.0016 lr: 0.02\n",
      "iteration: 25600 loss: 0.0014 lr: 0.02\n",
      "iteration: 25650 loss: 0.0018 lr: 0.02\n",
      "iteration: 25700 loss: 0.0016 lr: 0.02\n",
      "iteration: 25750 loss: 0.0017 lr: 0.02\n",
      "iteration: 25800 loss: 0.0017 lr: 0.02\n",
      "iteration: 25850 loss: 0.0015 lr: 0.02\n",
      "iteration: 25900 loss: 0.0018 lr: 0.02\n",
      "iteration: 25950 loss: 0.0015 lr: 0.02\n",
      "iteration: 26000 loss: 0.0015 lr: 0.02\n",
      "iteration: 26050 loss: 0.0017 lr: 0.02\n",
      "iteration: 26100 loss: 0.0015 lr: 0.02\n",
      "iteration: 26150 loss: 0.0027 lr: 0.02\n",
      "iteration: 26200 loss: 0.0020 lr: 0.02\n",
      "iteration: 26250 loss: 0.0016 lr: 0.02\n",
      "iteration: 26300 loss: 0.0016 lr: 0.02\n",
      "iteration: 26350 loss: 0.0017 lr: 0.02\n",
      "iteration: 26400 loss: 0.0019 lr: 0.02\n",
      "iteration: 26450 loss: 0.0016 lr: 0.02\n",
      "iteration: 26500 loss: 0.0016 lr: 0.02\n",
      "iteration: 26550 loss: 0.0017 lr: 0.02\n",
      "iteration: 26600 loss: 0.0018 lr: 0.02\n",
      "iteration: 26650 loss: 0.0015 lr: 0.02\n",
      "iteration: 26700 loss: 0.0017 lr: 0.02\n",
      "iteration: 26750 loss: 0.0015 lr: 0.02\n",
      "iteration: 26800 loss: 0.0015 lr: 0.02\n",
      "iteration: 26850 loss: 0.0015 lr: 0.02\n",
      "iteration: 26900 loss: 0.0016 lr: 0.02\n",
      "iteration: 26950 loss: 0.0017 lr: 0.02\n",
      "iteration: 27000 loss: 0.0017 lr: 0.02\n",
      "iteration: 27050 loss: 0.0015 lr: 0.02\n",
      "iteration: 27100 loss: 0.0017 lr: 0.02\n",
      "iteration: 27150 loss: 0.0016 lr: 0.02\n",
      "iteration: 27200 loss: 0.0017 lr: 0.02\n",
      "iteration: 27250 loss: 0.0015 lr: 0.02\n",
      "iteration: 27300 loss: 0.0015 lr: 0.02\n",
      "iteration: 27350 loss: 0.0014 lr: 0.02\n",
      "iteration: 27400 loss: 0.0014 lr: 0.02\n",
      "iteration: 27450 loss: 0.0015 lr: 0.02\n",
      "iteration: 27500 loss: 0.0016 lr: 0.02\n",
      "iteration: 27550 loss: 0.0017 lr: 0.02\n",
      "iteration: 27600 loss: 0.0015 lr: 0.02\n",
      "iteration: 27650 loss: 0.0016 lr: 0.02\n",
      "iteration: 27700 loss: 0.0014 lr: 0.02\n",
      "iteration: 27750 loss: 0.0015 lr: 0.02\n",
      "iteration: 27800 loss: 0.0017 lr: 0.02\n",
      "iteration: 27850 loss: 0.0018 lr: 0.02\n",
      "iteration: 27900 loss: 0.0019 lr: 0.02\n",
      "iteration: 27950 loss: 0.0018 lr: 0.02\n",
      "iteration: 28000 loss: 0.0015 lr: 0.02\n",
      "iteration: 28050 loss: 0.0013 lr: 0.02\n",
      "iteration: 28100 loss: 0.0014 lr: 0.02\n",
      "iteration: 28150 loss: 0.0014 lr: 0.02\n",
      "iteration: 28200 loss: 0.0017 lr: 0.02\n",
      "iteration: 28250 loss: 0.0015 lr: 0.02\n",
      "iteration: 28300 loss: 0.0015 lr: 0.02\n",
      "iteration: 28350 loss: 0.0017 lr: 0.02\n",
      "iteration: 28400 loss: 0.0014 lr: 0.02\n",
      "iteration: 28450 loss: 0.0016 lr: 0.02\n",
      "iteration: 28500 loss: 0.0016 lr: 0.02\n",
      "iteration: 28550 loss: 0.0016 lr: 0.02\n",
      "iteration: 28600 loss: 0.0015 lr: 0.02\n",
      "iteration: 28650 loss: 0.0014 lr: 0.02\n",
      "iteration: 28700 loss: 0.0017 lr: 0.02\n",
      "iteration: 28750 loss: 0.0014 lr: 0.02\n",
      "iteration: 28800 loss: 0.0016 lr: 0.02\n",
      "iteration: 28850 loss: 0.0015 lr: 0.02\n",
      "iteration: 28900 loss: 0.0013 lr: 0.02\n",
      "iteration: 28950 loss: 0.0015 lr: 0.02\n",
      "iteration: 29000 loss: 0.0014 lr: 0.02\n",
      "iteration: 29050 loss: 0.0015 lr: 0.02\n",
      "iteration: 29100 loss: 0.0014 lr: 0.02\n",
      "iteration: 29150 loss: 0.0015 lr: 0.02\n",
      "iteration: 29200 loss: 0.0015 lr: 0.02\n",
      "iteration: 29250 loss: 0.0015 lr: 0.02\n",
      "iteration: 29300 loss: 0.0016 lr: 0.02\n",
      "iteration: 29350 loss: 0.0016 lr: 0.02\n",
      "iteration: 29400 loss: 0.0016 lr: 0.02\n",
      "iteration: 29450 loss: 0.0014 lr: 0.02\n",
      "iteration: 29500 loss: 0.0015 lr: 0.02\n",
      "iteration: 29550 loss: 0.0018 lr: 0.02\n",
      "iteration: 29600 loss: 0.0017 lr: 0.02\n",
      "iteration: 29650 loss: 0.0015 lr: 0.02\n",
      "iteration: 29700 loss: 0.0016 lr: 0.02\n",
      "iteration: 29750 loss: 0.0016 lr: 0.02\n",
      "iteration: 29800 loss: 0.0016 lr: 0.02\n",
      "iteration: 29850 loss: 0.0017 lr: 0.02\n",
      "iteration: 29900 loss: 0.0016 lr: 0.02\n",
      "iteration: 29950 loss: 0.0016 lr: 0.02\n",
      "iteration: 30000 loss: 0.0015 lr: 0.02\n",
      "iteration: 30050 loss: 0.0015 lr: 0.02\n",
      "iteration: 30100 loss: 0.0017 lr: 0.02\n",
      "iteration: 30150 loss: 0.0018 lr: 0.02\n",
      "iteration: 30200 loss: 0.0016 lr: 0.02\n",
      "iteration: 30250 loss: 0.0017 lr: 0.02\n",
      "iteration: 30300 loss: 0.0013 lr: 0.02\n",
      "iteration: 30350 loss: 0.0012 lr: 0.02\n",
      "iteration: 30400 loss: 0.0017 lr: 0.02\n",
      "iteration: 30450 loss: 0.0014 lr: 0.02\n",
      "iteration: 30500 loss: 0.0018 lr: 0.02\n",
      "iteration: 30550 loss: 0.0014 lr: 0.02\n",
      "iteration: 30600 loss: 0.0013 lr: 0.02\n",
      "iteration: 30650 loss: 0.0014 lr: 0.02\n",
      "iteration: 30700 loss: 0.0017 lr: 0.02\n",
      "iteration: 30750 loss: 0.0017 lr: 0.02\n",
      "iteration: 30800 loss: 0.0014 lr: 0.02\n",
      "iteration: 30850 loss: 0.0015 lr: 0.02\n",
      "iteration: 30900 loss: 0.0015 lr: 0.02\n",
      "iteration: 30950 loss: 0.0017 lr: 0.02\n",
      "iteration: 31000 loss: 0.0016 lr: 0.02\n",
      "iteration: 31050 loss: 0.0015 lr: 0.02\n",
      "iteration: 31100 loss: 0.0015 lr: 0.02\n",
      "iteration: 31150 loss: 0.0017 lr: 0.02\n",
      "iteration: 31200 loss: 0.0019 lr: 0.02\n",
      "iteration: 31250 loss: 0.0015 lr: 0.02\n",
      "iteration: 31300 loss: 0.0017 lr: 0.02\n",
      "iteration: 31350 loss: 0.0013 lr: 0.02\n",
      "iteration: 31400 loss: 0.0014 lr: 0.02\n",
      "iteration: 31450 loss: 0.0016 lr: 0.02\n",
      "iteration: 31500 loss: 0.0015 lr: 0.02\n",
      "iteration: 31550 loss: 0.0015 lr: 0.02\n",
      "iteration: 31600 loss: 0.0015 lr: 0.02\n",
      "iteration: 31650 loss: 0.0014 lr: 0.02\n",
      "iteration: 31700 loss: 0.0014 lr: 0.02\n",
      "iteration: 31750 loss: 0.0015 lr: 0.02\n",
      "iteration: 31800 loss: 0.0015 lr: 0.02\n",
      "iteration: 31850 loss: 0.0017 lr: 0.02\n",
      "iteration: 31900 loss: 0.0015 lr: 0.02\n",
      "iteration: 31950 loss: 0.0015 lr: 0.02\n",
      "iteration: 32000 loss: 0.0018 lr: 0.02\n",
      "iteration: 32050 loss: 0.0014 lr: 0.02\n",
      "iteration: 32100 loss: 0.0016 lr: 0.02\n",
      "iteration: 32150 loss: 0.0014 lr: 0.02\n",
      "iteration: 32200 loss: 0.0018 lr: 0.02\n",
      "iteration: 32250 loss: 0.0012 lr: 0.02\n",
      "iteration: 32300 loss: 0.0014 lr: 0.02\n",
      "iteration: 32350 loss: 0.0013 lr: 0.02\n",
      "iteration: 32400 loss: 0.0016 lr: 0.02\n",
      "iteration: 32450 loss: 0.0014 lr: 0.02\n",
      "iteration: 32500 loss: 0.0016 lr: 0.02\n",
      "iteration: 32550 loss: 0.0014 lr: 0.02\n",
      "iteration: 32600 loss: 0.0014 lr: 0.02\n",
      "iteration: 32650 loss: 0.0016 lr: 0.02\n",
      "iteration: 32700 loss: 0.0015 lr: 0.02\n",
      "iteration: 32750 loss: 0.0015 lr: 0.02\n",
      "iteration: 32800 loss: 0.0013 lr: 0.02\n",
      "iteration: 32850 loss: 0.0015 lr: 0.02\n",
      "iteration: 32900 loss: 0.0014 lr: 0.02\n",
      "iteration: 32950 loss: 0.0016 lr: 0.02\n",
      "iteration: 33000 loss: 0.0014 lr: 0.02\n",
      "iteration: 33050 loss: 0.0012 lr: 0.02\n",
      "iteration: 33100 loss: 0.0016 lr: 0.02\n",
      "iteration: 33150 loss: 0.0014 lr: 0.02\n",
      "iteration: 33200 loss: 0.0017 lr: 0.02\n",
      "iteration: 33250 loss: 0.0016 lr: 0.02\n",
      "iteration: 33300 loss: 0.0015 lr: 0.02\n",
      "iteration: 33350 loss: 0.0015 lr: 0.02\n",
      "iteration: 33400 loss: 0.0017 lr: 0.02\n",
      "iteration: 33450 loss: 0.0014 lr: 0.02\n",
      "iteration: 33500 loss: 0.0015 lr: 0.02\n",
      "iteration: 33550 loss: 0.0015 lr: 0.02\n",
      "iteration: 33600 loss: 0.0013 lr: 0.02\n",
      "iteration: 33650 loss: 0.0014 lr: 0.02\n",
      "iteration: 33700 loss: 0.0014 lr: 0.02\n",
      "iteration: 33750 loss: 0.0016 lr: 0.02\n",
      "iteration: 33800 loss: 0.0016 lr: 0.02\n",
      "iteration: 33850 loss: 0.0013 lr: 0.02\n",
      "iteration: 33900 loss: 0.0014 lr: 0.02\n",
      "iteration: 33950 loss: 0.0018 lr: 0.02\n",
      "iteration: 34000 loss: 0.0018 lr: 0.02\n",
      "iteration: 34050 loss: 0.0015 lr: 0.02\n",
      "iteration: 34100 loss: 0.0017 lr: 0.02\n",
      "iteration: 34150 loss: 0.0015 lr: 0.02\n",
      "iteration: 34200 loss: 0.0016 lr: 0.02\n",
      "iteration: 34250 loss: 0.0015 lr: 0.02\n",
      "iteration: 34300 loss: 0.0015 lr: 0.02\n",
      "iteration: 34350 loss: 0.0014 lr: 0.02\n",
      "iteration: 34400 loss: 0.0011 lr: 0.02\n",
      "iteration: 34450 loss: 0.0013 lr: 0.02\n",
      "iteration: 34500 loss: 0.0014 lr: 0.02\n",
      "iteration: 34550 loss: 0.0014 lr: 0.02\n",
      "iteration: 34600 loss: 0.0018 lr: 0.02\n",
      "iteration: 34650 loss: 0.0016 lr: 0.02\n",
      "iteration: 34700 loss: 0.0014 lr: 0.02\n",
      "iteration: 34750 loss: 0.0015 lr: 0.02\n",
      "iteration: 34800 loss: 0.0015 lr: 0.02\n",
      "iteration: 34850 loss: 0.0014 lr: 0.02\n",
      "iteration: 34900 loss: 0.0017 lr: 0.02\n",
      "iteration: 34950 loss: 0.0014 lr: 0.02\n",
      "iteration: 35000 loss: 0.0015 lr: 0.02\n",
      "iteration: 35050 loss: 0.0015 lr: 0.02\n",
      "iteration: 35100 loss: 0.0014 lr: 0.02\n",
      "iteration: 35150 loss: 0.0014 lr: 0.02\n",
      "iteration: 35200 loss: 0.0016 lr: 0.02\n",
      "iteration: 35250 loss: 0.0015 lr: 0.02\n",
      "iteration: 35300 loss: 0.0016 lr: 0.02\n",
      "iteration: 35350 loss: 0.0015 lr: 0.02\n",
      "iteration: 35400 loss: 0.0013 lr: 0.02\n",
      "iteration: 35450 loss: 0.0012 lr: 0.02\n",
      "iteration: 35500 loss: 0.0015 lr: 0.02\n",
      "iteration: 35550 loss: 0.0014 lr: 0.02\n",
      "iteration: 35600 loss: 0.0017 lr: 0.02\n",
      "iteration: 35650 loss: 0.0014 lr: 0.02\n",
      "iteration: 35700 loss: 0.0014 lr: 0.02\n",
      "iteration: 35750 loss: 0.0012 lr: 0.02\n",
      "iteration: 35800 loss: 0.0013 lr: 0.02\n",
      "iteration: 35850 loss: 0.0018 lr: 0.02\n",
      "iteration: 35900 loss: 0.0015 lr: 0.02\n",
      "iteration: 35950 loss: 0.0013 lr: 0.02\n",
      "iteration: 36000 loss: 0.0014 lr: 0.02\n",
      "iteration: 36050 loss: 0.0013 lr: 0.02\n",
      "iteration: 36100 loss: 0.0016 lr: 0.02\n",
      "iteration: 36150 loss: 0.0015 lr: 0.02\n",
      "iteration: 36200 loss: 0.0016 lr: 0.02\n",
      "iteration: 36250 loss: 0.0014 lr: 0.02\n",
      "iteration: 36300 loss: 0.0014 lr: 0.02\n",
      "iteration: 36350 loss: 0.0012 lr: 0.02\n",
      "iteration: 36400 loss: 0.0015 lr: 0.02\n",
      "iteration: 36450 loss: 0.0013 lr: 0.02\n",
      "iteration: 36500 loss: 0.0015 lr: 0.02\n",
      "iteration: 36550 loss: 0.0015 lr: 0.02\n",
      "iteration: 36600 loss: 0.0015 lr: 0.02\n",
      "iteration: 36650 loss: 0.0015 lr: 0.02\n",
      "iteration: 36700 loss: 0.0014 lr: 0.02\n",
      "iteration: 36750 loss: 0.0013 lr: 0.02\n",
      "iteration: 36800 loss: 0.0012 lr: 0.02\n",
      "iteration: 36850 loss: 0.0013 lr: 0.02\n",
      "iteration: 36900 loss: 0.0013 lr: 0.02\n",
      "iteration: 36950 loss: 0.0016 lr: 0.02\n",
      "iteration: 37000 loss: 0.0013 lr: 0.02\n",
      "iteration: 37050 loss: 0.0014 lr: 0.02\n",
      "iteration: 37100 loss: 0.0017 lr: 0.02\n",
      "iteration: 37150 loss: 0.0013 lr: 0.02\n",
      "iteration: 37200 loss: 0.0015 lr: 0.02\n",
      "iteration: 37250 loss: 0.0016 lr: 0.02\n",
      "iteration: 37300 loss: 0.0015 lr: 0.02\n",
      "iteration: 37350 loss: 0.0015 lr: 0.02\n",
      "iteration: 37400 loss: 0.0016 lr: 0.02\n",
      "iteration: 37450 loss: 0.0017 lr: 0.02\n",
      "iteration: 37500 loss: 0.0014 lr: 0.02\n",
      "iteration: 37550 loss: 0.0014 lr: 0.02\n",
      "iteration: 37600 loss: 0.0015 lr: 0.02\n",
      "iteration: 37650 loss: 0.0013 lr: 0.02\n",
      "iteration: 37700 loss: 0.0015 lr: 0.02\n",
      "iteration: 37750 loss: 0.0014 lr: 0.02\n",
      "iteration: 37800 loss: 0.0012 lr: 0.02\n",
      "iteration: 37850 loss: 0.0016 lr: 0.02\n",
      "iteration: 37900 loss: 0.0016 lr: 0.02\n",
      "iteration: 37950 loss: 0.0015 lr: 0.02\n",
      "iteration: 38000 loss: 0.0013 lr: 0.02\n",
      "iteration: 38050 loss: 0.0016 lr: 0.02\n",
      "iteration: 38100 loss: 0.0014 lr: 0.02\n",
      "iteration: 38150 loss: 0.0014 lr: 0.02\n",
      "iteration: 38200 loss: 0.0016 lr: 0.02\n",
      "iteration: 38250 loss: 0.0014 lr: 0.02\n",
      "iteration: 38300 loss: 0.0013 lr: 0.02\n",
      "iteration: 38350 loss: 0.0012 lr: 0.02\n",
      "iteration: 38400 loss: 0.0015 lr: 0.02\n",
      "iteration: 38450 loss: 0.0015 lr: 0.02\n",
      "iteration: 38500 loss: 0.0015 lr: 0.02\n",
      "iteration: 38550 loss: 0.0014 lr: 0.02\n",
      "iteration: 38600 loss: 0.0013 lr: 0.02\n",
      "iteration: 38650 loss: 0.0013 lr: 0.02\n",
      "iteration: 38700 loss: 0.0015 lr: 0.02\n",
      "iteration: 38750 loss: 0.0012 lr: 0.02\n",
      "iteration: 38800 loss: 0.0014 lr: 0.02\n",
      "iteration: 38850 loss: 0.0016 lr: 0.02\n",
      "iteration: 38900 loss: 0.0012 lr: 0.02\n",
      "iteration: 38950 loss: 0.0015 lr: 0.02\n",
      "iteration: 39000 loss: 0.0015 lr: 0.02\n",
      "iteration: 39050 loss: 0.0015 lr: 0.02\n",
      "iteration: 39100 loss: 0.0012 lr: 0.02\n",
      "iteration: 39150 loss: 0.0015 lr: 0.02\n",
      "iteration: 39200 loss: 0.0014 lr: 0.02\n",
      "iteration: 39250 loss: 0.0016 lr: 0.02\n",
      "iteration: 39300 loss: 0.0015 lr: 0.02\n",
      "iteration: 39350 loss: 0.0012 lr: 0.02\n",
      "iteration: 39400 loss: 0.0020 lr: 0.02\n",
      "iteration: 39450 loss: 0.0016 lr: 0.02\n",
      "iteration: 39500 loss: 0.0013 lr: 0.02\n",
      "iteration: 39550 loss: 0.0013 lr: 0.02\n",
      "iteration: 39600 loss: 0.0015 lr: 0.02\n",
      "iteration: 39650 loss: 0.0012 lr: 0.02\n",
      "iteration: 39700 loss: 0.0017 lr: 0.02\n",
      "iteration: 39750 loss: 0.0013 lr: 0.02\n",
      "iteration: 39800 loss: 0.0012 lr: 0.02\n",
      "iteration: 39850 loss: 0.0012 lr: 0.02\n",
      "iteration: 39900 loss: 0.0014 lr: 0.02\n",
      "iteration: 39950 loss: 0.0013 lr: 0.02\n",
      "iteration: 40000 loss: 0.0014 lr: 0.02\n",
      "iteration: 40050 loss: 0.0011 lr: 0.02\n",
      "iteration: 40100 loss: 0.0013 lr: 0.02\n",
      "iteration: 40150 loss: 0.0016 lr: 0.02\n",
      "iteration: 40200 loss: 0.0016 lr: 0.02\n",
      "iteration: 40250 loss: 0.0014 lr: 0.02\n",
      "iteration: 40300 loss: 0.0014 lr: 0.02\n",
      "iteration: 40350 loss: 0.0012 lr: 0.02\n",
      "iteration: 40400 loss: 0.0013 lr: 0.02\n",
      "iteration: 40450 loss: 0.0014 lr: 0.02\n",
      "iteration: 40500 loss: 0.0013 lr: 0.02\n",
      "iteration: 40550 loss: 0.0012 lr: 0.02\n",
      "iteration: 40600 loss: 0.0013 lr: 0.02\n",
      "iteration: 40650 loss: 0.0014 lr: 0.02\n",
      "iteration: 40700 loss: 0.0015 lr: 0.02\n",
      "iteration: 40750 loss: 0.0015 lr: 0.02\n",
      "iteration: 40800 loss: 0.0014 lr: 0.02\n",
      "iteration: 40850 loss: 0.0014 lr: 0.02\n",
      "iteration: 40900 loss: 0.0013 lr: 0.02\n",
      "iteration: 40950 loss: 0.0013 lr: 0.02\n",
      "iteration: 41000 loss: 0.0013 lr: 0.02\n",
      "iteration: 41050 loss: 0.0011 lr: 0.02\n",
      "iteration: 41100 loss: 0.0016 lr: 0.02\n",
      "iteration: 41150 loss: 0.0015 lr: 0.02\n",
      "iteration: 41200 loss: 0.0013 lr: 0.02\n",
      "iteration: 41250 loss: 0.0013 lr: 0.02\n",
      "iteration: 41300 loss: 0.0014 lr: 0.02\n",
      "iteration: 41350 loss: 0.0015 lr: 0.02\n",
      "iteration: 41400 loss: 0.0014 lr: 0.02\n",
      "iteration: 41450 loss: 0.0012 lr: 0.02\n",
      "iteration: 41500 loss: 0.0013 lr: 0.02\n",
      "iteration: 41550 loss: 0.0015 lr: 0.02\n",
      "iteration: 41600 loss: 0.0014 lr: 0.02\n",
      "iteration: 41650 loss: 0.0017 lr: 0.02\n",
      "iteration: 41700 loss: 0.0015 lr: 0.02\n",
      "iteration: 41750 loss: 0.0015 lr: 0.02\n",
      "iteration: 41800 loss: 0.0014 lr: 0.02\n",
      "iteration: 41850 loss: 0.0011 lr: 0.02\n",
      "iteration: 41900 loss: 0.0014 lr: 0.02\n",
      "iteration: 41950 loss: 0.0011 lr: 0.02\n",
      "iteration: 42000 loss: 0.0013 lr: 0.02\n",
      "iteration: 42050 loss: 0.0013 lr: 0.02\n",
      "iteration: 42100 loss: 0.0015 lr: 0.02\n",
      "iteration: 42150 loss: 0.0014 lr: 0.02\n",
      "iteration: 42200 loss: 0.0014 lr: 0.02\n",
      "iteration: 42250 loss: 0.0014 lr: 0.02\n",
      "iteration: 42300 loss: 0.0014 lr: 0.02\n",
      "iteration: 42350 loss: 0.0012 lr: 0.02\n",
      "iteration: 42400 loss: 0.0013 lr: 0.02\n",
      "iteration: 42450 loss: 0.0013 lr: 0.02\n",
      "iteration: 42500 loss: 0.0013 lr: 0.02\n",
      "iteration: 42550 loss: 0.0015 lr: 0.02\n",
      "iteration: 42600 loss: 0.0017 lr: 0.02\n",
      "iteration: 42650 loss: 0.0013 lr: 0.02\n",
      "iteration: 42700 loss: 0.0014 lr: 0.02\n",
      "iteration: 42750 loss: 0.0012 lr: 0.02\n",
      "iteration: 42800 loss: 0.0014 lr: 0.02\n",
      "iteration: 42850 loss: 0.0015 lr: 0.02\n",
      "iteration: 42900 loss: 0.0012 lr: 0.02\n",
      "iteration: 42950 loss: 0.0013 lr: 0.02\n",
      "iteration: 43000 loss: 0.0016 lr: 0.02\n",
      "iteration: 43050 loss: 0.0014 lr: 0.02\n",
      "iteration: 43100 loss: 0.0014 lr: 0.02\n",
      "iteration: 43150 loss: 0.0012 lr: 0.02\n",
      "iteration: 43200 loss: 0.0015 lr: 0.02\n",
      "iteration: 43250 loss: 0.0012 lr: 0.02\n",
      "iteration: 43300 loss: 0.0012 lr: 0.02\n",
      "iteration: 43350 loss: 0.0014 lr: 0.02\n",
      "iteration: 43400 loss: 0.0016 lr: 0.02\n",
      "iteration: 43450 loss: 0.0012 lr: 0.02\n",
      "iteration: 43500 loss: 0.0014 lr: 0.02\n",
      "iteration: 43550 loss: 0.0013 lr: 0.02\n",
      "iteration: 43600 loss: 0.0014 lr: 0.02\n",
      "iteration: 43650 loss: 0.0014 lr: 0.02\n",
      "iteration: 43700 loss: 0.0013 lr: 0.02\n",
      "iteration: 43750 loss: 0.0017 lr: 0.02\n",
      "iteration: 43800 loss: 0.0014 lr: 0.02\n",
      "iteration: 43850 loss: 0.0014 lr: 0.02\n",
      "iteration: 43900 loss: 0.0013 lr: 0.02\n",
      "iteration: 43950 loss: 0.0012 lr: 0.02\n",
      "iteration: 44000 loss: 0.0012 lr: 0.02\n",
      "iteration: 44050 loss: 0.0015 lr: 0.02\n",
      "iteration: 44100 loss: 0.0013 lr: 0.02\n",
      "iteration: 44150 loss: 0.0017 lr: 0.02\n",
      "iteration: 44200 loss: 0.0011 lr: 0.02\n",
      "iteration: 44250 loss: 0.0012 lr: 0.02\n",
      "iteration: 44300 loss: 0.0013 lr: 0.02\n",
      "iteration: 44350 loss: 0.0013 lr: 0.02\n",
      "iteration: 44400 loss: 0.0015 lr: 0.02\n",
      "iteration: 44450 loss: 0.0011 lr: 0.02\n",
      "iteration: 44500 loss: 0.0013 lr: 0.02\n",
      "iteration: 44550 loss: 0.0012 lr: 0.02\n",
      "iteration: 44600 loss: 0.0014 lr: 0.02\n",
      "iteration: 44650 loss: 0.0012 lr: 0.02\n",
      "iteration: 44700 loss: 0.0013 lr: 0.02\n",
      "iteration: 44750 loss: 0.0014 lr: 0.02\n",
      "iteration: 44800 loss: 0.0013 lr: 0.02\n",
      "iteration: 44850 loss: 0.0011 lr: 0.02\n",
      "iteration: 44900 loss: 0.0013 lr: 0.02\n",
      "iteration: 44950 loss: 0.0013 lr: 0.02\n",
      "iteration: 45000 loss: 0.0012 lr: 0.02\n",
      "iteration: 45050 loss: 0.0013 lr: 0.02\n",
      "iteration: 45100 loss: 0.0014 lr: 0.02\n",
      "iteration: 45150 loss: 0.0013 lr: 0.02\n",
      "iteration: 45200 loss: 0.0013 lr: 0.02\n",
      "iteration: 45250 loss: 0.0012 lr: 0.02\n",
      "iteration: 45300 loss: 0.0014 lr: 0.02\n",
      "iteration: 45350 loss: 0.0011 lr: 0.02\n",
      "iteration: 45400 loss: 0.0012 lr: 0.02\n",
      "iteration: 45450 loss: 0.0013 lr: 0.02\n",
      "iteration: 45500 loss: 0.0012 lr: 0.02\n",
      "iteration: 45550 loss: 0.0013 lr: 0.02\n",
      "iteration: 45600 loss: 0.0013 lr: 0.02\n",
      "iteration: 45650 loss: 0.0014 lr: 0.02\n",
      "iteration: 45700 loss: 0.0014 lr: 0.02\n",
      "iteration: 45750 loss: 0.0012 lr: 0.02\n",
      "iteration: 45800 loss: 0.0013 lr: 0.02\n",
      "iteration: 45850 loss: 0.0013 lr: 0.02\n",
      "iteration: 45900 loss: 0.0015 lr: 0.02\n",
      "iteration: 45950 loss: 0.0012 lr: 0.02\n",
      "iteration: 46000 loss: 0.0012 lr: 0.02\n",
      "iteration: 46050 loss: 0.0013 lr: 0.02\n",
      "iteration: 46100 loss: 0.0012 lr: 0.02\n",
      "iteration: 46150 loss: 0.0013 lr: 0.02\n",
      "iteration: 46200 loss: 0.0013 lr: 0.02\n",
      "iteration: 46250 loss: 0.0013 lr: 0.02\n",
      "iteration: 46300 loss: 0.0015 lr: 0.02\n",
      "iteration: 46350 loss: 0.0015 lr: 0.02\n",
      "iteration: 46400 loss: 0.0015 lr: 0.02\n",
      "iteration: 46450 loss: 0.0012 lr: 0.02\n",
      "iteration: 46500 loss: 0.0013 lr: 0.02\n",
      "iteration: 46550 loss: 0.0012 lr: 0.02\n",
      "iteration: 46600 loss: 0.0015 lr: 0.02\n",
      "iteration: 46650 loss: 0.0014 lr: 0.02\n",
      "iteration: 46700 loss: 0.0013 lr: 0.02\n",
      "iteration: 46750 loss: 0.0014 lr: 0.02\n",
      "iteration: 46800 loss: 0.0014 lr: 0.02\n",
      "iteration: 46850 loss: 0.0015 lr: 0.02\n",
      "iteration: 46900 loss: 0.0013 lr: 0.02\n",
      "iteration: 46950 loss: 0.0013 lr: 0.02\n",
      "iteration: 47000 loss: 0.0014 lr: 0.02\n",
      "iteration: 47050 loss: 0.0013 lr: 0.02\n",
      "iteration: 47100 loss: 0.0012 lr: 0.02\n",
      "iteration: 47150 loss: 0.0013 lr: 0.02\n",
      "iteration: 47200 loss: 0.0013 lr: 0.02\n",
      "iteration: 47250 loss: 0.0014 lr: 0.02\n",
      "iteration: 47300 loss: 0.0012 lr: 0.02\n",
      "iteration: 47350 loss: 0.0012 lr: 0.02\n",
      "iteration: 47400 loss: 0.0012 lr: 0.02\n",
      "iteration: 47450 loss: 0.0014 lr: 0.02\n",
      "iteration: 47500 loss: 0.0012 lr: 0.02\n",
      "iteration: 47550 loss: 0.0013 lr: 0.02\n",
      "iteration: 47600 loss: 0.0011 lr: 0.02\n",
      "iteration: 47650 loss: 0.0011 lr: 0.02\n",
      "iteration: 47700 loss: 0.0016 lr: 0.02\n",
      "iteration: 47750 loss: 0.0013 lr: 0.02\n",
      "iteration: 47800 loss: 0.0013 lr: 0.02\n",
      "iteration: 47850 loss: 0.0012 lr: 0.02\n",
      "iteration: 47900 loss: 0.0012 lr: 0.02\n",
      "iteration: 47950 loss: 0.0012 lr: 0.02\n",
      "iteration: 48000 loss: 0.0011 lr: 0.02\n",
      "iteration: 48050 loss: 0.0011 lr: 0.02\n",
      "iteration: 48100 loss: 0.0011 lr: 0.02\n",
      "iteration: 48150 loss: 0.0012 lr: 0.02\n",
      "iteration: 48200 loss: 0.0015 lr: 0.02\n",
      "iteration: 48250 loss: 0.0012 lr: 0.02\n",
      "iteration: 48300 loss: 0.0013 lr: 0.02\n",
      "iteration: 48350 loss: 0.0012 lr: 0.02\n",
      "iteration: 48400 loss: 0.0013 lr: 0.02\n",
      "iteration: 48450 loss: 0.0012 lr: 0.02\n",
      "iteration: 48500 loss: 0.0017 lr: 0.02\n",
      "iteration: 48550 loss: 0.0013 lr: 0.02\n",
      "iteration: 48600 loss: 0.0013 lr: 0.02\n",
      "iteration: 48650 loss: 0.0011 lr: 0.02\n",
      "iteration: 48700 loss: 0.0014 lr: 0.02\n",
      "iteration: 48750 loss: 0.0013 lr: 0.02\n",
      "iteration: 48800 loss: 0.0013 lr: 0.02\n",
      "iteration: 48850 loss: 0.0012 lr: 0.02\n",
      "iteration: 48900 loss: 0.0015 lr: 0.02\n",
      "iteration: 48950 loss: 0.0013 lr: 0.02\n",
      "iteration: 49000 loss: 0.0014 lr: 0.02\n",
      "iteration: 49050 loss: 0.0013 lr: 0.02\n",
      "iteration: 49100 loss: 0.0018 lr: 0.02\n",
      "iteration: 49150 loss: 0.0014 lr: 0.02\n",
      "iteration: 49200 loss: 0.0013 lr: 0.02\n",
      "iteration: 49250 loss: 0.0014 lr: 0.02\n",
      "iteration: 49300 loss: 0.0013 lr: 0.02\n",
      "iteration: 49350 loss: 0.0014 lr: 0.02\n",
      "iteration: 49400 loss: 0.0013 lr: 0.02\n",
      "iteration: 49450 loss: 0.0013 lr: 0.02\n",
      "iteration: 49500 loss: 0.0014 lr: 0.02\n",
      "iteration: 49550 loss: 0.0014 lr: 0.02\n",
      "iteration: 49600 loss: 0.0014 lr: 0.02\n",
      "iteration: 49650 loss: 0.0012 lr: 0.02\n",
      "iteration: 49700 loss: 0.0012 lr: 0.02\n",
      "iteration: 49750 loss: 0.0014 lr: 0.02\n",
      "iteration: 49800 loss: 0.0013 lr: 0.02\n",
      "iteration: 49850 loss: 0.0012 lr: 0.02\n",
      "iteration: 49900 loss: 0.0014 lr: 0.02\n",
      "iteration: 49950 loss: 0.0013 lr: 0.02\n",
      "iteration: 50000 loss: 0.0013 lr: 0.02\n",
      "iteration: 50050 loss: 0.0015 lr: 0.02\n",
      "iteration: 50100 loss: 0.0013 lr: 0.02\n",
      "iteration: 50150 loss: 0.0013 lr: 0.02\n",
      "iteration: 50200 loss: 0.0011 lr: 0.02\n",
      "iteration: 50250 loss: 0.0011 lr: 0.02\n",
      "iteration: 50300 loss: 0.0012 lr: 0.02\n",
      "iteration: 50350 loss: 0.0013 lr: 0.02\n",
      "iteration: 50400 loss: 0.0013 lr: 0.02\n",
      "iteration: 50450 loss: 0.0012 lr: 0.02\n",
      "iteration: 50500 loss: 0.0012 lr: 0.02\n",
      "iteration: 50550 loss: 0.0012 lr: 0.02\n",
      "iteration: 50600 loss: 0.0014 lr: 0.02\n",
      "iteration: 50650 loss: 0.0014 lr: 0.02\n",
      "iteration: 50700 loss: 0.0015 lr: 0.02\n",
      "iteration: 50750 loss: 0.0013 lr: 0.02\n",
      "iteration: 50800 loss: 0.0015 lr: 0.02\n",
      "iteration: 50850 loss: 0.0014 lr: 0.02\n",
      "iteration: 50900 loss: 0.0016 lr: 0.02\n",
      "iteration: 50950 loss: 0.0013 lr: 0.02\n",
      "iteration: 51000 loss: 0.0013 lr: 0.02\n",
      "iteration: 51050 loss: 0.0015 lr: 0.02\n",
      "iteration: 51100 loss: 0.0015 lr: 0.02\n",
      "iteration: 51150 loss: 0.0013 lr: 0.02\n",
      "iteration: 51200 loss: 0.0012 lr: 0.02\n",
      "iteration: 51250 loss: 0.0014 lr: 0.02\n",
      "iteration: 51300 loss: 0.0016 lr: 0.02\n",
      "iteration: 51350 loss: 0.0014 lr: 0.02\n",
      "iteration: 51400 loss: 0.0013 lr: 0.02\n",
      "iteration: 51450 loss: 0.0013 lr: 0.02\n",
      "iteration: 51500 loss: 0.0016 lr: 0.02\n",
      "iteration: 51550 loss: 0.0014 lr: 0.02\n",
      "iteration: 51600 loss: 0.0012 lr: 0.02\n",
      "iteration: 51650 loss: 0.0013 lr: 0.02\n",
      "iteration: 51700 loss: 0.0015 lr: 0.02\n",
      "iteration: 51750 loss: 0.0015 lr: 0.02\n",
      "iteration: 51800 loss: 0.0011 lr: 0.02\n",
      "iteration: 51850 loss: 0.0013 lr: 0.02\n",
      "iteration: 51900 loss: 0.0012 lr: 0.02\n",
      "iteration: 51950 loss: 0.0013 lr: 0.02\n",
      "iteration: 52000 loss: 0.0013 lr: 0.02\n",
      "iteration: 52050 loss: 0.0015 lr: 0.02\n",
      "iteration: 52100 loss: 0.0012 lr: 0.02\n",
      "iteration: 52150 loss: 0.0013 lr: 0.02\n",
      "iteration: 52200 loss: 0.0012 lr: 0.02\n",
      "iteration: 52250 loss: 0.0012 lr: 0.02\n",
      "iteration: 52300 loss: 0.0011 lr: 0.02\n",
      "iteration: 52350 loss: 0.0012 lr: 0.02\n",
      "iteration: 52400 loss: 0.0012 lr: 0.02\n",
      "iteration: 52450 loss: 0.0012 lr: 0.02\n",
      "iteration: 52500 loss: 0.0016 lr: 0.02\n",
      "iteration: 52550 loss: 0.0012 lr: 0.02\n",
      "iteration: 52600 loss: 0.0012 lr: 0.02\n",
      "iteration: 52650 loss: 0.0015 lr: 0.02\n",
      "iteration: 52700 loss: 0.0013 lr: 0.02\n",
      "iteration: 52750 loss: 0.0012 lr: 0.02\n",
      "iteration: 52800 loss: 0.0015 lr: 0.02\n",
      "iteration: 52850 loss: 0.0014 lr: 0.02\n",
      "iteration: 52900 loss: 0.0013 lr: 0.02\n",
      "iteration: 52950 loss: 0.0013 lr: 0.02\n",
      "iteration: 53000 loss: 0.0013 lr: 0.02\n",
      "iteration: 53050 loss: 0.0014 lr: 0.02\n",
      "iteration: 53100 loss: 0.0014 lr: 0.02\n",
      "iteration: 53150 loss: 0.0012 lr: 0.02\n",
      "iteration: 53200 loss: 0.0013 lr: 0.02\n",
      "iteration: 53250 loss: 0.0014 lr: 0.02\n",
      "iteration: 53300 loss: 0.0011 lr: 0.02\n",
      "iteration: 53350 loss: 0.0011 lr: 0.02\n",
      "iteration: 53400 loss: 0.0013 lr: 0.02\n",
      "iteration: 53450 loss: 0.0013 lr: 0.02\n",
      "iteration: 53500 loss: 0.0013 lr: 0.02\n",
      "iteration: 53550 loss: 0.0012 lr: 0.02\n",
      "iteration: 53600 loss: 0.0012 lr: 0.02\n",
      "iteration: 53650 loss: 0.0014 lr: 0.02\n",
      "iteration: 53700 loss: 0.0012 lr: 0.02\n",
      "iteration: 53750 loss: 0.0012 lr: 0.02\n",
      "iteration: 53800 loss: 0.0012 lr: 0.02\n",
      "iteration: 53850 loss: 0.0014 lr: 0.02\n",
      "iteration: 53900 loss: 0.0019 lr: 0.02\n",
      "iteration: 53950 loss: 0.0012 lr: 0.02\n",
      "iteration: 54000 loss: 0.0015 lr: 0.02\n",
      "iteration: 54050 loss: 0.0013 lr: 0.02\n",
      "iteration: 54100 loss: 0.0013 lr: 0.02\n",
      "iteration: 54150 loss: 0.0010 lr: 0.02\n",
      "iteration: 54200 loss: 0.0012 lr: 0.02\n",
      "iteration: 54250 loss: 0.0016 lr: 0.02\n",
      "iteration: 54300 loss: 0.0012 lr: 0.02\n",
      "iteration: 54350 loss: 0.0013 lr: 0.02\n",
      "iteration: 54400 loss: 0.0014 lr: 0.02\n",
      "iteration: 54450 loss: 0.0015 lr: 0.02\n",
      "iteration: 54500 loss: 0.0014 lr: 0.02\n",
      "iteration: 54550 loss: 0.0012 lr: 0.02\n",
      "iteration: 54600 loss: 0.0012 lr: 0.02\n",
      "iteration: 54650 loss: 0.0015 lr: 0.02\n",
      "iteration: 54700 loss: 0.0012 lr: 0.02\n",
      "iteration: 54750 loss: 0.0011 lr: 0.02\n",
      "iteration: 54800 loss: 0.0013 lr: 0.02\n",
      "iteration: 54850 loss: 0.0011 lr: 0.02\n",
      "iteration: 54900 loss: 0.0012 lr: 0.02\n",
      "iteration: 54950 loss: 0.0013 lr: 0.02\n",
      "iteration: 55000 loss: 0.0013 lr: 0.02\n",
      "iteration: 55050 loss: 0.0013 lr: 0.02\n",
      "iteration: 55100 loss: 0.0014 lr: 0.02\n",
      "iteration: 55150 loss: 0.0011 lr: 0.02\n",
      "iteration: 55200 loss: 0.0012 lr: 0.02\n",
      "iteration: 55250 loss: 0.0011 lr: 0.02\n",
      "iteration: 55300 loss: 0.0019 lr: 0.02\n",
      "iteration: 55350 loss: 0.0014 lr: 0.02\n",
      "iteration: 55400 loss: 0.0013 lr: 0.02\n",
      "iteration: 55450 loss: 0.0012 lr: 0.02\n",
      "iteration: 55500 loss: 0.0012 lr: 0.02\n",
      "iteration: 55550 loss: 0.0013 lr: 0.02\n",
      "iteration: 55600 loss: 0.0011 lr: 0.02\n",
      "iteration: 55650 loss: 0.0012 lr: 0.02\n",
      "iteration: 55700 loss: 0.0013 lr: 0.02\n",
      "iteration: 55750 loss: 0.0011 lr: 0.02\n",
      "iteration: 55800 loss: 0.0013 lr: 0.02\n",
      "iteration: 55850 loss: 0.0014 lr: 0.02\n",
      "iteration: 55900 loss: 0.0012 lr: 0.02\n",
      "iteration: 55950 loss: 0.0013 lr: 0.02\n",
      "iteration: 56000 loss: 0.0014 lr: 0.02\n",
      "iteration: 56050 loss: 0.0014 lr: 0.02\n",
      "iteration: 56100 loss: 0.0013 lr: 0.02\n",
      "iteration: 56150 loss: 0.0013 lr: 0.02\n",
      "iteration: 56200 loss: 0.0013 lr: 0.02\n",
      "iteration: 56250 loss: 0.0011 lr: 0.02\n",
      "iteration: 56300 loss: 0.0013 lr: 0.02\n",
      "iteration: 56350 loss: 0.0013 lr: 0.02\n",
      "iteration: 56400 loss: 0.0011 lr: 0.02\n",
      "iteration: 56450 loss: 0.0012 lr: 0.02\n",
      "iteration: 56500 loss: 0.0011 lr: 0.02\n",
      "iteration: 56550 loss: 0.0011 lr: 0.02\n",
      "iteration: 56600 loss: 0.0013 lr: 0.02\n",
      "iteration: 56650 loss: 0.0012 lr: 0.02\n",
      "iteration: 56700 loss: 0.0012 lr: 0.02\n",
      "iteration: 56750 loss: 0.0014 lr: 0.02\n",
      "iteration: 56800 loss: 0.0015 lr: 0.02\n",
      "iteration: 56850 loss: 0.0013 lr: 0.02\n",
      "iteration: 56900 loss: 0.0011 lr: 0.02\n",
      "iteration: 56950 loss: 0.0013 lr: 0.02\n",
      "iteration: 57000 loss: 0.0012 lr: 0.02\n",
      "iteration: 57050 loss: 0.0012 lr: 0.02\n",
      "iteration: 57100 loss: 0.0010 lr: 0.02\n",
      "iteration: 57150 loss: 0.0011 lr: 0.02\n",
      "iteration: 57200 loss: 0.0012 lr: 0.02\n",
      "iteration: 57250 loss: 0.0012 lr: 0.02\n",
      "iteration: 57300 loss: 0.0013 lr: 0.02\n",
      "iteration: 57350 loss: 0.0013 lr: 0.02\n",
      "iteration: 57400 loss: 0.0013 lr: 0.02\n",
      "iteration: 57450 loss: 0.0012 lr: 0.02\n",
      "iteration: 57500 loss: 0.0011 lr: 0.02\n",
      "iteration: 57550 loss: 0.0011 lr: 0.02\n",
      "iteration: 57600 loss: 0.0011 lr: 0.02\n",
      "iteration: 57650 loss: 0.0011 lr: 0.02\n",
      "iteration: 57700 loss: 0.0011 lr: 0.02\n",
      "iteration: 57750 loss: 0.0013 lr: 0.02\n",
      "iteration: 57800 loss: 0.0011 lr: 0.02\n",
      "iteration: 57850 loss: 0.0011 lr: 0.02\n",
      "iteration: 57900 loss: 0.0012 lr: 0.02\n",
      "iteration: 57950 loss: 0.0011 lr: 0.02\n",
      "iteration: 58000 loss: 0.0012 lr: 0.02\n",
      "iteration: 58050 loss: 0.0015 lr: 0.02\n",
      "iteration: 58100 loss: 0.0012 lr: 0.02\n",
      "iteration: 58150 loss: 0.0012 lr: 0.02\n",
      "iteration: 58200 loss: 0.0014 lr: 0.02\n",
      "iteration: 58250 loss: 0.0012 lr: 0.02\n",
      "iteration: 58300 loss: 0.0010 lr: 0.02\n",
      "iteration: 58350 loss: 0.0013 lr: 0.02\n",
      "iteration: 58400 loss: 0.0014 lr: 0.02\n",
      "iteration: 58450 loss: 0.0013 lr: 0.02\n",
      "iteration: 58500 loss: 0.0013 lr: 0.02\n",
      "iteration: 58550 loss: 0.0011 lr: 0.02\n",
      "iteration: 58600 loss: 0.0013 lr: 0.02\n",
      "iteration: 58650 loss: 0.0012 lr: 0.02\n",
      "iteration: 58700 loss: 0.0011 lr: 0.02\n",
      "iteration: 58750 loss: 0.0012 lr: 0.02\n",
      "iteration: 58800 loss: 0.0013 lr: 0.02\n",
      "iteration: 58850 loss: 0.0014 lr: 0.02\n",
      "iteration: 58900 loss: 0.0011 lr: 0.02\n",
      "iteration: 58950 loss: 0.0014 lr: 0.02\n",
      "iteration: 59000 loss: 0.0013 lr: 0.02\n",
      "iteration: 59050 loss: 0.0014 lr: 0.02\n",
      "iteration: 59100 loss: 0.0010 lr: 0.02\n",
      "iteration: 59150 loss: 0.0012 lr: 0.02\n",
      "iteration: 59200 loss: 0.0013 lr: 0.02\n",
      "iteration: 59250 loss: 0.0013 lr: 0.02\n",
      "iteration: 59300 loss: 0.0014 lr: 0.02\n",
      "iteration: 59350 loss: 0.0014 lr: 0.02\n",
      "iteration: 59400 loss: 0.0014 lr: 0.02\n",
      "iteration: 59450 loss: 0.0013 lr: 0.02\n",
      "iteration: 59500 loss: 0.0011 lr: 0.02\n",
      "iteration: 59550 loss: 0.0011 lr: 0.02\n",
      "iteration: 59600 loss: 0.0012 lr: 0.02\n",
      "iteration: 59650 loss: 0.0012 lr: 0.02\n",
      "iteration: 59700 loss: 0.0011 lr: 0.02\n",
      "iteration: 59750 loss: 0.0014 lr: 0.02\n",
      "iteration: 59800 loss: 0.0013 lr: 0.02\n",
      "iteration: 59850 loss: 0.0012 lr: 0.02\n",
      "iteration: 59900 loss: 0.0012 lr: 0.02\n",
      "iteration: 59950 loss: 0.0015 lr: 0.02\n",
      "iteration: 60000 loss: 0.0012 lr: 0.02\n",
      "iteration: 60050 loss: 0.0015 lr: 0.02\n",
      "iteration: 60100 loss: 0.0011 lr: 0.02\n",
      "iteration: 60150 loss: 0.0010 lr: 0.02\n",
      "iteration: 60200 loss: 0.0010 lr: 0.02\n",
      "iteration: 60250 loss: 0.0010 lr: 0.02\n",
      "iteration: 60300 loss: 0.0014 lr: 0.02\n",
      "iteration: 60350 loss: 0.0013 lr: 0.02\n",
      "iteration: 60400 loss: 0.0011 lr: 0.02\n",
      "iteration: 60450 loss: 0.0012 lr: 0.02\n",
      "iteration: 60500 loss: 0.0012 lr: 0.02\n",
      "iteration: 60550 loss: 0.0012 lr: 0.02\n",
      "iteration: 60600 loss: 0.0013 lr: 0.02\n",
      "iteration: 60650 loss: 0.0012 lr: 0.02\n",
      "iteration: 60700 loss: 0.0012 lr: 0.02\n",
      "iteration: 60750 loss: 0.0009 lr: 0.02\n",
      "iteration: 60800 loss: 0.0012 lr: 0.02\n",
      "iteration: 60850 loss: 0.0012 lr: 0.02\n",
      "iteration: 60900 loss: 0.0013 lr: 0.02\n",
      "iteration: 60950 loss: 0.0013 lr: 0.02\n",
      "iteration: 61000 loss: 0.0013 lr: 0.02\n",
      "iteration: 61050 loss: 0.0013 lr: 0.02\n",
      "iteration: 61100 loss: 0.0013 lr: 0.02\n",
      "iteration: 61150 loss: 0.0013 lr: 0.02\n",
      "iteration: 61200 loss: 0.0012 lr: 0.02\n",
      "iteration: 61250 loss: 0.0011 lr: 0.02\n",
      "iteration: 61300 loss: 0.0011 lr: 0.02\n",
      "iteration: 61350 loss: 0.0013 lr: 0.02\n",
      "iteration: 61400 loss: 0.0013 lr: 0.02\n",
      "iteration: 61450 loss: 0.0012 lr: 0.02\n",
      "iteration: 61500 loss: 0.0015 lr: 0.02\n",
      "iteration: 61550 loss: 0.0010 lr: 0.02\n",
      "iteration: 61600 loss: 0.0011 lr: 0.02\n",
      "iteration: 61650 loss: 0.0013 lr: 0.02\n",
      "iteration: 61700 loss: 0.0014 lr: 0.02\n",
      "iteration: 61750 loss: 0.0011 lr: 0.02\n",
      "iteration: 61800 loss: 0.0013 lr: 0.02\n",
      "iteration: 61850 loss: 0.0013 lr: 0.02\n",
      "iteration: 61900 loss: 0.0012 lr: 0.02\n",
      "iteration: 61950 loss: 0.0012 lr: 0.02\n",
      "iteration: 62000 loss: 0.0011 lr: 0.02\n",
      "iteration: 62050 loss: 0.0011 lr: 0.02\n",
      "iteration: 62100 loss: 0.0011 lr: 0.02\n",
      "iteration: 62150 loss: 0.0012 lr: 0.02\n",
      "iteration: 62200 loss: 0.0011 lr: 0.02\n",
      "iteration: 62250 loss: 0.0010 lr: 0.02\n",
      "iteration: 62300 loss: 0.0011 lr: 0.02\n",
      "iteration: 62350 loss: 0.0011 lr: 0.02\n",
      "iteration: 62400 loss: 0.0014 lr: 0.02\n",
      "iteration: 62450 loss: 0.0013 lr: 0.02\n",
      "iteration: 62500 loss: 0.0012 lr: 0.02\n",
      "iteration: 62550 loss: 0.0011 lr: 0.02\n",
      "iteration: 62600 loss: 0.0012 lr: 0.02\n",
      "iteration: 62650 loss: 0.0013 lr: 0.02\n",
      "iteration: 62700 loss: 0.0011 lr: 0.02\n",
      "iteration: 62750 loss: 0.0012 lr: 0.02\n",
      "iteration: 62800 loss: 0.0013 lr: 0.02\n",
      "iteration: 62850 loss: 0.0013 lr: 0.02\n",
      "iteration: 62900 loss: 0.0012 lr: 0.02\n",
      "iteration: 62950 loss: 0.0013 lr: 0.02\n",
      "iteration: 63000 loss: 0.0014 lr: 0.02\n",
      "iteration: 63050 loss: 0.0013 lr: 0.02\n",
      "iteration: 63100 loss: 0.0012 lr: 0.02\n",
      "iteration: 63150 loss: 0.0013 lr: 0.02\n",
      "iteration: 63200 loss: 0.0011 lr: 0.02\n",
      "iteration: 63250 loss: 0.0013 lr: 0.02\n",
      "iteration: 63300 loss: 0.0014 lr: 0.02\n",
      "iteration: 63350 loss: 0.0011 lr: 0.02\n",
      "iteration: 63400 loss: 0.0012 lr: 0.02\n",
      "iteration: 63450 loss: 0.0012 lr: 0.02\n",
      "iteration: 63500 loss: 0.0012 lr: 0.02\n",
      "iteration: 63550 loss: 0.0014 lr: 0.02\n",
      "iteration: 63600 loss: 0.0012 lr: 0.02\n",
      "iteration: 63650 loss: 0.0012 lr: 0.02\n",
      "iteration: 63700 loss: 0.0013 lr: 0.02\n",
      "iteration: 63750 loss: 0.0014 lr: 0.02\n",
      "iteration: 63800 loss: 0.0013 lr: 0.02\n",
      "iteration: 63850 loss: 0.0012 lr: 0.02\n",
      "iteration: 63900 loss: 0.0014 lr: 0.02\n",
      "iteration: 63950 loss: 0.0013 lr: 0.02\n",
      "iteration: 64000 loss: 0.0012 lr: 0.02\n",
      "iteration: 64050 loss: 0.0012 lr: 0.02\n",
      "iteration: 64100 loss: 0.0012 lr: 0.02\n",
      "iteration: 64150 loss: 0.0011 lr: 0.02\n",
      "iteration: 64200 loss: 0.0011 lr: 0.02\n",
      "iteration: 64250 loss: 0.0011 lr: 0.02\n",
      "iteration: 64300 loss: 0.0012 lr: 0.02\n",
      "iteration: 64350 loss: 0.0013 lr: 0.02\n",
      "iteration: 64400 loss: 0.0012 lr: 0.02\n",
      "iteration: 64450 loss: 0.0011 lr: 0.02\n",
      "iteration: 64500 loss: 0.0011 lr: 0.02\n",
      "iteration: 64550 loss: 0.0012 lr: 0.02\n",
      "iteration: 64600 loss: 0.0012 lr: 0.02\n",
      "iteration: 64650 loss: 0.0013 lr: 0.02\n",
      "iteration: 64700 loss: 0.0012 lr: 0.02\n",
      "iteration: 64750 loss: 0.0013 lr: 0.02\n",
      "iteration: 64800 loss: 0.0011 lr: 0.02\n",
      "iteration: 64850 loss: 0.0010 lr: 0.02\n",
      "iteration: 64900 loss: 0.0013 lr: 0.02\n",
      "iteration: 64950 loss: 0.0012 lr: 0.02\n",
      "iteration: 65000 loss: 0.0012 lr: 0.02\n",
      "iteration: 65050 loss: 0.0011 lr: 0.02\n",
      "iteration: 65100 loss: 0.0013 lr: 0.02\n",
      "iteration: 65150 loss: 0.0013 lr: 0.02\n",
      "iteration: 65200 loss: 0.0010 lr: 0.02\n",
      "iteration: 65250 loss: 0.0012 lr: 0.02\n",
      "iteration: 65300 loss: 0.0012 lr: 0.02\n",
      "iteration: 65350 loss: 0.0011 lr: 0.02\n",
      "iteration: 65400 loss: 0.0011 lr: 0.02\n",
      "iteration: 65450 loss: 0.0012 lr: 0.02\n",
      "iteration: 65500 loss: 0.0013 lr: 0.02\n",
      "iteration: 65550 loss: 0.0013 lr: 0.02\n",
      "iteration: 65600 loss: 0.0012 lr: 0.02\n",
      "iteration: 65650 loss: 0.0011 lr: 0.02\n",
      "iteration: 65700 loss: 0.0012 lr: 0.02\n",
      "iteration: 65750 loss: 0.0011 lr: 0.02\n",
      "iteration: 65800 loss: 0.0012 lr: 0.02\n",
      "iteration: 65850 loss: 0.0014 lr: 0.02\n",
      "iteration: 65900 loss: 0.0010 lr: 0.02\n",
      "iteration: 65950 loss: 0.0009 lr: 0.02\n",
      "iteration: 66000 loss: 0.0010 lr: 0.02\n",
      "iteration: 66050 loss: 0.0011 lr: 0.02\n",
      "iteration: 66100 loss: 0.0012 lr: 0.02\n",
      "iteration: 66150 loss: 0.0012 lr: 0.02\n",
      "iteration: 66200 loss: 0.0011 lr: 0.02\n",
      "iteration: 66250 loss: 0.0012 lr: 0.02\n",
      "iteration: 66300 loss: 0.0012 lr: 0.02\n",
      "iteration: 66350 loss: 0.0014 lr: 0.02\n",
      "iteration: 66400 loss: 0.0011 lr: 0.02\n",
      "iteration: 66450 loss: 0.0011 lr: 0.02\n",
      "iteration: 66500 loss: 0.0012 lr: 0.02\n",
      "iteration: 66550 loss: 0.0011 lr: 0.02\n",
      "iteration: 66600 loss: 0.0012 lr: 0.02\n",
      "iteration: 66650 loss: 0.0011 lr: 0.02\n",
      "iteration: 66700 loss: 0.0011 lr: 0.02\n",
      "iteration: 66750 loss: 0.0011 lr: 0.02\n",
      "iteration: 66800 loss: 0.0013 lr: 0.02\n",
      "iteration: 66850 loss: 0.0012 lr: 0.02\n",
      "iteration: 66900 loss: 0.0011 lr: 0.02\n",
      "iteration: 66950 loss: 0.0011 lr: 0.02\n",
      "iteration: 67000 loss: 0.0014 lr: 0.02\n",
      "iteration: 67050 loss: 0.0013 lr: 0.02\n",
      "iteration: 67100 loss: 0.0014 lr: 0.02\n",
      "iteration: 67150 loss: 0.0012 lr: 0.02\n",
      "iteration: 67200 loss: 0.0013 lr: 0.02\n",
      "iteration: 67250 loss: 0.0013 lr: 0.02\n",
      "iteration: 67300 loss: 0.0013 lr: 0.02\n",
      "iteration: 67350 loss: 0.0012 lr: 0.02\n",
      "iteration: 67400 loss: 0.0012 lr: 0.02\n",
      "iteration: 67450 loss: 0.0012 lr: 0.02\n",
      "iteration: 67500 loss: 0.0013 lr: 0.02\n",
      "iteration: 67550 loss: 0.0011 lr: 0.02\n",
      "iteration: 67600 loss: 0.0012 lr: 0.02\n",
      "iteration: 67650 loss: 0.0012 lr: 0.02\n",
      "iteration: 67700 loss: 0.0013 lr: 0.02\n",
      "iteration: 67750 loss: 0.0011 lr: 0.02\n",
      "iteration: 67800 loss: 0.0013 lr: 0.02\n",
      "iteration: 67850 loss: 0.0011 lr: 0.02\n",
      "iteration: 67900 loss: 0.0011 lr: 0.02\n",
      "iteration: 67950 loss: 0.0012 lr: 0.02\n",
      "iteration: 68000 loss: 0.0011 lr: 0.02\n",
      "iteration: 68050 loss: 0.0011 lr: 0.02\n",
      "iteration: 68100 loss: 0.0010 lr: 0.02\n",
      "iteration: 68150 loss: 0.0011 lr: 0.02\n",
      "iteration: 68200 loss: 0.0011 lr: 0.02\n",
      "iteration: 68250 loss: 0.0011 lr: 0.02\n",
      "iteration: 68300 loss: 0.0011 lr: 0.02\n",
      "iteration: 68350 loss: 0.0011 lr: 0.02\n",
      "iteration: 68400 loss: 0.0012 lr: 0.02\n",
      "iteration: 68450 loss: 0.0010 lr: 0.02\n",
      "iteration: 68500 loss: 0.0013 lr: 0.02\n",
      "iteration: 68550 loss: 0.0012 lr: 0.02\n",
      "iteration: 68600 loss: 0.0011 lr: 0.02\n",
      "iteration: 68650 loss: 0.0011 lr: 0.02\n",
      "iteration: 68700 loss: 0.0012 lr: 0.02\n",
      "iteration: 68750 loss: 0.0013 lr: 0.02\n",
      "iteration: 68800 loss: 0.0011 lr: 0.02\n",
      "iteration: 68850 loss: 0.0011 lr: 0.02\n",
      "iteration: 68900 loss: 0.0012 lr: 0.02\n",
      "iteration: 68950 loss: 0.0015 lr: 0.02\n",
      "iteration: 69000 loss: 0.0012 lr: 0.02\n",
      "iteration: 69050 loss: 0.0012 lr: 0.02\n",
      "iteration: 69100 loss: 0.0012 lr: 0.02\n",
      "iteration: 69150 loss: 0.0011 lr: 0.02\n",
      "iteration: 69200 loss: 0.0011 lr: 0.02\n",
      "iteration: 69250 loss: 0.0013 lr: 0.02\n",
      "iteration: 69300 loss: 0.0011 lr: 0.02\n",
      "iteration: 69350 loss: 0.0013 lr: 0.02\n",
      "iteration: 69400 loss: 0.0012 lr: 0.02\n",
      "iteration: 69450 loss: 0.0011 lr: 0.02\n",
      "iteration: 69500 loss: 0.0012 lr: 0.02\n",
      "iteration: 69550 loss: 0.0011 lr: 0.02\n",
      "iteration: 69600 loss: 0.0011 lr: 0.02\n",
      "iteration: 69650 loss: 0.0011 lr: 0.02\n",
      "iteration: 69700 loss: 0.0013 lr: 0.02\n",
      "iteration: 69750 loss: 0.0013 lr: 0.02\n",
      "iteration: 69800 loss: 0.0013 lr: 0.02\n",
      "iteration: 69850 loss: 0.0011 lr: 0.02\n",
      "iteration: 69900 loss: 0.0011 lr: 0.02\n",
      "iteration: 69950 loss: 0.0011 lr: 0.02\n",
      "iteration: 70000 loss: 0.0012 lr: 0.02\n",
      "iteration: 70050 loss: 0.0011 lr: 0.02\n",
      "iteration: 70100 loss: 0.0012 lr: 0.02\n",
      "iteration: 70150 loss: 0.0012 lr: 0.02\n",
      "iteration: 70200 loss: 0.0014 lr: 0.02\n",
      "iteration: 70250 loss: 0.0012 lr: 0.02\n",
      "iteration: 70300 loss: 0.0011 lr: 0.02\n",
      "iteration: 70350 loss: 0.0011 lr: 0.02\n",
      "iteration: 70400 loss: 0.0012 lr: 0.02\n",
      "iteration: 70450 loss: 0.0011 lr: 0.02\n",
      "iteration: 70500 loss: 0.0011 lr: 0.02\n",
      "iteration: 70550 loss: 0.0011 lr: 0.02\n",
      "iteration: 70600 loss: 0.0011 lr: 0.02\n",
      "iteration: 70650 loss: 0.0012 lr: 0.02\n",
      "iteration: 70700 loss: 0.0012 lr: 0.02\n",
      "iteration: 70750 loss: 0.0011 lr: 0.02\n",
      "iteration: 70800 loss: 0.0011 lr: 0.02\n",
      "iteration: 70850 loss: 0.0013 lr: 0.02\n",
      "iteration: 70900 loss: 0.0011 lr: 0.02\n",
      "iteration: 70950 loss: 0.0013 lr: 0.02\n",
      "iteration: 71000 loss: 0.0013 lr: 0.02\n",
      "iteration: 71050 loss: 0.0010 lr: 0.02\n",
      "iteration: 71100 loss: 0.0010 lr: 0.02\n",
      "iteration: 71150 loss: 0.0011 lr: 0.02\n",
      "iteration: 71200 loss: 0.0010 lr: 0.02\n",
      "iteration: 71250 loss: 0.0012 lr: 0.02\n",
      "iteration: 71300 loss: 0.0013 lr: 0.02\n",
      "iteration: 71350 loss: 0.0012 lr: 0.02\n",
      "iteration: 71400 loss: 0.0010 lr: 0.02\n",
      "iteration: 71450 loss: 0.0011 lr: 0.02\n",
      "iteration: 71500 loss: 0.0011 lr: 0.02\n",
      "iteration: 71550 loss: 0.0011 lr: 0.02\n",
      "iteration: 71600 loss: 0.0013 lr: 0.02\n",
      "iteration: 71650 loss: 0.0013 lr: 0.02\n",
      "iteration: 71700 loss: 0.0011 lr: 0.02\n",
      "iteration: 71750 loss: 0.0014 lr: 0.02\n",
      "iteration: 71800 loss: 0.0010 lr: 0.02\n",
      "iteration: 71850 loss: 0.0012 lr: 0.02\n",
      "iteration: 71900 loss: 0.0012 lr: 0.02\n",
      "iteration: 71950 loss: 0.0010 lr: 0.02\n",
      "iteration: 72000 loss: 0.0011 lr: 0.02\n",
      "iteration: 72050 loss: 0.0013 lr: 0.02\n",
      "iteration: 72100 loss: 0.0015 lr: 0.02\n",
      "iteration: 72150 loss: 0.0013 lr: 0.02\n",
      "iteration: 72200 loss: 0.0013 lr: 0.02\n",
      "iteration: 72250 loss: 0.0012 lr: 0.02\n",
      "iteration: 72300 loss: 0.0011 lr: 0.02\n",
      "iteration: 72350 loss: 0.0013 lr: 0.02\n",
      "iteration: 72400 loss: 0.0012 lr: 0.02\n",
      "iteration: 72450 loss: 0.0010 lr: 0.02\n",
      "iteration: 72500 loss: 0.0009 lr: 0.02\n",
      "iteration: 72550 loss: 0.0012 lr: 0.02\n",
      "iteration: 72600 loss: 0.0011 lr: 0.02\n",
      "iteration: 72650 loss: 0.0011 lr: 0.02\n",
      "iteration: 72700 loss: 0.0011 lr: 0.02\n",
      "iteration: 72750 loss: 0.0011 lr: 0.02\n",
      "iteration: 72800 loss: 0.0014 lr: 0.02\n",
      "iteration: 72850 loss: 0.0012 lr: 0.02\n",
      "iteration: 72900 loss: 0.0013 lr: 0.02\n",
      "iteration: 72950 loss: 0.0011 lr: 0.02\n",
      "iteration: 73000 loss: 0.0012 lr: 0.02\n",
      "iteration: 73050 loss: 0.0011 lr: 0.02\n",
      "iteration: 73100 loss: 0.0011 lr: 0.02\n",
      "iteration: 73150 loss: 0.0013 lr: 0.02\n",
      "iteration: 73200 loss: 0.0013 lr: 0.02\n",
      "iteration: 73250 loss: 0.0012 lr: 0.02\n",
      "iteration: 73300 loss: 0.0011 lr: 0.02\n",
      "iteration: 73350 loss: 0.0014 lr: 0.02\n",
      "iteration: 73400 loss: 0.0013 lr: 0.02\n",
      "iteration: 73450 loss: 0.0012 lr: 0.02\n",
      "iteration: 73500 loss: 0.0011 lr: 0.02\n",
      "iteration: 73550 loss: 0.0014 lr: 0.02\n",
      "iteration: 73600 loss: 0.0012 lr: 0.02\n",
      "iteration: 73650 loss: 0.0013 lr: 0.02\n",
      "iteration: 73700 loss: 0.0012 lr: 0.02\n",
      "iteration: 73750 loss: 0.0011 lr: 0.02\n",
      "iteration: 73800 loss: 0.0012 lr: 0.02\n",
      "iteration: 73850 loss: 0.0010 lr: 0.02\n",
      "iteration: 73900 loss: 0.0012 lr: 0.02\n",
      "iteration: 73950 loss: 0.0012 lr: 0.02\n",
      "iteration: 74000 loss: 0.0012 lr: 0.02\n",
      "iteration: 74050 loss: 0.0014 lr: 0.02\n",
      "iteration: 74100 loss: 0.0013 lr: 0.02\n",
      "iteration: 74150 loss: 0.0013 lr: 0.02\n",
      "iteration: 74200 loss: 0.0011 lr: 0.02\n",
      "iteration: 74250 loss: 0.0013 lr: 0.02\n",
      "iteration: 74300 loss: 0.0011 lr: 0.02\n",
      "iteration: 74350 loss: 0.0013 lr: 0.02\n",
      "iteration: 74400 loss: 0.0011 lr: 0.02\n",
      "iteration: 74450 loss: 0.0010 lr: 0.02\n",
      "iteration: 74500 loss: 0.0010 lr: 0.02\n",
      "iteration: 74550 loss: 0.0010 lr: 0.02\n",
      "iteration: 74600 loss: 0.0012 lr: 0.02\n",
      "iteration: 74650 loss: 0.0014 lr: 0.02\n",
      "iteration: 74700 loss: 0.0012 lr: 0.02\n",
      "iteration: 74750 loss: 0.0011 lr: 0.02\n",
      "iteration: 74800 loss: 0.0011 lr: 0.02\n",
      "iteration: 74850 loss: 0.0011 lr: 0.02\n",
      "iteration: 74900 loss: 0.0011 lr: 0.02\n",
      "iteration: 74950 loss: 0.0012 lr: 0.02\n",
      "iteration: 75000 loss: 0.0011 lr: 0.02\n",
      "iteration: 75050 loss: 0.0011 lr: 0.02\n",
      "iteration: 75100 loss: 0.0012 lr: 0.02\n",
      "iteration: 75150 loss: 0.0011 lr: 0.02\n",
      "iteration: 75200 loss: 0.0011 lr: 0.02\n",
      "iteration: 75250 loss: 0.0014 lr: 0.02\n",
      "iteration: 75300 loss: 0.0009 lr: 0.02\n",
      "iteration: 75350 loss: 0.0011 lr: 0.02\n",
      "iteration: 75400 loss: 0.0011 lr: 0.02\n",
      "iteration: 75450 loss: 0.0012 lr: 0.02\n",
      "iteration: 75500 loss: 0.0010 lr: 0.02\n",
      "iteration: 75550 loss: 0.0012 lr: 0.02\n",
      "iteration: 75600 loss: 0.0011 lr: 0.02\n",
      "iteration: 75650 loss: 0.0011 lr: 0.02\n",
      "iteration: 75700 loss: 0.0011 lr: 0.02\n",
      "iteration: 75750 loss: 0.0011 lr: 0.02\n",
      "iteration: 75800 loss: 0.0012 lr: 0.02\n",
      "iteration: 75850 loss: 0.0011 lr: 0.02\n",
      "iteration: 75900 loss: 0.0012 lr: 0.02\n",
      "iteration: 75950 loss: 0.0011 lr: 0.02\n",
      "iteration: 76000 loss: 0.0010 lr: 0.02\n",
      "iteration: 76050 loss: 0.0012 lr: 0.02\n",
      "iteration: 76100 loss: 0.0011 lr: 0.02\n",
      "iteration: 76150 loss: 0.0009 lr: 0.02\n",
      "iteration: 76200 loss: 0.0010 lr: 0.02\n",
      "iteration: 76250 loss: 0.0011 lr: 0.02\n",
      "iteration: 76300 loss: 0.0012 lr: 0.02\n",
      "iteration: 76350 loss: 0.0010 lr: 0.02\n",
      "iteration: 76400 loss: 0.0011 lr: 0.02\n",
      "iteration: 76450 loss: 0.0011 lr: 0.02\n",
      "iteration: 76500 loss: 0.0011 lr: 0.02\n",
      "iteration: 76550 loss: 0.0012 lr: 0.02\n",
      "iteration: 76600 loss: 0.0013 lr: 0.02\n",
      "iteration: 76650 loss: 0.0009 lr: 0.02\n",
      "iteration: 76700 loss: 0.0010 lr: 0.02\n",
      "iteration: 76750 loss: 0.0011 lr: 0.02\n",
      "iteration: 76800 loss: 0.0012 lr: 0.02\n",
      "iteration: 76850 loss: 0.0010 lr: 0.02\n",
      "iteration: 76900 loss: 0.0010 lr: 0.02\n",
      "iteration: 76950 loss: 0.0012 lr: 0.02\n",
      "iteration: 77000 loss: 0.0011 lr: 0.02\n",
      "iteration: 77050 loss: 0.0010 lr: 0.02\n",
      "iteration: 77100 loss: 0.0013 lr: 0.02\n",
      "iteration: 77150 loss: 0.0011 lr: 0.02\n",
      "iteration: 77200 loss: 0.0011 lr: 0.02\n",
      "iteration: 77250 loss: 0.0012 lr: 0.02\n",
      "iteration: 77300 loss: 0.0012 lr: 0.02\n",
      "iteration: 77350 loss: 0.0011 lr: 0.02\n",
      "iteration: 77400 loss: 0.0010 lr: 0.02\n",
      "iteration: 77450 loss: 0.0012 lr: 0.02\n",
      "iteration: 77500 loss: 0.0013 lr: 0.02\n",
      "iteration: 77550 loss: 0.0014 lr: 0.02\n",
      "iteration: 77600 loss: 0.0011 lr: 0.02\n",
      "iteration: 77650 loss: 0.0011 lr: 0.02\n",
      "iteration: 77700 loss: 0.0012 lr: 0.02\n",
      "iteration: 77750 loss: 0.0012 lr: 0.02\n",
      "iteration: 77800 loss: 0.0011 lr: 0.02\n",
      "iteration: 77850 loss: 0.0011 lr: 0.02\n",
      "iteration: 77900 loss: 0.0013 lr: 0.02\n",
      "iteration: 77950 loss: 0.0010 lr: 0.02\n",
      "iteration: 78000 loss: 0.0012 lr: 0.02\n",
      "iteration: 78050 loss: 0.0011 lr: 0.02\n",
      "iteration: 78100 loss: 0.0010 lr: 0.02\n",
      "iteration: 78150 loss: 0.0010 lr: 0.02\n",
      "iteration: 78200 loss: 0.0012 lr: 0.02\n",
      "iteration: 78250 loss: 0.0013 lr: 0.02\n",
      "iteration: 78300 loss: 0.0014 lr: 0.02\n",
      "iteration: 78350 loss: 0.0012 lr: 0.02\n",
      "iteration: 78400 loss: 0.0013 lr: 0.02\n",
      "iteration: 78450 loss: 0.0011 lr: 0.02\n",
      "iteration: 78500 loss: 0.0011 lr: 0.02\n",
      "iteration: 78550 loss: 0.0012 lr: 0.02\n",
      "iteration: 78600 loss: 0.0012 lr: 0.02\n",
      "iteration: 78650 loss: 0.0010 lr: 0.02\n",
      "iteration: 78700 loss: 0.0011 lr: 0.02\n",
      "iteration: 78750 loss: 0.0011 lr: 0.02\n",
      "iteration: 78800 loss: 0.0013 lr: 0.02\n",
      "iteration: 78850 loss: 0.0012 lr: 0.02\n",
      "iteration: 78900 loss: 0.0012 lr: 0.02\n",
      "iteration: 78950 loss: 0.0016 lr: 0.02\n",
      "iteration: 79000 loss: 0.0011 lr: 0.02\n",
      "iteration: 79050 loss: 0.0010 lr: 0.02\n",
      "iteration: 79100 loss: 0.0011 lr: 0.02\n",
      "iteration: 79150 loss: 0.0012 lr: 0.02\n",
      "iteration: 79200 loss: 0.0011 lr: 0.02\n",
      "iteration: 79250 loss: 0.0011 lr: 0.02\n",
      "iteration: 79300 loss: 0.0011 lr: 0.02\n",
      "iteration: 79350 loss: 0.0011 lr: 0.02\n",
      "iteration: 79400 loss: 0.0012 lr: 0.02\n",
      "iteration: 79450 loss: 0.0012 lr: 0.02\n",
      "iteration: 79500 loss: 0.0008 lr: 0.02\n",
      "iteration: 79550 loss: 0.0012 lr: 0.02\n",
      "iteration: 79600 loss: 0.0010 lr: 0.02\n",
      "iteration: 79650 loss: 0.0012 lr: 0.02\n",
      "iteration: 79700 loss: 0.0010 lr: 0.02\n",
      "iteration: 79750 loss: 0.0013 lr: 0.02\n",
      "iteration: 79800 loss: 0.0011 lr: 0.02\n",
      "iteration: 79850 loss: 0.0012 lr: 0.02\n",
      "iteration: 79900 loss: 0.0010 lr: 0.02\n",
      "iteration: 79950 loss: 0.0012 lr: 0.02\n",
      "iteration: 80000 loss: 0.0010 lr: 0.02\n",
      "iteration: 80050 loss: 0.0010 lr: 0.02\n",
      "iteration: 80100 loss: 0.0009 lr: 0.02\n",
      "iteration: 80150 loss: 0.0010 lr: 0.02\n",
      "iteration: 80200 loss: 0.0012 lr: 0.02\n",
      "iteration: 80250 loss: 0.0011 lr: 0.02\n",
      "iteration: 80300 loss: 0.0011 lr: 0.02\n",
      "iteration: 80350 loss: 0.0011 lr: 0.02\n",
      "iteration: 80400 loss: 0.0011 lr: 0.02\n",
      "iteration: 80450 loss: 0.0011 lr: 0.02\n",
      "iteration: 80500 loss: 0.0011 lr: 0.02\n",
      "iteration: 80550 loss: 0.0010 lr: 0.02\n",
      "iteration: 80600 loss: 0.0010 lr: 0.02\n",
      "iteration: 80650 loss: 0.0011 lr: 0.02\n",
      "iteration: 80700 loss: 0.0009 lr: 0.02\n",
      "iteration: 80750 loss: 0.0013 lr: 0.02\n",
      "iteration: 80800 loss: 0.0013 lr: 0.02\n",
      "iteration: 80850 loss: 0.0011 lr: 0.02\n",
      "iteration: 80900 loss: 0.0011 lr: 0.02\n",
      "iteration: 80950 loss: 0.0011 lr: 0.02\n",
      "iteration: 81000 loss: 0.0009 lr: 0.02\n",
      "iteration: 81050 loss: 0.0011 lr: 0.02\n",
      "iteration: 81100 loss: 0.0010 lr: 0.02\n",
      "iteration: 81150 loss: 0.0011 lr: 0.02\n",
      "iteration: 81200 loss: 0.0010 lr: 0.02\n",
      "iteration: 81250 loss: 0.0011 lr: 0.02\n",
      "iteration: 81300 loss: 0.0010 lr: 0.02\n",
      "iteration: 81350 loss: 0.0012 lr: 0.02\n",
      "iteration: 81400 loss: 0.0012 lr: 0.02\n",
      "iteration: 81450 loss: 0.0010 lr: 0.02\n",
      "iteration: 81500 loss: 0.0010 lr: 0.02\n",
      "iteration: 81550 loss: 0.0011 lr: 0.02\n",
      "iteration: 81600 loss: 0.0010 lr: 0.02\n",
      "iteration: 81650 loss: 0.0012 lr: 0.02\n",
      "iteration: 81700 loss: 0.0011 lr: 0.02\n",
      "iteration: 81750 loss: 0.0009 lr: 0.02\n",
      "iteration: 81800 loss: 0.0011 lr: 0.02\n",
      "iteration: 81850 loss: 0.0011 lr: 0.02\n",
      "iteration: 81900 loss: 0.0012 lr: 0.02\n",
      "iteration: 81950 loss: 0.0012 lr: 0.02\n",
      "iteration: 82000 loss: 0.0011 lr: 0.02\n",
      "iteration: 82050 loss: 0.0010 lr: 0.02\n",
      "iteration: 82100 loss: 0.0010 lr: 0.02\n",
      "iteration: 82150 loss: 0.0010 lr: 0.02\n",
      "iteration: 82200 loss: 0.0011 lr: 0.02\n",
      "iteration: 82250 loss: 0.0010 lr: 0.02\n",
      "iteration: 82300 loss: 0.0011 lr: 0.02\n",
      "iteration: 82350 loss: 0.0012 lr: 0.02\n",
      "iteration: 82400 loss: 0.0010 lr: 0.02\n",
      "iteration: 82450 loss: 0.0011 lr: 0.02\n",
      "iteration: 82500 loss: 0.0010 lr: 0.02\n",
      "iteration: 82550 loss: 0.0011 lr: 0.02\n",
      "iteration: 82600 loss: 0.0010 lr: 0.02\n",
      "iteration: 82650 loss: 0.0010 lr: 0.02\n",
      "iteration: 82700 loss: 0.0011 lr: 0.02\n",
      "iteration: 82750 loss: 0.0011 lr: 0.02\n",
      "iteration: 82800 loss: 0.0013 lr: 0.02\n",
      "iteration: 82850 loss: 0.0010 lr: 0.02\n",
      "iteration: 82900 loss: 0.0009 lr: 0.02\n",
      "iteration: 82950 loss: 0.0013 lr: 0.02\n",
      "iteration: 83000 loss: 0.0009 lr: 0.02\n",
      "iteration: 83050 loss: 0.0012 lr: 0.02\n",
      "iteration: 83100 loss: 0.0009 lr: 0.02\n",
      "iteration: 83150 loss: 0.0011 lr: 0.02\n",
      "iteration: 83200 loss: 0.0010 lr: 0.02\n",
      "iteration: 83250 loss: 0.0009 lr: 0.02\n",
      "iteration: 83300 loss: 0.0010 lr: 0.02\n",
      "iteration: 83350 loss: 0.0011 lr: 0.02\n",
      "iteration: 83400 loss: 0.0010 lr: 0.02\n",
      "iteration: 83450 loss: 0.0012 lr: 0.02\n",
      "iteration: 83500 loss: 0.0011 lr: 0.02\n",
      "iteration: 83550 loss: 0.0010 lr: 0.02\n",
      "iteration: 83600 loss: 0.0012 lr: 0.02\n",
      "iteration: 83650 loss: 0.0012 lr: 0.02\n",
      "iteration: 83700 loss: 0.0010 lr: 0.02\n",
      "iteration: 83750 loss: 0.0011 lr: 0.02\n",
      "iteration: 83800 loss: 0.0011 lr: 0.02\n",
      "iteration: 83850 loss: 0.0012 lr: 0.02\n",
      "iteration: 83900 loss: 0.0011 lr: 0.02\n",
      "iteration: 83950 loss: 0.0012 lr: 0.02\n",
      "iteration: 84000 loss: 0.0011 lr: 0.02\n",
      "iteration: 84050 loss: 0.0010 lr: 0.02\n",
      "iteration: 84100 loss: 0.0010 lr: 0.02\n",
      "iteration: 84150 loss: 0.0010 lr: 0.02\n",
      "iteration: 84200 loss: 0.0010 lr: 0.02\n",
      "iteration: 84250 loss: 0.0011 lr: 0.02\n",
      "iteration: 84300 loss: 0.0012 lr: 0.02\n",
      "iteration: 84350 loss: 0.0010 lr: 0.02\n",
      "iteration: 84400 loss: 0.0011 lr: 0.02\n",
      "iteration: 84450 loss: 0.0010 lr: 0.02\n",
      "iteration: 84500 loss: 0.0011 lr: 0.02\n",
      "iteration: 84550 loss: 0.0012 lr: 0.02\n",
      "iteration: 84600 loss: 0.0010 lr: 0.02\n",
      "iteration: 84650 loss: 0.0010 lr: 0.02\n",
      "iteration: 84700 loss: 0.0009 lr: 0.02\n",
      "iteration: 84750 loss: 0.0010 lr: 0.02\n",
      "iteration: 84800 loss: 0.0010 lr: 0.02\n",
      "iteration: 84850 loss: 0.0010 lr: 0.02\n",
      "iteration: 84900 loss: 0.0010 lr: 0.02\n",
      "iteration: 84950 loss: 0.0011 lr: 0.02\n",
      "iteration: 85000 loss: 0.0009 lr: 0.02\n",
      "iteration: 85050 loss: 0.0011 lr: 0.02\n",
      "iteration: 85100 loss: 0.0011 lr: 0.02\n",
      "iteration: 85150 loss: 0.0011 lr: 0.02\n",
      "iteration: 85200 loss: 0.0011 lr: 0.02\n",
      "iteration: 85250 loss: 0.0010 lr: 0.02\n",
      "iteration: 85300 loss: 0.0010 lr: 0.02\n",
      "iteration: 85350 loss: 0.0013 lr: 0.02\n",
      "iteration: 85400 loss: 0.0009 lr: 0.02\n",
      "iteration: 85450 loss: 0.0011 lr: 0.02\n",
      "iteration: 85500 loss: 0.0013 lr: 0.02\n",
      "iteration: 85550 loss: 0.0010 lr: 0.02\n",
      "iteration: 85600 loss: 0.0011 lr: 0.02\n",
      "iteration: 85650 loss: 0.0010 lr: 0.02\n",
      "iteration: 85700 loss: 0.0010 lr: 0.02\n",
      "iteration: 85750 loss: 0.0011 lr: 0.02\n",
      "iteration: 85800 loss: 0.0010 lr: 0.02\n",
      "iteration: 85850 loss: 0.0012 lr: 0.02\n",
      "iteration: 85900 loss: 0.0010 lr: 0.02\n",
      "iteration: 85950 loss: 0.0011 lr: 0.02\n",
      "iteration: 86000 loss: 0.0010 lr: 0.02\n",
      "iteration: 86050 loss: 0.0009 lr: 0.02\n",
      "iteration: 86100 loss: 0.0010 lr: 0.02\n",
      "iteration: 86150 loss: 0.0010 lr: 0.02\n",
      "iteration: 86200 loss: 0.0011 lr: 0.02\n",
      "iteration: 86250 loss: 0.0011 lr: 0.02\n",
      "iteration: 86300 loss: 0.0010 lr: 0.02\n",
      "iteration: 86350 loss: 0.0011 lr: 0.02\n",
      "iteration: 86400 loss: 0.0010 lr: 0.02\n",
      "iteration: 86450 loss: 0.0011 lr: 0.02\n",
      "iteration: 86500 loss: 0.0011 lr: 0.02\n",
      "iteration: 86550 loss: 0.0010 lr: 0.02\n",
      "iteration: 86600 loss: 0.0011 lr: 0.02\n",
      "iteration: 86650 loss: 0.0011 lr: 0.02\n",
      "iteration: 86700 loss: 0.0012 lr: 0.02\n",
      "iteration: 86750 loss: 0.0010 lr: 0.02\n",
      "iteration: 86800 loss: 0.0010 lr: 0.02\n",
      "iteration: 86850 loss: 0.0010 lr: 0.02\n",
      "iteration: 86900 loss: 0.0009 lr: 0.02\n",
      "iteration: 86950 loss: 0.0012 lr: 0.02\n",
      "iteration: 87000 loss: 0.0011 lr: 0.02\n",
      "iteration: 87050 loss: 0.0014 lr: 0.02\n",
      "iteration: 87100 loss: 0.0010 lr: 0.02\n",
      "iteration: 87150 loss: 0.0011 lr: 0.02\n",
      "iteration: 87200 loss: 0.0011 lr: 0.02\n",
      "iteration: 87250 loss: 0.0012 lr: 0.02\n",
      "iteration: 87300 loss: 0.0010 lr: 0.02\n",
      "iteration: 87350 loss: 0.0010 lr: 0.02\n",
      "iteration: 87400 loss: 0.0012 lr: 0.02\n",
      "iteration: 87450 loss: 0.0011 lr: 0.02\n",
      "iteration: 87500 loss: 0.0011 lr: 0.02\n",
      "iteration: 87550 loss: 0.0012 lr: 0.02\n",
      "iteration: 87600 loss: 0.0011 lr: 0.02\n",
      "iteration: 87650 loss: 0.0012 lr: 0.02\n",
      "iteration: 87700 loss: 0.0011 lr: 0.02\n",
      "iteration: 87750 loss: 0.0011 lr: 0.02\n",
      "iteration: 87800 loss: 0.0009 lr: 0.02\n",
      "iteration: 87850 loss: 0.0011 lr: 0.02\n",
      "iteration: 87900 loss: 0.0011 lr: 0.02\n",
      "iteration: 87950 loss: 0.0012 lr: 0.02\n",
      "iteration: 88000 loss: 0.0010 lr: 0.02\n",
      "iteration: 88050 loss: 0.0010 lr: 0.02\n",
      "iteration: 88100 loss: 0.0012 lr: 0.02\n",
      "iteration: 88150 loss: 0.0010 lr: 0.02\n",
      "iteration: 88200 loss: 0.0011 lr: 0.02\n",
      "iteration: 88250 loss: 0.0011 lr: 0.02\n",
      "iteration: 88300 loss: 0.0011 lr: 0.02\n",
      "iteration: 88350 loss: 0.0014 lr: 0.02\n",
      "iteration: 88400 loss: 0.0011 lr: 0.02\n",
      "iteration: 88450 loss: 0.0011 lr: 0.02\n",
      "iteration: 88500 loss: 0.0009 lr: 0.02\n",
      "iteration: 88550 loss: 0.0009 lr: 0.02\n",
      "iteration: 88600 loss: 0.0010 lr: 0.02\n",
      "iteration: 88650 loss: 0.0011 lr: 0.02\n",
      "iteration: 88700 loss: 0.0010 lr: 0.02\n",
      "iteration: 88750 loss: 0.0013 lr: 0.02\n",
      "iteration: 88800 loss: 0.0011 lr: 0.02\n",
      "iteration: 88850 loss: 0.0009 lr: 0.02\n",
      "iteration: 88900 loss: 0.0009 lr: 0.02\n",
      "iteration: 88950 loss: 0.0013 lr: 0.02\n",
      "iteration: 89000 loss: 0.0010 lr: 0.02\n",
      "iteration: 89050 loss: 0.0010 lr: 0.02\n",
      "iteration: 89100 loss: 0.0011 lr: 0.02\n",
      "iteration: 89150 loss: 0.0011 lr: 0.02\n",
      "iteration: 89200 loss: 0.0010 lr: 0.02\n",
      "iteration: 89250 loss: 0.0013 lr: 0.02\n",
      "iteration: 89300 loss: 0.0010 lr: 0.02\n",
      "iteration: 89350 loss: 0.0013 lr: 0.02\n",
      "iteration: 89400 loss: 0.0010 lr: 0.02\n",
      "iteration: 89450 loss: 0.0011 lr: 0.02\n",
      "iteration: 89500 loss: 0.0012 lr: 0.02\n",
      "iteration: 89550 loss: 0.0011 lr: 0.02\n",
      "iteration: 89600 loss: 0.0010 lr: 0.02\n",
      "iteration: 89650 loss: 0.0010 lr: 0.02\n",
      "iteration: 89700 loss: 0.0010 lr: 0.02\n",
      "iteration: 89750 loss: 0.0011 lr: 0.02\n",
      "iteration: 89800 loss: 0.0010 lr: 0.02\n",
      "iteration: 89850 loss: 0.0009 lr: 0.02\n",
      "iteration: 89900 loss: 0.0011 lr: 0.02\n",
      "iteration: 89950 loss: 0.0010 lr: 0.02\n",
      "iteration: 90000 loss: 0.0011 lr: 0.02\n",
      "iteration: 90050 loss: 0.0011 lr: 0.02\n",
      "iteration: 90100 loss: 0.0012 lr: 0.02\n",
      "iteration: 90150 loss: 0.0012 lr: 0.02\n",
      "iteration: 90200 loss: 0.0010 lr: 0.02\n",
      "iteration: 90250 loss: 0.0012 lr: 0.02\n",
      "iteration: 90300 loss: 0.0010 lr: 0.02\n",
      "iteration: 90350 loss: 0.0010 lr: 0.02\n",
      "iteration: 90400 loss: 0.0010 lr: 0.02\n",
      "iteration: 90450 loss: 0.0012 lr: 0.02\n",
      "iteration: 90500 loss: 0.0010 lr: 0.02\n",
      "iteration: 90550 loss: 0.0011 lr: 0.02\n",
      "iteration: 90600 loss: 0.0011 lr: 0.02\n",
      "iteration: 90650 loss: 0.0011 lr: 0.02\n",
      "iteration: 90700 loss: 0.0008 lr: 0.02\n",
      "iteration: 90750 loss: 0.0010 lr: 0.02\n",
      "iteration: 90800 loss: 0.0012 lr: 0.02\n",
      "iteration: 90850 loss: 0.0012 lr: 0.02\n",
      "iteration: 90900 loss: 0.0012 lr: 0.02\n",
      "iteration: 90950 loss: 0.0010 lr: 0.02\n",
      "iteration: 91000 loss: 0.0011 lr: 0.02\n",
      "iteration: 91050 loss: 0.0011 lr: 0.02\n",
      "iteration: 91100 loss: 0.0010 lr: 0.02\n",
      "iteration: 91150 loss: 0.0011 lr: 0.02\n",
      "iteration: 91200 loss: 0.0010 lr: 0.02\n",
      "iteration: 91250 loss: 0.0010 lr: 0.02\n",
      "iteration: 91300 loss: 0.0011 lr: 0.02\n",
      "iteration: 91350 loss: 0.0011 lr: 0.02\n",
      "iteration: 91400 loss: 0.0011 lr: 0.02\n",
      "iteration: 91450 loss: 0.0009 lr: 0.02\n",
      "iteration: 91500 loss: 0.0010 lr: 0.02\n",
      "iteration: 91550 loss: 0.0010 lr: 0.02\n",
      "iteration: 91600 loss: 0.0011 lr: 0.02\n",
      "iteration: 91650 loss: 0.0011 lr: 0.02\n",
      "iteration: 91700 loss: 0.0011 lr: 0.02\n",
      "iteration: 91750 loss: 0.0009 lr: 0.02\n",
      "iteration: 91800 loss: 0.0009 lr: 0.02\n",
      "iteration: 91850 loss: 0.0011 lr: 0.02\n",
      "iteration: 91900 loss: 0.0010 lr: 0.02\n",
      "iteration: 91950 loss: 0.0011 lr: 0.02\n",
      "iteration: 92000 loss: 0.0011 lr: 0.02\n",
      "iteration: 92050 loss: 0.0010 lr: 0.02\n",
      "iteration: 92100 loss: 0.0011 lr: 0.02\n",
      "iteration: 92150 loss: 0.0011 lr: 0.02\n",
      "iteration: 92200 loss: 0.0011 lr: 0.02\n",
      "iteration: 92250 loss: 0.0010 lr: 0.02\n",
      "iteration: 92300 loss: 0.0012 lr: 0.02\n",
      "iteration: 92350 loss: 0.0012 lr: 0.02\n",
      "iteration: 92400 loss: 0.0013 lr: 0.02\n",
      "iteration: 92450 loss: 0.0012 lr: 0.02\n",
      "iteration: 92500 loss: 0.0011 lr: 0.02\n",
      "iteration: 92550 loss: 0.0012 lr: 0.02\n",
      "iteration: 92600 loss: 0.0011 lr: 0.02\n",
      "iteration: 92650 loss: 0.0012 lr: 0.02\n",
      "iteration: 92700 loss: 0.0010 lr: 0.02\n",
      "iteration: 92750 loss: 0.0011 lr: 0.02\n",
      "iteration: 92800 loss: 0.0011 lr: 0.02\n",
      "iteration: 92850 loss: 0.0010 lr: 0.02\n",
      "iteration: 92900 loss: 0.0010 lr: 0.02\n",
      "iteration: 92950 loss: 0.0011 lr: 0.02\n",
      "iteration: 93000 loss: 0.0010 lr: 0.02\n",
      "iteration: 93050 loss: 0.0011 lr: 0.02\n",
      "iteration: 93100 loss: 0.0012 lr: 0.02\n",
      "iteration: 93150 loss: 0.0011 lr: 0.02\n",
      "iteration: 93200 loss: 0.0010 lr: 0.02\n",
      "iteration: 93250 loss: 0.0011 lr: 0.02\n",
      "iteration: 93300 loss: 0.0010 lr: 0.02\n",
      "iteration: 93350 loss: 0.0011 lr: 0.02\n",
      "iteration: 93400 loss: 0.0010 lr: 0.02\n",
      "iteration: 93450 loss: 0.0011 lr: 0.02\n",
      "iteration: 93500 loss: 0.0010 lr: 0.02\n",
      "iteration: 93550 loss: 0.0010 lr: 0.02\n",
      "iteration: 93600 loss: 0.0011 lr: 0.02\n",
      "iteration: 93650 loss: 0.0012 lr: 0.02\n",
      "iteration: 93700 loss: 0.0011 lr: 0.02\n",
      "iteration: 93750 loss: 0.0010 lr: 0.02\n",
      "iteration: 93800 loss: 0.0010 lr: 0.02\n",
      "iteration: 93850 loss: 0.0010 lr: 0.02\n",
      "iteration: 93900 loss: 0.0009 lr: 0.02\n",
      "iteration: 93950 loss: 0.0011 lr: 0.02\n",
      "iteration: 94000 loss: 0.0010 lr: 0.02\n",
      "iteration: 94050 loss: 0.0010 lr: 0.02\n",
      "iteration: 94100 loss: 0.0008 lr: 0.02\n",
      "iteration: 94150 loss: 0.0010 lr: 0.02\n",
      "iteration: 94200 loss: 0.0012 lr: 0.02\n",
      "iteration: 94250 loss: 0.0014 lr: 0.02\n",
      "iteration: 94300 loss: 0.0009 lr: 0.02\n",
      "iteration: 94350 loss: 0.0011 lr: 0.02\n",
      "iteration: 94400 loss: 0.0009 lr: 0.02\n",
      "iteration: 94450 loss: 0.0009 lr: 0.02\n",
      "iteration: 94500 loss: 0.0010 lr: 0.02\n",
      "iteration: 94550 loss: 0.0011 lr: 0.02\n",
      "iteration: 94600 loss: 0.0011 lr: 0.02\n",
      "iteration: 94650 loss: 0.0010 lr: 0.02\n",
      "iteration: 94700 loss: 0.0011 lr: 0.02\n",
      "iteration: 94750 loss: 0.0009 lr: 0.02\n",
      "iteration: 94800 loss: 0.0010 lr: 0.02\n",
      "iteration: 94850 loss: 0.0012 lr: 0.02\n",
      "iteration: 94900 loss: 0.0012 lr: 0.02\n",
      "iteration: 94950 loss: 0.0010 lr: 0.02\n",
      "iteration: 95000 loss: 0.0010 lr: 0.02\n",
      "iteration: 95050 loss: 0.0010 lr: 0.02\n",
      "iteration: 95100 loss: 0.0009 lr: 0.02\n",
      "iteration: 95150 loss: 0.0011 lr: 0.02\n",
      "iteration: 95200 loss: 0.0011 lr: 0.02\n",
      "iteration: 95250 loss: 0.0010 lr: 0.02\n",
      "iteration: 95300 loss: 0.0010 lr: 0.02\n",
      "iteration: 95350 loss: 0.0010 lr: 0.02\n",
      "iteration: 95400 loss: 0.0010 lr: 0.02\n",
      "iteration: 95450 loss: 0.0010 lr: 0.02\n",
      "iteration: 95500 loss: 0.0012 lr: 0.02\n",
      "iteration: 95550 loss: 0.0010 lr: 0.02\n",
      "iteration: 95600 loss: 0.0011 lr: 0.02\n",
      "iteration: 95650 loss: 0.0011 lr: 0.02\n",
      "iteration: 95700 loss: 0.0012 lr: 0.02\n",
      "iteration: 95750 loss: 0.0011 lr: 0.02\n",
      "iteration: 95800 loss: 0.0012 lr: 0.02\n",
      "iteration: 95850 loss: 0.0011 lr: 0.02\n",
      "iteration: 95900 loss: 0.0011 lr: 0.02\n",
      "iteration: 95950 loss: 0.0009 lr: 0.02\n",
      "iteration: 96000 loss: 0.0011 lr: 0.02\n",
      "iteration: 96050 loss: 0.0010 lr: 0.02\n",
      "iteration: 96100 loss: 0.0013 lr: 0.02\n",
      "iteration: 96150 loss: 0.0011 lr: 0.02\n",
      "iteration: 96200 loss: 0.0010 lr: 0.02\n",
      "iteration: 96250 loss: 0.0009 lr: 0.02\n",
      "iteration: 96300 loss: 0.0011 lr: 0.02\n",
      "iteration: 96350 loss: 0.0010 lr: 0.02\n",
      "iteration: 96400 loss: 0.0010 lr: 0.02\n",
      "iteration: 96450 loss: 0.0011 lr: 0.02\n",
      "iteration: 96500 loss: 0.0009 lr: 0.02\n",
      "iteration: 96550 loss: 0.0011 lr: 0.02\n",
      "iteration: 96600 loss: 0.0009 lr: 0.02\n",
      "iteration: 96650 loss: 0.0011 lr: 0.02\n",
      "iteration: 96700 loss: 0.0011 lr: 0.02\n",
      "iteration: 96750 loss: 0.0011 lr: 0.02\n",
      "iteration: 96800 loss: 0.0011 lr: 0.02\n",
      "iteration: 96850 loss: 0.0010 lr: 0.02\n",
      "iteration: 96900 loss: 0.0011 lr: 0.02\n",
      "iteration: 96950 loss: 0.0010 lr: 0.02\n",
      "iteration: 97000 loss: 0.0010 lr: 0.02\n",
      "iteration: 97050 loss: 0.0011 lr: 0.02\n",
      "iteration: 97100 loss: 0.0010 lr: 0.02\n",
      "iteration: 97150 loss: 0.0009 lr: 0.02\n",
      "iteration: 97200 loss: 0.0010 lr: 0.02\n",
      "iteration: 97250 loss: 0.0012 lr: 0.02\n",
      "iteration: 97300 loss: 0.0011 lr: 0.02\n",
      "iteration: 97350 loss: 0.0011 lr: 0.02\n",
      "iteration: 97400 loss: 0.0010 lr: 0.02\n",
      "iteration: 97450 loss: 0.0012 lr: 0.02\n",
      "iteration: 97500 loss: 0.0010 lr: 0.02\n",
      "iteration: 97550 loss: 0.0009 lr: 0.02\n",
      "iteration: 97600 loss: 0.0010 lr: 0.02\n",
      "iteration: 97650 loss: 0.0009 lr: 0.02\n",
      "iteration: 97700 loss: 0.0011 lr: 0.02\n",
      "iteration: 97750 loss: 0.0010 lr: 0.02\n",
      "iteration: 97800 loss: 0.0014 lr: 0.02\n",
      "iteration: 97850 loss: 0.0011 lr: 0.02\n",
      "iteration: 97900 loss: 0.0012 lr: 0.02\n",
      "iteration: 97950 loss: 0.0011 lr: 0.02\n",
      "iteration: 98000 loss: 0.0011 lr: 0.02\n",
      "iteration: 98050 loss: 0.0010 lr: 0.02\n",
      "iteration: 98100 loss: 0.0009 lr: 0.02\n",
      "iteration: 98150 loss: 0.0010 lr: 0.02\n",
      "iteration: 98200 loss: 0.0009 lr: 0.02\n",
      "iteration: 98250 loss: 0.0010 lr: 0.02\n",
      "iteration: 98300 loss: 0.0009 lr: 0.02\n",
      "iteration: 98350 loss: 0.0011 lr: 0.02\n",
      "iteration: 98400 loss: 0.0010 lr: 0.02\n",
      "iteration: 98450 loss: 0.0011 lr: 0.02\n",
      "iteration: 98500 loss: 0.0011 lr: 0.02\n",
      "iteration: 98550 loss: 0.0010 lr: 0.02\n",
      "iteration: 98600 loss: 0.0010 lr: 0.02\n",
      "iteration: 98650 loss: 0.0011 lr: 0.02\n",
      "iteration: 98700 loss: 0.0009 lr: 0.02\n",
      "iteration: 98750 loss: 0.0010 lr: 0.02\n",
      "iteration: 98800 loss: 0.0010 lr: 0.02\n",
      "iteration: 98850 loss: 0.0011 lr: 0.02\n",
      "iteration: 98900 loss: 0.0011 lr: 0.02\n",
      "iteration: 98950 loss: 0.0009 lr: 0.02\n",
      "iteration: 99000 loss: 0.0009 lr: 0.02\n",
      "iteration: 99050 loss: 0.0010 lr: 0.02\n",
      "iteration: 99100 loss: 0.0010 lr: 0.02\n",
      "iteration: 99150 loss: 0.0011 lr: 0.02\n",
      "iteration: 99200 loss: 0.0010 lr: 0.02\n",
      "iteration: 99250 loss: 0.0012 lr: 0.02\n",
      "iteration: 99300 loss: 0.0010 lr: 0.02\n",
      "iteration: 99350 loss: 0.0010 lr: 0.02\n",
      "iteration: 99400 loss: 0.0009 lr: 0.02\n",
      "iteration: 99450 loss: 0.0011 lr: 0.02\n",
      "iteration: 99500 loss: 0.0010 lr: 0.02\n",
      "iteration: 99550 loss: 0.0011 lr: 0.02\n",
      "iteration: 99600 loss: 0.0010 lr: 0.02\n",
      "iteration: 99650 loss: 0.0010 lr: 0.02\n",
      "iteration: 99700 loss: 0.0011 lr: 0.02\n",
      "iteration: 99750 loss: 0.0010 lr: 0.02\n",
      "iteration: 99800 loss: 0.0008 lr: 0.02\n",
      "iteration: 99850 loss: 0.0009 lr: 0.02\n",
      "iteration: 99900 loss: 0.0010 lr: 0.02\n",
      "iteration: 99950 loss: 0.0012 lr: 0.02\n",
      "iteration: 100000 loss: 0.0011 lr: 0.02\n",
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1334, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1319, in _run_fn\n",
      "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1407, in _call_tf_sessionrun\n",
      "    run_metadata)\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Enqueue operation was cancelled\n",
      "\t [[{{node fifo_queue_enqueue}}]]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\train.py\", line 69, in load_and_enqueue\n",
      "    sess.run(enqueue_op, feed_dict=food)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 929, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1152, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1328, in _do_run\n",
      "    run_metadata)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1348, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Enqueue operation was cancelled\n",
      "\t [[node fifo_queue_enqueue (defined at C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\train.py:55) ]]\n",
      "\n",
      "Caused by op 'fifo_queue_enqueue', defined at:\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 563, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\asyncio\\base_events.py\", line 438, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\asyncio\\base_events.py\", line 1451, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\asyncio\\events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n",
      "    lambda f: self._run_callback(functools.partial(callback, future))\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 272, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 542, in execute_request\n",
      "    user_expressions, allow_stdin,\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2683, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2793, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2847, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-12-1c0a7cc67a44>\", line 1, in <module>\n",
      "    deeplabcut.train_network(path_config_file, displayiters=50, maxiters=100000)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\training.py\", line 95, in train_network\n",
      "    train(str(poseconfigfile),displayiters,saveiters,maxiters,max_to_keep=max_snapshots_to_keep) #pass on path and file name for pose_cfg.yaml!\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\train.py\", line 104, in train\n",
      "    batch, enqueue_op, placeholders = setup_preloading(batch_spec)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\train.py\", line 55, in setup_preloading\n",
      "    enqueue_op = q.enqueue(placeholders_list)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\ops\\data_flow_ops.py\", line 345, in enqueue\n",
      "    self._queue_ref, vals, name=scope)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\ops\\gen_data_flow_ops.py\", line 4158, in queue_enqueue_v2\n",
      "    timeout_ms=timeout_ms, name=name)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3300, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1801, in __init__\n",
      "    self._traceback = tf_stack.extract_stack()\n",
      "\n",
      "CancelledError (see above for traceback): Enqueue operation was cancelled\n",
      "\t [[node fifo_queue_enqueue (defined at C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\train.py:55) ]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network is now trained and ready to evaluate. Use the function 'evaluate_network' to evaluate the network.\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.train_network(path_config_file, displayiters=50, maxiters=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "extract_outlier_frames() missing 1 required positional argument: 'videos'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-f8ee8997eb50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdeeplabcut\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_outlier_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_config_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: extract_outlier_frames() missing 1 required positional argument: 'videos'"
     ]
    }
   ],
   "source": [
    "deeplabcut.extract_outlier_frames(path_config_file,['/videos/video3.avi']) #pass a specific video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nv4zlbrnoEJg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0], [1], [2]],\n",
      " 'all_joints_names': ['COM', 'cal1', 'cal2'],\n",
      " 'batch_size': 1,\n",
      " 'bottomheight': 400,\n",
      " 'crop': True,\n",
      " 'crop_pad': 0,\n",
      " 'cropratio': 0.4,\n",
      " 'dataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_arena_chaseOct21\\\\arena_chase_Andy95shuffle1.mat',\n",
      " 'dataset_type': 'default',\n",
      " 'deterministic': False,\n",
      " 'display_iters': 1000,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': 'C:\\\\Users\\\\LabAdmin\\\\.conda\\\\envs\\\\dlc-windowsGPU\\\\lib\\\\site-packages\\\\deeplabcut\\\\pose_estimation_tensorflow\\\\models\\\\pretrained\\\\resnet_v1_50.ckpt',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'leftwidth': 400,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 0.05,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'max_input_size': 1500,\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'metadataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_arena_chaseOct21\\\\Documentation_data-arena_chase_95shuffle1.pickle',\n",
      " 'min_input_size': 64,\n",
      " 'minsize': 100,\n",
      " 'mirror': False,\n",
      " 'multi_step': [[0.005, 10000],\n",
      "                [0.02, 430000],\n",
      "                [0.002, 730000],\n",
      "                [0.001, 1030000]],\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 3,\n",
      " 'optimizer': 'sgd',\n",
      " 'pos_dist_thresh': 17,\n",
      " 'project_path': 'E:\\\\Users\\\\Phil\\\\DeepLabCut\\\\dev\\\\arena_chase-Andy-2019-10-21',\n",
      " 'regularize': False,\n",
      " 'rightwidth': 400,\n",
      " 'save_iters': 50000,\n",
      " 'scale_jitter_lo': 0.5,\n",
      " 'scale_jitter_up': 1.25,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': 'E:\\\\Users\\\\Phil\\\\DeepLabCut\\\\dev\\\\arena_chase-Andy-2019-10-21\\\\dlc-models\\\\iteration-0\\\\arena_chaseOct21-trainset95shuffle1\\\\test\\\\snapshot',\n",
      " 'stride': 8.0,\n",
      " 'topheight': 400,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running  DeepCut_resnet50_arena_chaseOct21shuffle1_100000  with # of trainingiterations: 100000\n",
      "INFO:tensorflow:Restoring parameters from E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-10-21\\dlc-models\\iteration-0\\arena_chaseOct21-trainset95shuffle1\\train\\snapshot-100000\n",
      "Analyzing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:03, 12.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done and results stored for snapshot:  snapshot-100000\n",
      "Results for 100000  training iterations: 95 1 train error: 1.9 pixels. Test error: 359.31  pixels.\n",
      "With pcutoff of 0.1  train error: 1.9 pixels. Test error: 189.28 pixels\n",
      "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
      "Plotting...\n",
      "The network is evaluated and the results are stored in the subdirectory 'evaluation_results'.\n",
      "If it generalizes well, choose the best model for prediction and update the config file with the appropriate index for the 'snapshotindex'.\n",
      "Use the function 'analyze_video' to make predictions on new videos.\n",
      "Otherwise consider retraining the network (see DeepLabCut workflow Fig 2)\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.evaluate_network(path_config_file, plotting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0426_0221F_4-1.mp4\n",
      "0426_0221F_C1_1.mp4\n"
     ]
    }
   ],
   "source": [
    "def listAllDataVideos(directory,skip=\"\", codec=\".avi\"):\n",
    "    vid_list=[]\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for name in files:\n",
    "            print(name)\n",
    "            if name.endswith(codec) and not re.search(skip,root):\n",
    "                vid_list.append(os.path.join(root,name))\n",
    "            else:\n",
    "                continue\n",
    "    return vid_list\n",
    "                \n",
    "vids_to_analyze = listAllDataVideos(vid_directory,\"\",\".mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y_LZiS_0oEJl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0], [1], [2]],\n",
      " 'all_joints_names': ['COM', 'cal1', 'cal2'],\n",
      " 'batch_size': 8,\n",
      " 'bottomheight': 400,\n",
      " 'crop': True,\n",
      " 'crop_pad': 0,\n",
      " 'cropratio': 0.4,\n",
      " 'dataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_arena_chaseOct21\\\\arena_chase_Andy95shuffle1.mat',\n",
      " 'dataset_type': 'default',\n",
      " 'deterministic': False,\n",
      " 'display_iters': 1000,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': 'C:\\\\Users\\\\LabAdmin\\\\.conda\\\\envs\\\\dlc-windowsGPU\\\\lib\\\\site-packages\\\\deeplabcut\\\\pose_estimation_tensorflow\\\\models\\\\pretrained\\\\resnet_v1_50.ckpt',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'leftwidth': 400,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 0.05,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'max_input_size': 1500,\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'metadataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_arena_chaseOct21\\\\Documentation_data-arena_chase_95shuffle1.pickle',\n",
      " 'min_input_size': 64,\n",
      " 'minsize': 100,\n",
      " 'mirror': False,\n",
      " 'multi_step': [[0.005, 10000],\n",
      "                [0.02, 430000],\n",
      "                [0.002, 730000],\n",
      "                [0.001, 1030000]],\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 3,\n",
      " 'num_outputs': 1,\n",
      " 'optimizer': 'sgd',\n",
      " 'pos_dist_thresh': 17,\n",
      " 'project_path': 'E:\\\\Users\\\\Phil\\\\DeepLabCut\\\\dev\\\\arena_chase-Andy-2019-10-21',\n",
      " 'regularize': False,\n",
      " 'rightwidth': 400,\n",
      " 'save_iters': 50000,\n",
      " 'scale_jitter_lo': 0.5,\n",
      " 'scale_jitter_up': 1.25,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': 'E:\\\\Users\\\\Phil\\\\DeepLabCut\\\\dev\\\\arena_chase-Andy-2019-10-21\\\\dlc-models\\\\iteration-0\\\\arena_chaseOct21-trainset95shuffle1\\\\test\\\\snapshot',\n",
      " 'stride': 8.0,\n",
      " 'topheight': 400,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using snapshot-100000 for model E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-10-21\\dlc-models\\iteration-0\\arena_chaseOct21-trainset95shuffle1\n",
      "num_outputs =  1\n",
      "INFO:tensorflow:Restoring parameters from E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-10-21\\dlc-models\\iteration-0\\arena_chaseOct21-trainset95shuffle1\\train\\snapshot-100000\n",
      "Starting to analyze %  E:\\Users\\Phil\\Jerboa\\0426_0221F_4-1.mp4\n",
      "Loading  E:\\Users\\Phil\\Jerboa\\0426_0221F_4-1.mp4\n",
      "Duration of video [s]:  57.17 , recorded with  119.88 fps!\n",
      "Overall # of frames:  6853  found with (before cropping) frame dimensions:  1280 720\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6868it [04:40, 24.08it/s]                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  6853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6868it [04:42, 24.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in E:\\Users\\Phil\\Jerboa...\n",
      "Starting to analyze %  E:\\Users\\Phil\\Jerboa\\0426_0221F_C1_1.mp4\n",
      "Loading  E:\\Users\\Phil\\Jerboa\\0426_0221F_C1_1.mp4\n",
      "Duration of video [s]:  101.54 , recorded with  119.88 fps!\n",
      "Overall # of frames:  12173  found with (before cropping) frame dimensions:  1280 720\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12221it [08:17, 24.29it/s]                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  12173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12221it [08:20, 24.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in E:\\Users\\Phil\\Jerboa...\n",
      "The videos are analyzed. Now your research can truly start! \n",
      " You can create labeled videos with 'create_labeled_video'.\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract any outlier frames!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'DeepCut_resnet50_arena_chaseOct21shuffle1_100000'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deeplabcut.analyze_videos(path_config_file,vid_list, videotype='.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting %  E:\\Users\\Phil\\Jerboa ['E:\\\\Users\\\\Phil\\\\Jerboa\\\\0426_0221F_4-1.mp4', 'E:\\\\Users\\\\Phil\\\\Jerboa\\\\0426_0221F_C1_1.mp4']\n",
      "Loading  E:\\Users\\Phil\\Jerboa\\0426_0221F_4-1.mp4 and data.\n",
      "False 0 1280 0 720\n",
      "6853\n",
      "Duration of video [s]:  57.17 , recorded with  119.88 fps!\n",
      "Overall # of frames:  6853 with cropped frame dimensions:  1280 720\n",
      "Generating frames and creating video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6853/6853 [00:38<00:00, 176.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting %  E:\\Users\\Phil\\Jerboa ['E:\\\\Users\\\\Phil\\\\Jerboa\\\\0426_0221F_4-1.mp4', 'E:\\\\Users\\\\Phil\\\\Jerboa\\\\0426_0221F_C1_1.mp4']\n",
      "Loading  E:\\Users\\Phil\\Jerboa\\0426_0221F_C1_1.mp4 and data.\n",
      "False 0 1280 0 720\n",
      "12173\n",
      "Duration of video [s]:  101.54 , recorded with  119.88 fps!\n",
      "Overall # of frames:  12173 with cropped frame dimensions:  1280 720\n",
      "Generating frames and creating video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 12173/12173 [01:10<00:00, 173.75it/s]\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.create_labeled_video(path_config_file,vid_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iGu_PdTWoEJr"
   },
   "source": [
    "## Extract outlier frames [optional step]\n",
    "\n",
    "This is an optional step and is used only when the evaluation results are poor i.e. the labels are incorrectly predicted. In such a case, the user can use the following function to extract frames where the labels are incorrectly predicted. This step has many options, so please look at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network parameters: DeepCut_resnet50_arena_chaseOct10shuffle1_100000\n",
      "Method  jump  found  2214  putative outlier frames.\n",
      "Do you want to proceed with extracting  20  of those?\n",
      "If this list is very large, perhaps consider changing the paramters (start, stop, epsilon, comparisonbodyparts) or use a different method.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "yes/no yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames from video 0426_0221F_4-1  already extracted (more will be added)!\n",
      "Loading video...\n",
      "Duration of video [s]:  57.165441666666666 , recorded @  119.88011988011988 fps!\n",
      "Overall # of frames:  6853 with (cropped) frame dimensions: \n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 57.17  seconds.\n",
      "Extracting and downsampling... 2214  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2214it [01:21, 27.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Let's select frames indices: [5410, 1017, 5208, 1220, 6758, 306, 5078, 604, 3875, 3792, 2307, 5479, 5263, 5971, 1163, 3627, 821, 1348, 2369, 4597]\n",
      "New video was added to the project! Use the function 'extract_frames' to select frames for labeling.\n",
      "The outlier frames are extracted. They are stored in the subdirectory labeled-data\\0426_0221F_4-1.\n",
      "Once you extracted frames for all videos, use 'refine_labels' to manually correct the labels.\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.extract_outlier_frames(path_config_file,[vids_to_analyze[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ib0uvhaoEJx"
   },
   "source": [
    "## Refine Labels [optional step]\n",
    "Following the extraction of outlier frames, the user can use the following function to move the predicted labels to the correct location. Thus augmenting the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n_FpEXtyoEJy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windows\n",
      "Checking labels if they are outside the image\n",
      "A training dataset file is already found for this video. The refined machine labels are merged to this data!\n",
      "Closing... The refined labels are stored in a subdirectory under labeled-data. Use the function 'merge_datasets' to augment the training dataset, and then re-train a network using create_training_dataset followed by train_network!\n"
     ]
    }
   ],
   "source": [
    "%gui wx\n",
    "deeplabcut.refine_labels(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Afterwards, if you want to look at the adjusted frames, you can load them in the main GUI by running: ``deeplabcut.label_frames(path_config_file)``\n",
    "\n",
    "(you can add a new \"cell\" below to add this code!)\n",
    "\n",
    "#### Once all folders are relabeled, check the labels again! If you are not happy, adjust them in the main GUI:\n",
    "\n",
    "``deeplabcut.label_frames(path_config_file)``\n",
    "\n",
    "Check Labels:\n",
    "\n",
    "``deeplabcut.check_labels(path_config_file)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CHzstWr8oEJ2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data sets and updated refinement iteration to 1.\n",
      "Now you can create a new training set for the expanded annotated images (use create_training_dataset).\n"
     ]
    }
   ],
   "source": [
    "#NOW, merge this with your original data:\n",
    "\n",
    "deeplabcut.merge_datasets(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QCHj7qyboEJ6"
   },
   "source": [
    "## Create a new iteration of training dataset [optional step]\n",
    "Following the refinement of labels and appending them to the original dataset, this creates a new iteration of training dataset. This is automatically set in the config.yaml file, so let's get training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ytQoxIldoEJ7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.create_training_dataset(path_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0], [1], [2]],\n",
      " 'all_joints_names': ['COM', 'wand_1', 'wand_2'],\n",
      " 'batch_size': 8,\n",
      " 'bottomheight': 400,\n",
      " 'crop': True,\n",
      " 'crop_pad': 0,\n",
      " 'cropratio': 0.4,\n",
      " 'dataset': 'training-datasets\\\\iteration-1\\\\UnaugmentedDataSet_arena_chaseOct10\\\\arena_chase_Andy95shuffle1.mat',\n",
      " 'dataset_type': 'default',\n",
      " 'deterministic': False,\n",
      " 'display_iters': 1000,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': 'C:\\\\Users\\\\LabAdmin\\\\.conda\\\\envs\\\\dlc-windowsGPU\\\\lib\\\\site-packages\\\\deeplabcut\\\\pose_estimation_tensorflow\\\\models\\\\pretrained\\\\resnet_v1_50.ckpt',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'leftwidth': 400,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 0.05,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'max_input_size': 1500,\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'metadataset': 'training-datasets\\\\iteration-1\\\\UnaugmentedDataSet_arena_chaseOct10\\\\Documentation_data-arena_chase_95shuffle1.pickle',\n",
      " 'min_input_size': 64,\n",
      " 'minsize': 100,\n",
      " 'mirror': False,\n",
      " 'multi_step': [[0.005, 10000],\n",
      "                [0.02, 430000],\n",
      "                [0.002, 730000],\n",
      "                [0.001, 1030000]],\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 3,\n",
      " 'num_outputs': 1,\n",
      " 'optimizer': 'sgd',\n",
      " 'pos_dist_thresh': 17,\n",
      " 'project_path': 'E:\\\\Users\\\\Phil\\\\DeepLabCut\\\\dev\\\\arena_chase-Andy-2019-10-10',\n",
      " 'regularize': False,\n",
      " 'rightwidth': 400,\n",
      " 'save_iters': 50000,\n",
      " 'scale_jitter_lo': 0.5,\n",
      " 'scale_jitter_up': 1.25,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': 'E:\\\\Users\\\\Phil\\\\DeepLabCut\\\\dev\\\\arena_chase-Andy-2019-10-10\\\\dlc-models\\\\iteration-1\\\\arena_chaseOct10-trainset95shuffle1\\\\train\\\\snapshot',\n",
      " 'stride': 8.0,\n",
      " 'topheight': 400,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with standard pose-dataset loader.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt\n",
      "Max_iters overwritten as 100000\n",
      "Display_iters overwritten as 50\n",
      "Training parameter:\n",
      "{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'weigh_only_present_joints': False, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': 'E:\\\\Users\\\\Phil\\\\DeepLabCut\\\\dev\\\\arena_chase-Andy-2019-10-10\\\\dlc-models\\\\iteration-1\\\\arena_chaseOct10-trainset95shuffle1\\\\train\\\\snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'mirror': False, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 1, 'dataset_type': 'default', 'deterministic': False, 'crop': True, 'cropratio': 0.4, 'minsize': 100, 'leftwidth': 400, 'rightwidth': 400, 'topheight': 400, 'bottomheight': 400, 'all_joints': [[0], [1], [2]], 'all_joints_names': ['COM', 'wand_1', 'wand_2'], 'dataset': 'training-datasets\\\\iteration-1\\\\UnaugmentedDataSet_arena_chaseOct10\\\\arena_chase_Andy95shuffle1.mat', 'display_iters': 1000, 'init_weights': 'C:\\\\Users\\\\LabAdmin\\\\.conda\\\\envs\\\\dlc-windowsGPU\\\\lib\\\\site-packages\\\\deeplabcut\\\\pose_estimation_tensorflow\\\\models\\\\pretrained\\\\resnet_v1_50.ckpt', 'max_input_size': 1500, 'metadataset': 'training-datasets\\\\iteration-1\\\\UnaugmentedDataSet_arena_chaseOct10\\\\Documentation_data-arena_chase_95shuffle1.pickle', 'min_input_size': 64, 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'net_type': 'resnet_50', 'num_joints': 3, 'pos_dist_thresh': 17, 'project_path': 'E:\\\\Users\\\\Phil\\\\DeepLabCut\\\\dev\\\\arena_chase-Andy-2019-10-10', 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'num_outputs': 1}\n",
      "Starting training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 50 loss: 0.0881 lr: 0.005\n",
      "iteration: 100 loss: 0.0283 lr: 0.005\n",
      "iteration: 150 loss: 0.0235 lr: 0.005\n",
      "iteration: 200 loss: 0.0188 lr: 0.005\n",
      "iteration: 250 loss: 0.0195 lr: 0.005\n",
      "iteration: 300 loss: 0.0193 lr: 0.005\n",
      "iteration: 350 loss: 0.0166 lr: 0.005\n",
      "iteration: 400 loss: 0.0172 lr: 0.005\n",
      "iteration: 450 loss: 0.0156 lr: 0.005\n",
      "iteration: 500 loss: 0.0174 lr: 0.005\n",
      "iteration: 550 loss: 0.0158 lr: 0.005\n",
      "iteration: 600 loss: 0.0136 lr: 0.005\n",
      "iteration: 650 loss: 0.0146 lr: 0.005\n",
      "iteration: 700 loss: 0.0126 lr: 0.005\n",
      "iteration: 750 loss: 0.0128 lr: 0.005\n",
      "iteration: 800 loss: 0.0115 lr: 0.005\n",
      "iteration: 850 loss: 0.0128 lr: 0.005\n",
      "iteration: 900 loss: 0.0109 lr: 0.005\n",
      "iteration: 950 loss: 0.0111 lr: 0.005\n",
      "iteration: 1000 loss: 0.0104 lr: 0.005\n",
      "iteration: 1050 loss: 0.0095 lr: 0.005\n",
      "iteration: 1100 loss: 0.0115 lr: 0.005\n",
      "iteration: 1150 loss: 0.0097 lr: 0.005\n",
      "iteration: 1200 loss: 0.0090 lr: 0.005\n",
      "iteration: 1250 loss: 0.0092 lr: 0.005\n",
      "iteration: 1300 loss: 0.0094 lr: 0.005\n",
      "iteration: 1350 loss: 0.0083 lr: 0.005\n",
      "iteration: 1400 loss: 0.0089 lr: 0.005\n",
      "iteration: 1450 loss: 0.0084 lr: 0.005\n",
      "iteration: 1500 loss: 0.0084 lr: 0.005\n",
      "iteration: 1550 loss: 0.0081 lr: 0.005\n",
      "iteration: 1600 loss: 0.0085 lr: 0.005\n",
      "iteration: 1650 loss: 0.0079 lr: 0.005\n",
      "iteration: 1700 loss: 0.0083 lr: 0.005\n",
      "iteration: 1750 loss: 0.0079 lr: 0.005\n",
      "iteration: 1800 loss: 0.0087 lr: 0.005\n",
      "iteration: 1850 loss: 0.0088 lr: 0.005\n",
      "iteration: 1900 loss: 0.0084 lr: 0.005\n",
      "iteration: 1950 loss: 0.0086 lr: 0.005\n",
      "iteration: 2000 loss: 0.0070 lr: 0.005\n",
      "iteration: 2050 loss: 0.0072 lr: 0.005\n",
      "iteration: 2100 loss: 0.0081 lr: 0.005\n",
      "iteration: 2150 loss: 0.0071 lr: 0.005\n",
      "iteration: 2200 loss: 0.0082 lr: 0.005\n",
      "iteration: 2250 loss: 0.0072 lr: 0.005\n",
      "iteration: 2300 loss: 0.0074 lr: 0.005\n",
      "iteration: 2350 loss: 0.0063 lr: 0.005\n",
      "iteration: 2400 loss: 0.0055 lr: 0.005\n",
      "iteration: 2450 loss: 0.0073 lr: 0.005\n",
      "iteration: 2500 loss: 0.0060 lr: 0.005\n",
      "iteration: 2550 loss: 0.0059 lr: 0.005\n",
      "iteration: 2600 loss: 0.0059 lr: 0.005\n",
      "iteration: 2650 loss: 0.0064 lr: 0.005\n",
      "iteration: 2700 loss: 0.0065 lr: 0.005\n",
      "iteration: 2750 loss: 0.0080 lr: 0.005\n",
      "iteration: 2800 loss: 0.0074 lr: 0.005\n",
      "iteration: 2850 loss: 0.0073 lr: 0.005\n",
      "iteration: 2900 loss: 0.0065 lr: 0.005\n",
      "iteration: 2950 loss: 0.0066 lr: 0.005\n",
      "iteration: 3000 loss: 0.0055 lr: 0.005\n",
      "iteration: 3050 loss: 0.0062 lr: 0.005\n",
      "iteration: 3100 loss: 0.0064 lr: 0.005\n",
      "iteration: 3150 loss: 0.0060 lr: 0.005\n",
      "iteration: 3200 loss: 0.0048 lr: 0.005\n",
      "iteration: 3250 loss: 0.0054 lr: 0.005\n",
      "iteration: 3300 loss: 0.0051 lr: 0.005\n",
      "iteration: 3350 loss: 0.0069 lr: 0.005\n",
      "iteration: 3400 loss: 0.0059 lr: 0.005\n",
      "iteration: 3450 loss: 0.0052 lr: 0.005\n",
      "iteration: 3500 loss: 0.0061 lr: 0.005\n",
      "iteration: 3550 loss: 0.0051 lr: 0.005\n",
      "iteration: 3600 loss: 0.0056 lr: 0.005\n",
      "iteration: 3650 loss: 0.0062 lr: 0.005\n",
      "iteration: 3700 loss: 0.0060 lr: 0.005\n",
      "iteration: 3750 loss: 0.0049 lr: 0.005\n",
      "iteration: 3800 loss: 0.0051 lr: 0.005\n",
      "iteration: 3850 loss: 0.0057 lr: 0.005\n",
      "iteration: 3900 loss: 0.0051 lr: 0.005\n",
      "iteration: 3950 loss: 0.0052 lr: 0.005\n",
      "iteration: 4000 loss: 0.0057 lr: 0.005\n",
      "iteration: 4050 loss: 0.0053 lr: 0.005\n",
      "iteration: 4100 loss: 0.0057 lr: 0.005\n",
      "iteration: 4150 loss: 0.0055 lr: 0.005\n",
      "iteration: 4200 loss: 0.0049 lr: 0.005\n",
      "iteration: 4250 loss: 0.0055 lr: 0.005\n",
      "iteration: 4300 loss: 0.0047 lr: 0.005\n",
      "iteration: 4350 loss: 0.0056 lr: 0.005\n",
      "iteration: 4400 loss: 0.0051 lr: 0.005\n",
      "iteration: 4450 loss: 0.0043 lr: 0.005\n",
      "iteration: 4500 loss: 0.0048 lr: 0.005\n",
      "iteration: 4550 loss: 0.0043 lr: 0.005\n",
      "iteration: 4600 loss: 0.0059 lr: 0.005\n",
      "iteration: 4650 loss: 0.0048 lr: 0.005\n",
      "iteration: 4700 loss: 0.0061 lr: 0.005\n",
      "iteration: 4750 loss: 0.0049 lr: 0.005\n",
      "iteration: 4800 loss: 0.0045 lr: 0.005\n",
      "iteration: 4850 loss: 0.0055 lr: 0.005\n",
      "iteration: 4900 loss: 0.0050 lr: 0.005\n",
      "iteration: 4950 loss: 0.0049 lr: 0.005\n",
      "iteration: 5000 loss: 0.0054 lr: 0.005\n",
      "iteration: 5050 loss: 0.0047 lr: 0.005\n",
      "iteration: 5100 loss: 0.0047 lr: 0.005\n",
      "iteration: 5150 loss: 0.0048 lr: 0.005\n",
      "iteration: 5200 loss: 0.0050 lr: 0.005\n",
      "iteration: 5250 loss: 0.0045 lr: 0.005\n",
      "iteration: 5300 loss: 0.0055 lr: 0.005\n",
      "iteration: 5350 loss: 0.0046 lr: 0.005\n",
      "iteration: 5400 loss: 0.0046 lr: 0.005\n",
      "iteration: 5450 loss: 0.0047 lr: 0.005\n",
      "iteration: 5500 loss: 0.0045 lr: 0.005\n",
      "iteration: 5550 loss: 0.0043 lr: 0.005\n",
      "iteration: 5600 loss: 0.0047 lr: 0.005\n",
      "iteration: 5650 loss: 0.0041 lr: 0.005\n",
      "iteration: 5700 loss: 0.0053 lr: 0.005\n",
      "iteration: 5750 loss: 0.0044 lr: 0.005\n",
      "iteration: 5800 loss: 0.0041 lr: 0.005\n",
      "iteration: 5850 loss: 0.0046 lr: 0.005\n",
      "iteration: 5900 loss: 0.0046 lr: 0.005\n",
      "iteration: 5950 loss: 0.0051 lr: 0.005\n",
      "iteration: 6000 loss: 0.0040 lr: 0.005\n",
      "iteration: 6050 loss: 0.0051 lr: 0.005\n",
      "iteration: 6100 loss: 0.0051 lr: 0.005\n",
      "iteration: 6150 loss: 0.0040 lr: 0.005\n",
      "iteration: 6200 loss: 0.0044 lr: 0.005\n",
      "iteration: 6250 loss: 0.0065 lr: 0.005\n",
      "iteration: 6300 loss: 0.0062 lr: 0.005\n",
      "iteration: 6350 loss: 0.0046 lr: 0.005\n",
      "iteration: 6400 loss: 0.0044 lr: 0.005\n",
      "iteration: 6450 loss: 0.0040 lr: 0.005\n",
      "iteration: 6500 loss: 0.0043 lr: 0.005\n",
      "iteration: 6550 loss: 0.0061 lr: 0.005\n",
      "iteration: 6600 loss: 0.0047 lr: 0.005\n",
      "iteration: 6650 loss: 0.0046 lr: 0.005\n",
      "iteration: 6700 loss: 0.0044 lr: 0.005\n",
      "iteration: 6750 loss: 0.0044 lr: 0.005\n",
      "iteration: 6800 loss: 0.0041 lr: 0.005\n",
      "iteration: 6850 loss: 0.0048 lr: 0.005\n",
      "iteration: 6900 loss: 0.0039 lr: 0.005\n",
      "iteration: 6950 loss: 0.0040 lr: 0.005\n",
      "iteration: 7000 loss: 0.0042 lr: 0.005\n",
      "iteration: 7050 loss: 0.0035 lr: 0.005\n",
      "iteration: 7100 loss: 0.0041 lr: 0.005\n",
      "iteration: 7150 loss: 0.0044 lr: 0.005\n",
      "iteration: 7200 loss: 0.0045 lr: 0.005\n",
      "iteration: 7250 loss: 0.0032 lr: 0.005\n",
      "iteration: 7300 loss: 0.0042 lr: 0.005\n",
      "iteration: 7350 loss: 0.0032 lr: 0.005\n",
      "iteration: 7400 loss: 0.0038 lr: 0.005\n",
      "iteration: 7450 loss: 0.0042 lr: 0.005\n",
      "iteration: 7500 loss: 0.0036 lr: 0.005\n",
      "iteration: 7550 loss: 0.0047 lr: 0.005\n",
      "iteration: 7600 loss: 0.0033 lr: 0.005\n",
      "iteration: 7650 loss: 0.0046 lr: 0.005\n",
      "iteration: 7700 loss: 0.0045 lr: 0.005\n",
      "iteration: 7750 loss: 0.0038 lr: 0.005\n",
      "iteration: 7800 loss: 0.0043 lr: 0.005\n",
      "iteration: 7850 loss: 0.0035 lr: 0.005\n",
      "iteration: 7900 loss: 0.0036 lr: 0.005\n",
      "iteration: 7950 loss: 0.0036 lr: 0.005\n",
      "iteration: 8000 loss: 0.0042 lr: 0.005\n",
      "iteration: 8050 loss: 0.0043 lr: 0.005\n",
      "iteration: 8100 loss: 0.0035 lr: 0.005\n",
      "iteration: 8150 loss: 0.0033 lr: 0.005\n",
      "iteration: 8200 loss: 0.0039 lr: 0.005\n",
      "iteration: 8250 loss: 0.0043 lr: 0.005\n",
      "iteration: 8300 loss: 0.0042 lr: 0.005\n",
      "iteration: 8350 loss: 0.0035 lr: 0.005\n",
      "iteration: 8400 loss: 0.0037 lr: 0.005\n",
      "iteration: 8450 loss: 0.0040 lr: 0.005\n",
      "iteration: 8500 loss: 0.0043 lr: 0.005\n",
      "iteration: 8550 loss: 0.0036 lr: 0.005\n",
      "iteration: 8600 loss: 0.0047 lr: 0.005\n",
      "iteration: 8650 loss: 0.0029 lr: 0.005\n",
      "iteration: 8700 loss: 0.0032 lr: 0.005\n",
      "iteration: 8750 loss: 0.0035 lr: 0.005\n",
      "iteration: 8800 loss: 0.0031 lr: 0.005\n",
      "iteration: 8850 loss: 0.0043 lr: 0.005\n",
      "iteration: 8900 loss: 0.0038 lr: 0.005\n",
      "iteration: 8950 loss: 0.0030 lr: 0.005\n",
      "iteration: 9000 loss: 0.0042 lr: 0.005\n",
      "iteration: 9050 loss: 0.0036 lr: 0.005\n",
      "iteration: 9100 loss: 0.0036 lr: 0.005\n",
      "iteration: 9150 loss: 0.0034 lr: 0.005\n",
      "iteration: 9200 loss: 0.0033 lr: 0.005\n",
      "iteration: 9250 loss: 0.0039 lr: 0.005\n",
      "iteration: 9300 loss: 0.0035 lr: 0.005\n",
      "iteration: 9350 loss: 0.0039 lr: 0.005\n",
      "iteration: 9400 loss: 0.0039 lr: 0.005\n",
      "iteration: 9450 loss: 0.0038 lr: 0.005\n",
      "iteration: 9500 loss: 0.0029 lr: 0.005\n",
      "iteration: 9550 loss: 0.0034 lr: 0.005\n",
      "iteration: 9600 loss: 0.0041 lr: 0.005\n",
      "iteration: 9650 loss: 0.0034 lr: 0.005\n",
      "iteration: 9700 loss: 0.0038 lr: 0.005\n",
      "iteration: 9750 loss: 0.0033 lr: 0.005\n",
      "iteration: 9800 loss: 0.0043 lr: 0.005\n",
      "iteration: 9850 loss: 0.0038 lr: 0.005\n",
      "iteration: 9900 loss: 0.0041 lr: 0.005\n",
      "iteration: 9950 loss: 0.0049 lr: 0.005\n",
      "iteration: 10000 loss: 0.0030 lr: 0.005\n",
      "iteration: 10050 loss: 0.0080 lr: 0.02\n",
      "iteration: 10100 loss: 0.0078 lr: 0.02\n",
      "iteration: 10150 loss: 0.0079 lr: 0.02\n",
      "iteration: 10200 loss: 0.0070 lr: 0.02\n",
      "iteration: 10250 loss: 0.0062 lr: 0.02\n",
      "iteration: 10300 loss: 0.0064 lr: 0.02\n",
      "iteration: 10350 loss: 0.0082 lr: 0.02\n",
      "iteration: 10400 loss: 0.0076 lr: 0.02\n",
      "iteration: 10450 loss: 0.0074 lr: 0.02\n",
      "iteration: 10500 loss: 0.0063 lr: 0.02\n",
      "iteration: 10550 loss: 0.0060 lr: 0.02\n",
      "iteration: 10600 loss: 0.0059 lr: 0.02\n",
      "iteration: 10650 loss: 0.0063 lr: 0.02\n",
      "iteration: 10700 loss: 0.0055 lr: 0.02\n",
      "iteration: 10750 loss: 0.0054 lr: 0.02\n",
      "iteration: 10800 loss: 0.0053 lr: 0.02\n",
      "iteration: 10850 loss: 0.0051 lr: 0.02\n",
      "iteration: 10900 loss: 0.0053 lr: 0.02\n",
      "iteration: 10950 loss: 0.0049 lr: 0.02\n",
      "iteration: 11000 loss: 0.0048 lr: 0.02\n",
      "iteration: 11050 loss: 0.0055 lr: 0.02\n",
      "iteration: 11100 loss: 0.0057 lr: 0.02\n",
      "iteration: 11150 loss: 0.0063 lr: 0.02\n",
      "iteration: 11200 loss: 0.0057 lr: 0.02\n",
      "iteration: 11250 loss: 0.0046 lr: 0.02\n",
      "iteration: 11300 loss: 0.0046 lr: 0.02\n",
      "iteration: 11350 loss: 0.0055 lr: 0.02\n",
      "iteration: 11400 loss: 0.0049 lr: 0.02\n",
      "iteration: 11450 loss: 0.0048 lr: 0.02\n",
      "iteration: 11500 loss: 0.0044 lr: 0.02\n",
      "iteration: 11550 loss: 0.0056 lr: 0.02\n",
      "iteration: 11600 loss: 0.0043 lr: 0.02\n",
      "iteration: 11650 loss: 0.0038 lr: 0.02\n",
      "iteration: 11700 loss: 0.0039 lr: 0.02\n",
      "iteration: 11750 loss: 0.0052 lr: 0.02\n",
      "iteration: 11800 loss: 0.0042 lr: 0.02\n",
      "iteration: 11850 loss: 0.0047 lr: 0.02\n",
      "iteration: 11900 loss: 0.0055 lr: 0.02\n",
      "iteration: 11950 loss: 0.0045 lr: 0.02\n",
      "iteration: 12000 loss: 0.0058 lr: 0.02\n",
      "iteration: 12050 loss: 0.0043 lr: 0.02\n",
      "iteration: 12100 loss: 0.0047 lr: 0.02\n",
      "iteration: 12150 loss: 0.0044 lr: 0.02\n",
      "iteration: 12200 loss: 0.0038 lr: 0.02\n",
      "iteration: 12250 loss: 0.0033 lr: 0.02\n",
      "iteration: 12300 loss: 0.0042 lr: 0.02\n",
      "iteration: 12350 loss: 0.0045 lr: 0.02\n",
      "iteration: 12400 loss: 0.0038 lr: 0.02\n",
      "iteration: 12450 loss: 0.0038 lr: 0.02\n",
      "iteration: 12500 loss: 0.0043 lr: 0.02\n",
      "iteration: 12550 loss: 0.0037 lr: 0.02\n",
      "iteration: 12600 loss: 0.0040 lr: 0.02\n",
      "iteration: 12650 loss: 0.0047 lr: 0.02\n",
      "iteration: 12700 loss: 0.0037 lr: 0.02\n",
      "iteration: 12750 loss: 0.0042 lr: 0.02\n",
      "iteration: 12800 loss: 0.0040 lr: 0.02\n",
      "iteration: 12850 loss: 0.0040 lr: 0.02\n",
      "iteration: 12900 loss: 0.0036 lr: 0.02\n",
      "iteration: 12950 loss: 0.0034 lr: 0.02\n",
      "iteration: 13000 loss: 0.0037 lr: 0.02\n",
      "iteration: 13050 loss: 0.0036 lr: 0.02\n",
      "iteration: 13100 loss: 0.0038 lr: 0.02\n",
      "iteration: 13150 loss: 0.0039 lr: 0.02\n",
      "iteration: 13200 loss: 0.0048 lr: 0.02\n",
      "iteration: 13250 loss: 0.0038 lr: 0.02\n",
      "iteration: 13300 loss: 0.0036 lr: 0.02\n",
      "iteration: 13350 loss: 0.0037 lr: 0.02\n",
      "iteration: 13400 loss: 0.0035 lr: 0.02\n",
      "iteration: 13450 loss: 0.0043 lr: 0.02\n",
      "iteration: 13500 loss: 0.0034 lr: 0.02\n",
      "iteration: 13550 loss: 0.0035 lr: 0.02\n",
      "iteration: 13600 loss: 0.0033 lr: 0.02\n",
      "iteration: 13650 loss: 0.0030 lr: 0.02\n",
      "iteration: 13700 loss: 0.0032 lr: 0.02\n",
      "iteration: 13750 loss: 0.0029 lr: 0.02\n",
      "iteration: 13800 loss: 0.0032 lr: 0.02\n",
      "iteration: 13850 loss: 0.0038 lr: 0.02\n",
      "iteration: 13900 loss: 0.0037 lr: 0.02\n",
      "iteration: 13950 loss: 0.0037 lr: 0.02\n",
      "iteration: 14000 loss: 0.0042 lr: 0.02\n",
      "iteration: 14050 loss: 0.0035 lr: 0.02\n",
      "iteration: 14100 loss: 0.0036 lr: 0.02\n",
      "iteration: 14150 loss: 0.0043 lr: 0.02\n",
      "iteration: 14200 loss: 0.0035 lr: 0.02\n",
      "iteration: 14250 loss: 0.0029 lr: 0.02\n",
      "iteration: 14300 loss: 0.0032 lr: 0.02\n",
      "iteration: 14350 loss: 0.0030 lr: 0.02\n",
      "iteration: 14400 loss: 0.0028 lr: 0.02\n",
      "iteration: 14450 loss: 0.0039 lr: 0.02\n",
      "iteration: 14500 loss: 0.0037 lr: 0.02\n",
      "iteration: 14550 loss: 0.0034 lr: 0.02\n",
      "iteration: 14600 loss: 0.0032 lr: 0.02\n",
      "iteration: 14650 loss: 0.0033 lr: 0.02\n",
      "iteration: 14700 loss: 0.0032 lr: 0.02\n",
      "iteration: 14750 loss: 0.0035 lr: 0.02\n",
      "iteration: 14800 loss: 0.0032 lr: 0.02\n",
      "iteration: 14850 loss: 0.0034 lr: 0.02\n",
      "iteration: 14900 loss: 0.0030 lr: 0.02\n",
      "iteration: 14950 loss: 0.0035 lr: 0.02\n",
      "iteration: 15000 loss: 0.0035 lr: 0.02\n",
      "iteration: 15050 loss: 0.0031 lr: 0.02\n",
      "iteration: 15100 loss: 0.0032 lr: 0.02\n",
      "iteration: 15150 loss: 0.0032 lr: 0.02\n",
      "iteration: 15200 loss: 0.0029 lr: 0.02\n",
      "iteration: 15250 loss: 0.0036 lr: 0.02\n",
      "iteration: 15300 loss: 0.0032 lr: 0.02\n",
      "iteration: 15350 loss: 0.0029 lr: 0.02\n",
      "iteration: 15400 loss: 0.0034 lr: 0.02\n",
      "iteration: 15450 loss: 0.0032 lr: 0.02\n",
      "iteration: 15500 loss: 0.0030 lr: 0.02\n",
      "iteration: 15550 loss: 0.0031 lr: 0.02\n",
      "iteration: 15600 loss: 0.0037 lr: 0.02\n",
      "iteration: 15650 loss: 0.0034 lr: 0.02\n",
      "iteration: 15700 loss: 0.0031 lr: 0.02\n",
      "iteration: 15750 loss: 0.0031 lr: 0.02\n",
      "iteration: 15800 loss: 0.0033 lr: 0.02\n",
      "iteration: 15850 loss: 0.0034 lr: 0.02\n",
      "iteration: 15900 loss: 0.0029 lr: 0.02\n",
      "iteration: 15950 loss: 0.0034 lr: 0.02\n",
      "iteration: 16000 loss: 0.0029 lr: 0.02\n",
      "iteration: 16050 loss: 0.0028 lr: 0.02\n",
      "iteration: 16100 loss: 0.0031 lr: 0.02\n",
      "iteration: 16150 loss: 0.0026 lr: 0.02\n",
      "iteration: 16200 loss: 0.0030 lr: 0.02\n",
      "iteration: 16250 loss: 0.0031 lr: 0.02\n",
      "iteration: 16300 loss: 0.0038 lr: 0.02\n",
      "iteration: 16350 loss: 0.0034 lr: 0.02\n",
      "iteration: 16400 loss: 0.0028 lr: 0.02\n",
      "iteration: 16450 loss: 0.0028 lr: 0.02\n",
      "iteration: 16500 loss: 0.0032 lr: 0.02\n",
      "iteration: 16550 loss: 0.0030 lr: 0.02\n",
      "iteration: 16600 loss: 0.0028 lr: 0.02\n",
      "iteration: 16650 loss: 0.0030 lr: 0.02\n",
      "iteration: 16700 loss: 0.0035 lr: 0.02\n",
      "iteration: 16750 loss: 0.0030 lr: 0.02\n",
      "iteration: 16800 loss: 0.0027 lr: 0.02\n",
      "iteration: 16850 loss: 0.0034 lr: 0.02\n",
      "iteration: 16900 loss: 0.0028 lr: 0.02\n",
      "iteration: 16950 loss: 0.0028 lr: 0.02\n",
      "iteration: 17000 loss: 0.0031 lr: 0.02\n",
      "iteration: 17050 loss: 0.0031 lr: 0.02\n",
      "iteration: 17100 loss: 0.0031 lr: 0.02\n",
      "iteration: 17150 loss: 0.0027 lr: 0.02\n",
      "iteration: 17200 loss: 0.0029 lr: 0.02\n",
      "iteration: 17250 loss: 0.0027 lr: 0.02\n",
      "iteration: 17300 loss: 0.0022 lr: 0.02\n",
      "iteration: 17350 loss: 0.0029 lr: 0.02\n",
      "iteration: 17400 loss: 0.0023 lr: 0.02\n",
      "iteration: 17450 loss: 0.0026 lr: 0.02\n",
      "iteration: 17500 loss: 0.0031 lr: 0.02\n",
      "iteration: 17550 loss: 0.0025 lr: 0.02\n",
      "iteration: 17600 loss: 0.0027 lr: 0.02\n",
      "iteration: 17650 loss: 0.0032 lr: 0.02\n",
      "iteration: 17700 loss: 0.0027 lr: 0.02\n",
      "iteration: 17750 loss: 0.0034 lr: 0.02\n",
      "iteration: 17800 loss: 0.0029 lr: 0.02\n",
      "iteration: 17850 loss: 0.0029 lr: 0.02\n",
      "iteration: 17900 loss: 0.0023 lr: 0.02\n",
      "iteration: 17950 loss: 0.0041 lr: 0.02\n",
      "iteration: 18000 loss: 0.0028 lr: 0.02\n",
      "iteration: 18050 loss: 0.0031 lr: 0.02\n",
      "iteration: 18100 loss: 0.0031 lr: 0.02\n",
      "iteration: 18150 loss: 0.0027 lr: 0.02\n",
      "iteration: 18200 loss: 0.0029 lr: 0.02\n",
      "iteration: 18250 loss: 0.0027 lr: 0.02\n",
      "iteration: 18300 loss: 0.0026 lr: 0.02\n",
      "iteration: 18350 loss: 0.0033 lr: 0.02\n",
      "iteration: 18400 loss: 0.0029 lr: 0.02\n",
      "iteration: 18450 loss: 0.0025 lr: 0.02\n",
      "iteration: 18500 loss: 0.0032 lr: 0.02\n",
      "iteration: 18550 loss: 0.0026 lr: 0.02\n",
      "iteration: 18600 loss: 0.0024 lr: 0.02\n",
      "iteration: 18650 loss: 0.0024 lr: 0.02\n",
      "iteration: 18700 loss: 0.0024 lr: 0.02\n",
      "iteration: 18750 loss: 0.0024 lr: 0.02\n",
      "iteration: 18800 loss: 0.0027 lr: 0.02\n",
      "iteration: 18850 loss: 0.0030 lr: 0.02\n",
      "iteration: 18900 loss: 0.0025 lr: 0.02\n",
      "iteration: 18950 loss: 0.0025 lr: 0.02\n",
      "iteration: 19000 loss: 0.0029 lr: 0.02\n",
      "iteration: 19050 loss: 0.0026 lr: 0.02\n",
      "iteration: 19100 loss: 0.0024 lr: 0.02\n",
      "iteration: 19150 loss: 0.0028 lr: 0.02\n",
      "iteration: 19200 loss: 0.0026 lr: 0.02\n",
      "iteration: 19250 loss: 0.0026 lr: 0.02\n",
      "iteration: 19300 loss: 0.0025 lr: 0.02\n",
      "iteration: 19350 loss: 0.0028 lr: 0.02\n",
      "iteration: 19400 loss: 0.0026 lr: 0.02\n",
      "iteration: 19450 loss: 0.0022 lr: 0.02\n",
      "iteration: 19500 loss: 0.0026 lr: 0.02\n",
      "iteration: 19550 loss: 0.0026 lr: 0.02\n",
      "iteration: 19600 loss: 0.0023 lr: 0.02\n",
      "iteration: 19650 loss: 0.0028 lr: 0.02\n",
      "iteration: 19700 loss: 0.0026 lr: 0.02\n",
      "iteration: 19750 loss: 0.0027 lr: 0.02\n",
      "iteration: 19800 loss: 0.0025 lr: 0.02\n",
      "iteration: 19850 loss: 0.0022 lr: 0.02\n",
      "iteration: 19900 loss: 0.0024 lr: 0.02\n",
      "iteration: 19950 loss: 0.0023 lr: 0.02\n",
      "iteration: 20000 loss: 0.0029 lr: 0.02\n",
      "iteration: 20050 loss: 0.0026 lr: 0.02\n",
      "iteration: 20100 loss: 0.0032 lr: 0.02\n",
      "iteration: 20150 loss: 0.0028 lr: 0.02\n",
      "iteration: 20200 loss: 0.0030 lr: 0.02\n",
      "iteration: 20250 loss: 0.0031 lr: 0.02\n",
      "iteration: 20300 loss: 0.0026 lr: 0.02\n",
      "iteration: 20350 loss: 0.0023 lr: 0.02\n",
      "iteration: 20400 loss: 0.0022 lr: 0.02\n",
      "iteration: 20450 loss: 0.0028 lr: 0.02\n",
      "iteration: 20500 loss: 0.0020 lr: 0.02\n",
      "iteration: 20550 loss: 0.0026 lr: 0.02\n",
      "iteration: 20600 loss: 0.0025 lr: 0.02\n",
      "iteration: 20650 loss: 0.0032 lr: 0.02\n",
      "iteration: 20700 loss: 0.0025 lr: 0.02\n",
      "iteration: 20750 loss: 0.0023 lr: 0.02\n",
      "iteration: 20800 loss: 0.0021 lr: 0.02\n",
      "iteration: 20850 loss: 0.0029 lr: 0.02\n",
      "iteration: 20900 loss: 0.0027 lr: 0.02\n",
      "iteration: 20950 loss: 0.0026 lr: 0.02\n",
      "iteration: 21000 loss: 0.0023 lr: 0.02\n",
      "iteration: 21050 loss: 0.0026 lr: 0.02\n",
      "iteration: 21100 loss: 0.0024 lr: 0.02\n",
      "iteration: 21150 loss: 0.0020 lr: 0.02\n",
      "iteration: 21200 loss: 0.0026 lr: 0.02\n",
      "iteration: 21250 loss: 0.0022 lr: 0.02\n",
      "iteration: 21300 loss: 0.0024 lr: 0.02\n",
      "iteration: 21350 loss: 0.0029 lr: 0.02\n",
      "iteration: 21400 loss: 0.0028 lr: 0.02\n",
      "iteration: 21450 loss: 0.0023 lr: 0.02\n",
      "iteration: 21500 loss: 0.0025 lr: 0.02\n",
      "iteration: 21550 loss: 0.0022 lr: 0.02\n",
      "iteration: 21600 loss: 0.0024 lr: 0.02\n",
      "iteration: 21650 loss: 0.0022 lr: 0.02\n",
      "iteration: 21700 loss: 0.0029 lr: 0.02\n",
      "iteration: 21750 loss: 0.0022 lr: 0.02\n",
      "iteration: 21800 loss: 0.0027 lr: 0.02\n",
      "iteration: 21850 loss: 0.0025 lr: 0.02\n",
      "iteration: 21900 loss: 0.0025 lr: 0.02\n",
      "iteration: 21950 loss: 0.0021 lr: 0.02\n",
      "iteration: 22000 loss: 0.0025 lr: 0.02\n",
      "iteration: 22050 loss: 0.0022 lr: 0.02\n",
      "iteration: 22100 loss: 0.0025 lr: 0.02\n",
      "iteration: 22150 loss: 0.0022 lr: 0.02\n",
      "iteration: 22200 loss: 0.0026 lr: 0.02\n",
      "iteration: 22250 loss: 0.0026 lr: 0.02\n",
      "iteration: 22300 loss: 0.0020 lr: 0.02\n",
      "iteration: 22350 loss: 0.0021 lr: 0.02\n",
      "iteration: 22400 loss: 0.0022 lr: 0.02\n",
      "iteration: 22450 loss: 0.0028 lr: 0.02\n",
      "iteration: 22500 loss: 0.0024 lr: 0.02\n",
      "iteration: 22550 loss: 0.0023 lr: 0.02\n",
      "iteration: 22600 loss: 0.0024 lr: 0.02\n",
      "iteration: 22650 loss: 0.0024 lr: 0.02\n",
      "iteration: 22700 loss: 0.0024 lr: 0.02\n",
      "iteration: 22750 loss: 0.0032 lr: 0.02\n",
      "iteration: 22800 loss: 0.0026 lr: 0.02\n",
      "iteration: 22850 loss: 0.0021 lr: 0.02\n",
      "iteration: 22900 loss: 0.0027 lr: 0.02\n",
      "iteration: 22950 loss: 0.0027 lr: 0.02\n",
      "iteration: 23000 loss: 0.0023 lr: 0.02\n",
      "iteration: 23050 loss: 0.0025 lr: 0.02\n",
      "iteration: 23100 loss: 0.0024 lr: 0.02\n",
      "iteration: 23150 loss: 0.0021 lr: 0.02\n",
      "iteration: 23200 loss: 0.0026 lr: 0.02\n",
      "iteration: 23250 loss: 0.0024 lr: 0.02\n",
      "iteration: 23300 loss: 0.0023 lr: 0.02\n",
      "iteration: 23350 loss: 0.0021 lr: 0.02\n",
      "iteration: 23400 loss: 0.0023 lr: 0.02\n",
      "iteration: 23450 loss: 0.0027 lr: 0.02\n",
      "iteration: 23500 loss: 0.0022 lr: 0.02\n",
      "iteration: 23550 loss: 0.0019 lr: 0.02\n",
      "iteration: 23600 loss: 0.0022 lr: 0.02\n",
      "iteration: 23650 loss: 0.0022 lr: 0.02\n",
      "iteration: 23700 loss: 0.0024 lr: 0.02\n",
      "iteration: 23750 loss: 0.0019 lr: 0.02\n",
      "iteration: 23800 loss: 0.0022 lr: 0.02\n",
      "iteration: 23850 loss: 0.0027 lr: 0.02\n",
      "iteration: 23900 loss: 0.0022 lr: 0.02\n",
      "iteration: 23950 loss: 0.0020 lr: 0.02\n",
      "iteration: 24000 loss: 0.0024 lr: 0.02\n",
      "iteration: 24050 loss: 0.0023 lr: 0.02\n",
      "iteration: 24100 loss: 0.0022 lr: 0.02\n",
      "iteration: 24150 loss: 0.0020 lr: 0.02\n",
      "iteration: 24200 loss: 0.0018 lr: 0.02\n",
      "iteration: 24250 loss: 0.0019 lr: 0.02\n",
      "iteration: 24300 loss: 0.0024 lr: 0.02\n",
      "iteration: 24350 loss: 0.0025 lr: 0.02\n",
      "iteration: 24400 loss: 0.0023 lr: 0.02\n",
      "iteration: 24450 loss: 0.0019 lr: 0.02\n",
      "iteration: 24500 loss: 0.0024 lr: 0.02\n",
      "iteration: 24550 loss: 0.0025 lr: 0.02\n",
      "iteration: 24600 loss: 0.0027 lr: 0.02\n",
      "iteration: 24650 loss: 0.0025 lr: 0.02\n",
      "iteration: 24700 loss: 0.0028 lr: 0.02\n",
      "iteration: 24750 loss: 0.0021 lr: 0.02\n",
      "iteration: 24800 loss: 0.0025 lr: 0.02\n",
      "iteration: 24850 loss: 0.0021 lr: 0.02\n",
      "iteration: 24900 loss: 0.0024 lr: 0.02\n",
      "iteration: 24950 loss: 0.0026 lr: 0.02\n",
      "iteration: 25000 loss: 0.0021 lr: 0.02\n",
      "iteration: 25050 loss: 0.0021 lr: 0.02\n",
      "iteration: 25100 loss: 0.0024 lr: 0.02\n",
      "iteration: 25150 loss: 0.0023 lr: 0.02\n",
      "iteration: 25200 loss: 0.0023 lr: 0.02\n",
      "iteration: 25250 loss: 0.0017 lr: 0.02\n",
      "iteration: 25300 loss: 0.0019 lr: 0.02\n",
      "iteration: 25350 loss: 0.0022 lr: 0.02\n",
      "iteration: 25400 loss: 0.0025 lr: 0.02\n",
      "iteration: 25450 loss: 0.0022 lr: 0.02\n",
      "iteration: 25500 loss: 0.0022 lr: 0.02\n",
      "iteration: 25550 loss: 0.0032 lr: 0.02\n",
      "iteration: 25600 loss: 0.0024 lr: 0.02\n",
      "iteration: 25650 loss: 0.0023 lr: 0.02\n",
      "iteration: 25700 loss: 0.0019 lr: 0.02\n",
      "iteration: 25750 loss: 0.0025 lr: 0.02\n",
      "iteration: 25800 loss: 0.0019 lr: 0.02\n",
      "iteration: 25850 loss: 0.0028 lr: 0.02\n",
      "iteration: 25900 loss: 0.0024 lr: 0.02\n",
      "iteration: 25950 loss: 0.0025 lr: 0.02\n",
      "iteration: 26000 loss: 0.0020 lr: 0.02\n",
      "iteration: 26050 loss: 0.0027 lr: 0.02\n",
      "iteration: 26100 loss: 0.0020 lr: 0.02\n",
      "iteration: 26150 loss: 0.0024 lr: 0.02\n",
      "iteration: 26200 loss: 0.0019 lr: 0.02\n",
      "iteration: 26250 loss: 0.0023 lr: 0.02\n",
      "iteration: 26300 loss: 0.0018 lr: 0.02\n",
      "iteration: 26350 loss: 0.0019 lr: 0.02\n",
      "iteration: 26400 loss: 0.0020 lr: 0.02\n",
      "iteration: 26450 loss: 0.0021 lr: 0.02\n",
      "iteration: 26500 loss: 0.0019 lr: 0.02\n",
      "iteration: 26550 loss: 0.0022 lr: 0.02\n",
      "iteration: 26600 loss: 0.0020 lr: 0.02\n",
      "iteration: 26650 loss: 0.0023 lr: 0.02\n",
      "iteration: 26700 loss: 0.0022 lr: 0.02\n",
      "iteration: 26750 loss: 0.0023 lr: 0.02\n",
      "iteration: 26800 loss: 0.0023 lr: 0.02\n",
      "iteration: 26850 loss: 0.0019 lr: 0.02\n",
      "iteration: 26900 loss: 0.0021 lr: 0.02\n",
      "iteration: 26950 loss: 0.0020 lr: 0.02\n",
      "iteration: 27000 loss: 0.0026 lr: 0.02\n",
      "iteration: 27050 loss: 0.0019 lr: 0.02\n",
      "iteration: 27100 loss: 0.0022 lr: 0.02\n",
      "iteration: 27150 loss: 0.0018 lr: 0.02\n",
      "iteration: 27200 loss: 0.0022 lr: 0.02\n",
      "iteration: 27250 loss: 0.0020 lr: 0.02\n",
      "iteration: 27300 loss: 0.0019 lr: 0.02\n",
      "iteration: 27350 loss: 0.0024 lr: 0.02\n",
      "iteration: 27400 loss: 0.0024 lr: 0.02\n",
      "iteration: 27450 loss: 0.0024 lr: 0.02\n",
      "iteration: 27500 loss: 0.0021 lr: 0.02\n",
      "iteration: 27550 loss: 0.0022 lr: 0.02\n",
      "iteration: 27600 loss: 0.0019 lr: 0.02\n",
      "iteration: 27650 loss: 0.0024 lr: 0.02\n",
      "iteration: 27700 loss: 0.0021 lr: 0.02\n",
      "iteration: 27750 loss: 0.0017 lr: 0.02\n",
      "iteration: 27800 loss: 0.0018 lr: 0.02\n",
      "iteration: 27850 loss: 0.0024 lr: 0.02\n",
      "iteration: 27900 loss: 0.0017 lr: 0.02\n",
      "iteration: 27950 loss: 0.0021 lr: 0.02\n",
      "iteration: 28000 loss: 0.0019 lr: 0.02\n",
      "iteration: 28050 loss: 0.0023 lr: 0.02\n",
      "iteration: 28100 loss: 0.0021 lr: 0.02\n",
      "iteration: 28150 loss: 0.0019 lr: 0.02\n",
      "iteration: 28200 loss: 0.0019 lr: 0.02\n",
      "iteration: 28250 loss: 0.0017 lr: 0.02\n",
      "iteration: 28300 loss: 0.0018 lr: 0.02\n",
      "iteration: 28350 loss: 0.0018 lr: 0.02\n",
      "iteration: 28400 loss: 0.0022 lr: 0.02\n",
      "iteration: 28450 loss: 0.0019 lr: 0.02\n",
      "iteration: 28500 loss: 0.0021 lr: 0.02\n",
      "iteration: 28550 loss: 0.0026 lr: 0.02\n",
      "iteration: 28600 loss: 0.0028 lr: 0.02\n",
      "iteration: 28650 loss: 0.0022 lr: 0.02\n",
      "iteration: 28700 loss: 0.0021 lr: 0.02\n",
      "iteration: 28750 loss: 0.0019 lr: 0.02\n",
      "iteration: 28800 loss: 0.0021 lr: 0.02\n",
      "iteration: 28850 loss: 0.0023 lr: 0.02\n",
      "iteration: 28900 loss: 0.0023 lr: 0.02\n",
      "iteration: 28950 loss: 0.0022 lr: 0.02\n",
      "iteration: 29000 loss: 0.0023 lr: 0.02\n",
      "iteration: 29050 loss: 0.0018 lr: 0.02\n",
      "iteration: 29100 loss: 0.0020 lr: 0.02\n",
      "iteration: 29150 loss: 0.0018 lr: 0.02\n",
      "iteration: 29200 loss: 0.0022 lr: 0.02\n",
      "iteration: 29250 loss: 0.0022 lr: 0.02\n",
      "iteration: 29300 loss: 0.0016 lr: 0.02\n",
      "iteration: 29350 loss: 0.0022 lr: 0.02\n",
      "iteration: 29400 loss: 0.0017 lr: 0.02\n",
      "iteration: 29450 loss: 0.0020 lr: 0.02\n",
      "iteration: 29500 loss: 0.0023 lr: 0.02\n",
      "iteration: 29550 loss: 0.0020 lr: 0.02\n",
      "iteration: 29600 loss: 0.0021 lr: 0.02\n",
      "iteration: 29650 loss: 0.0024 lr: 0.02\n",
      "iteration: 29700 loss: 0.0018 lr: 0.02\n",
      "iteration: 29750 loss: 0.0020 lr: 0.02\n",
      "iteration: 29800 loss: 0.0023 lr: 0.02\n",
      "iteration: 29850 loss: 0.0023 lr: 0.02\n",
      "iteration: 29900 loss: 0.0019 lr: 0.02\n",
      "iteration: 29950 loss: 0.0017 lr: 0.02\n",
      "iteration: 30000 loss: 0.0018 lr: 0.02\n",
      "iteration: 30050 loss: 0.0023 lr: 0.02\n",
      "iteration: 30100 loss: 0.0020 lr: 0.02\n",
      "iteration: 30150 loss: 0.0019 lr: 0.02\n",
      "iteration: 30200 loss: 0.0020 lr: 0.02\n",
      "iteration: 30250 loss: 0.0019 lr: 0.02\n",
      "iteration: 30300 loss: 0.0020 lr: 0.02\n",
      "iteration: 30350 loss: 0.0019 lr: 0.02\n",
      "iteration: 30400 loss: 0.0017 lr: 0.02\n",
      "iteration: 30450 loss: 0.0022 lr: 0.02\n",
      "iteration: 30500 loss: 0.0026 lr: 0.02\n",
      "iteration: 30550 loss: 0.0022 lr: 0.02\n",
      "iteration: 30600 loss: 0.0018 lr: 0.02\n",
      "iteration: 30650 loss: 0.0020 lr: 0.02\n",
      "iteration: 30700 loss: 0.0019 lr: 0.02\n",
      "iteration: 30750 loss: 0.0016 lr: 0.02\n",
      "iteration: 30800 loss: 0.0020 lr: 0.02\n",
      "iteration: 30850 loss: 0.0023 lr: 0.02\n",
      "iteration: 30900 loss: 0.0020 lr: 0.02\n",
      "iteration: 30950 loss: 0.0018 lr: 0.02\n",
      "iteration: 31000 loss: 0.0018 lr: 0.02\n",
      "iteration: 31050 loss: 0.0017 lr: 0.02\n",
      "iteration: 31100 loss: 0.0019 lr: 0.02\n",
      "iteration: 31150 loss: 0.0019 lr: 0.02\n",
      "iteration: 31200 loss: 0.0023 lr: 0.02\n",
      "iteration: 31250 loss: 0.0016 lr: 0.02\n",
      "iteration: 31300 loss: 0.0018 lr: 0.02\n",
      "iteration: 31350 loss: 0.0018 lr: 0.02\n",
      "iteration: 31400 loss: 0.0017 lr: 0.02\n",
      "iteration: 31450 loss: 0.0019 lr: 0.02\n",
      "iteration: 31500 loss: 0.0018 lr: 0.02\n",
      "iteration: 31550 loss: 0.0019 lr: 0.02\n",
      "iteration: 31600 loss: 0.0019 lr: 0.02\n",
      "iteration: 31650 loss: 0.0019 lr: 0.02\n",
      "iteration: 31700 loss: 0.0015 lr: 0.02\n",
      "iteration: 31750 loss: 0.0022 lr: 0.02\n",
      "iteration: 31800 loss: 0.0017 lr: 0.02\n",
      "iteration: 31850 loss: 0.0022 lr: 0.02\n",
      "iteration: 31900 loss: 0.0019 lr: 0.02\n",
      "iteration: 31950 loss: 0.0016 lr: 0.02\n",
      "iteration: 32000 loss: 0.0022 lr: 0.02\n",
      "iteration: 32050 loss: 0.0019 lr: 0.02\n",
      "iteration: 32100 loss: 0.0019 lr: 0.02\n",
      "iteration: 32150 loss: 0.0017 lr: 0.02\n",
      "iteration: 32200 loss: 0.0017 lr: 0.02\n",
      "iteration: 32250 loss: 0.0018 lr: 0.02\n",
      "iteration: 32300 loss: 0.0016 lr: 0.02\n",
      "iteration: 32350 loss: 0.0015 lr: 0.02\n",
      "iteration: 32400 loss: 0.0017 lr: 0.02\n",
      "iteration: 32450 loss: 0.0022 lr: 0.02\n",
      "iteration: 32500 loss: 0.0017 lr: 0.02\n",
      "iteration: 32550 loss: 0.0018 lr: 0.02\n",
      "iteration: 32600 loss: 0.0019 lr: 0.02\n",
      "iteration: 32650 loss: 0.0022 lr: 0.02\n",
      "iteration: 32700 loss: 0.0018 lr: 0.02\n",
      "iteration: 32750 loss: 0.0018 lr: 0.02\n",
      "iteration: 32800 loss: 0.0017 lr: 0.02\n",
      "iteration: 32850 loss: 0.0016 lr: 0.02\n",
      "iteration: 32900 loss: 0.0016 lr: 0.02\n",
      "iteration: 32950 loss: 0.0018 lr: 0.02\n",
      "iteration: 33000 loss: 0.0018 lr: 0.02\n",
      "iteration: 33050 loss: 0.0021 lr: 0.02\n",
      "iteration: 33100 loss: 0.0021 lr: 0.02\n",
      "iteration: 33150 loss: 0.0016 lr: 0.02\n",
      "iteration: 33200 loss: 0.0017 lr: 0.02\n",
      "iteration: 33250 loss: 0.0019 lr: 0.02\n",
      "iteration: 33300 loss: 0.0017 lr: 0.02\n",
      "iteration: 33350 loss: 0.0024 lr: 0.02\n",
      "iteration: 33400 loss: 0.0021 lr: 0.02\n",
      "iteration: 33450 loss: 0.0020 lr: 0.02\n",
      "iteration: 33500 loss: 0.0017 lr: 0.02\n",
      "iteration: 33550 loss: 0.0019 lr: 0.02\n",
      "iteration: 33600 loss: 0.0017 lr: 0.02\n",
      "iteration: 33650 loss: 0.0017 lr: 0.02\n",
      "iteration: 33700 loss: 0.0019 lr: 0.02\n",
      "iteration: 33750 loss: 0.0018 lr: 0.02\n",
      "iteration: 33800 loss: 0.0018 lr: 0.02\n",
      "iteration: 33850 loss: 0.0018 lr: 0.02\n",
      "iteration: 33900 loss: 0.0020 lr: 0.02\n",
      "iteration: 33950 loss: 0.0019 lr: 0.02\n",
      "iteration: 34000 loss: 0.0020 lr: 0.02\n",
      "iteration: 34050 loss: 0.0018 lr: 0.02\n",
      "iteration: 34100 loss: 0.0016 lr: 0.02\n",
      "iteration: 34150 loss: 0.0024 lr: 0.02\n",
      "iteration: 34200 loss: 0.0023 lr: 0.02\n",
      "iteration: 34250 loss: 0.0016 lr: 0.02\n",
      "iteration: 34300 loss: 0.0017 lr: 0.02\n",
      "iteration: 34350 loss: 0.0021 lr: 0.02\n",
      "iteration: 34400 loss: 0.0017 lr: 0.02\n",
      "iteration: 34450 loss: 0.0018 lr: 0.02\n",
      "iteration: 34500 loss: 0.0023 lr: 0.02\n",
      "iteration: 34550 loss: 0.0022 lr: 0.02\n",
      "iteration: 34600 loss: 0.0018 lr: 0.02\n",
      "iteration: 34650 loss: 0.0018 lr: 0.02\n",
      "iteration: 34700 loss: 0.0019 lr: 0.02\n",
      "iteration: 34750 loss: 0.0020 lr: 0.02\n",
      "iteration: 34800 loss: 0.0024 lr: 0.02\n",
      "iteration: 34850 loss: 0.0019 lr: 0.02\n",
      "iteration: 34900 loss: 0.0018 lr: 0.02\n",
      "iteration: 34950 loss: 0.0019 lr: 0.02\n",
      "iteration: 35000 loss: 0.0018 lr: 0.02\n",
      "iteration: 35050 loss: 0.0017 lr: 0.02\n",
      "iteration: 35100 loss: 0.0020 lr: 0.02\n",
      "iteration: 35150 loss: 0.0016 lr: 0.02\n",
      "iteration: 35200 loss: 0.0018 lr: 0.02\n",
      "iteration: 35250 loss: 0.0021 lr: 0.02\n",
      "iteration: 35300 loss: 0.0020 lr: 0.02\n",
      "iteration: 35350 loss: 0.0016 lr: 0.02\n",
      "iteration: 35400 loss: 0.0019 lr: 0.02\n",
      "iteration: 35450 loss: 0.0017 lr: 0.02\n",
      "iteration: 35500 loss: 0.0017 lr: 0.02\n",
      "iteration: 35550 loss: 0.0015 lr: 0.02\n",
      "iteration: 35600 loss: 0.0018 lr: 0.02\n",
      "iteration: 35650 loss: 0.0019 lr: 0.02\n",
      "iteration: 35700 loss: 0.0016 lr: 0.02\n",
      "iteration: 35750 loss: 0.0015 lr: 0.02\n",
      "iteration: 35800 loss: 0.0019 lr: 0.02\n",
      "iteration: 35850 loss: 0.0018 lr: 0.02\n",
      "iteration: 35900 loss: 0.0018 lr: 0.02\n",
      "iteration: 35950 loss: 0.0016 lr: 0.02\n",
      "iteration: 36000 loss: 0.0020 lr: 0.02\n",
      "iteration: 36050 loss: 0.0015 lr: 0.02\n",
      "iteration: 36100 loss: 0.0018 lr: 0.02\n",
      "iteration: 36150 loss: 0.0013 lr: 0.02\n",
      "iteration: 36200 loss: 0.0018 lr: 0.02\n",
      "iteration: 36250 loss: 0.0015 lr: 0.02\n",
      "iteration: 36300 loss: 0.0026 lr: 0.02\n",
      "iteration: 36350 loss: 0.0017 lr: 0.02\n",
      "iteration: 36400 loss: 0.0018 lr: 0.02\n",
      "iteration: 36450 loss: 0.0019 lr: 0.02\n",
      "iteration: 36500 loss: 0.0021 lr: 0.02\n",
      "iteration: 36550 loss: 0.0017 lr: 0.02\n",
      "iteration: 36600 loss: 0.0016 lr: 0.02\n",
      "iteration: 36650 loss: 0.0020 lr: 0.02\n",
      "iteration: 36700 loss: 0.0016 lr: 0.02\n",
      "iteration: 36750 loss: 0.0015 lr: 0.02\n",
      "iteration: 36800 loss: 0.0017 lr: 0.02\n",
      "iteration: 36850 loss: 0.0018 lr: 0.02\n",
      "iteration: 36900 loss: 0.0018 lr: 0.02\n",
      "iteration: 36950 loss: 0.0018 lr: 0.02\n",
      "iteration: 37000 loss: 0.0020 lr: 0.02\n",
      "iteration: 37050 loss: 0.0017 lr: 0.02\n",
      "iteration: 37100 loss: 0.0016 lr: 0.02\n",
      "iteration: 37150 loss: 0.0016 lr: 0.02\n",
      "iteration: 37200 loss: 0.0016 lr: 0.02\n",
      "iteration: 37250 loss: 0.0019 lr: 0.02\n",
      "iteration: 37300 loss: 0.0017 lr: 0.02\n",
      "iteration: 37350 loss: 0.0015 lr: 0.02\n",
      "iteration: 37400 loss: 0.0015 lr: 0.02\n",
      "iteration: 37450 loss: 0.0020 lr: 0.02\n",
      "iteration: 37500 loss: 0.0021 lr: 0.02\n",
      "iteration: 37550 loss: 0.0017 lr: 0.02\n",
      "iteration: 37600 loss: 0.0017 lr: 0.02\n",
      "iteration: 37650 loss: 0.0015 lr: 0.02\n",
      "iteration: 37700 loss: 0.0021 lr: 0.02\n",
      "iteration: 37750 loss: 0.0018 lr: 0.02\n",
      "iteration: 37800 loss: 0.0017 lr: 0.02\n",
      "iteration: 37850 loss: 0.0021 lr: 0.02\n",
      "iteration: 37900 loss: 0.0017 lr: 0.02\n",
      "iteration: 37950 loss: 0.0019 lr: 0.02\n",
      "iteration: 38000 loss: 0.0016 lr: 0.02\n",
      "iteration: 38050 loss: 0.0024 lr: 0.02\n",
      "iteration: 38100 loss: 0.0017 lr: 0.02\n",
      "iteration: 38150 loss: 0.0021 lr: 0.02\n",
      "iteration: 38200 loss: 0.0017 lr: 0.02\n",
      "iteration: 38250 loss: 0.0015 lr: 0.02\n",
      "iteration: 38300 loss: 0.0024 lr: 0.02\n",
      "iteration: 38350 loss: 0.0015 lr: 0.02\n",
      "iteration: 38400 loss: 0.0018 lr: 0.02\n",
      "iteration: 38450 loss: 0.0017 lr: 0.02\n",
      "iteration: 38500 loss: 0.0015 lr: 0.02\n",
      "iteration: 38550 loss: 0.0018 lr: 0.02\n",
      "iteration: 38600 loss: 0.0016 lr: 0.02\n",
      "iteration: 38650 loss: 0.0017 lr: 0.02\n",
      "iteration: 38700 loss: 0.0016 lr: 0.02\n",
      "iteration: 38750 loss: 0.0020 lr: 0.02\n",
      "iteration: 38800 loss: 0.0015 lr: 0.02\n",
      "iteration: 38850 loss: 0.0017 lr: 0.02\n",
      "iteration: 38900 loss: 0.0018 lr: 0.02\n",
      "iteration: 38950 loss: 0.0020 lr: 0.02\n",
      "iteration: 39000 loss: 0.0017 lr: 0.02\n",
      "iteration: 39050 loss: 0.0019 lr: 0.02\n",
      "iteration: 39100 loss: 0.0017 lr: 0.02\n",
      "iteration: 39150 loss: 0.0018 lr: 0.02\n",
      "iteration: 39200 loss: 0.0017 lr: 0.02\n",
      "iteration: 39250 loss: 0.0017 lr: 0.02\n",
      "iteration: 39300 loss: 0.0019 lr: 0.02\n",
      "iteration: 39350 loss: 0.0015 lr: 0.02\n",
      "iteration: 39400 loss: 0.0018 lr: 0.02\n",
      "iteration: 39450 loss: 0.0017 lr: 0.02\n",
      "iteration: 39500 loss: 0.0018 lr: 0.02\n",
      "iteration: 39550 loss: 0.0020 lr: 0.02\n",
      "iteration: 39600 loss: 0.0016 lr: 0.02\n",
      "iteration: 39650 loss: 0.0018 lr: 0.02\n",
      "iteration: 39700 loss: 0.0017 lr: 0.02\n",
      "iteration: 39750 loss: 0.0016 lr: 0.02\n",
      "iteration: 39800 loss: 0.0018 lr: 0.02\n",
      "iteration: 39850 loss: 0.0014 lr: 0.02\n",
      "iteration: 39900 loss: 0.0015 lr: 0.02\n",
      "iteration: 39950 loss: 0.0019 lr: 0.02\n",
      "iteration: 40000 loss: 0.0017 lr: 0.02\n",
      "iteration: 40050 loss: 0.0015 lr: 0.02\n",
      "iteration: 40100 loss: 0.0019 lr: 0.02\n",
      "iteration: 40150 loss: 0.0017 lr: 0.02\n",
      "iteration: 40200 loss: 0.0016 lr: 0.02\n",
      "iteration: 40250 loss: 0.0018 lr: 0.02\n",
      "iteration: 40300 loss: 0.0015 lr: 0.02\n",
      "iteration: 40350 loss: 0.0019 lr: 0.02\n",
      "iteration: 40400 loss: 0.0015 lr: 0.02\n",
      "iteration: 40450 loss: 0.0017 lr: 0.02\n",
      "iteration: 40500 loss: 0.0017 lr: 0.02\n",
      "iteration: 40550 loss: 0.0017 lr: 0.02\n",
      "iteration: 40600 loss: 0.0016 lr: 0.02\n",
      "iteration: 40650 loss: 0.0018 lr: 0.02\n",
      "iteration: 40700 loss: 0.0017 lr: 0.02\n",
      "iteration: 40750 loss: 0.0020 lr: 0.02\n",
      "iteration: 40800 loss: 0.0016 lr: 0.02\n",
      "iteration: 40850 loss: 0.0016 lr: 0.02\n",
      "iteration: 40900 loss: 0.0017 lr: 0.02\n",
      "iteration: 40950 loss: 0.0014 lr: 0.02\n",
      "iteration: 41000 loss: 0.0016 lr: 0.02\n",
      "iteration: 41050 loss: 0.0019 lr: 0.02\n",
      "iteration: 41100 loss: 0.0017 lr: 0.02\n",
      "iteration: 41150 loss: 0.0020 lr: 0.02\n",
      "iteration: 41200 loss: 0.0017 lr: 0.02\n",
      "iteration: 41250 loss: 0.0018 lr: 0.02\n",
      "iteration: 41300 loss: 0.0019 lr: 0.02\n",
      "iteration: 41350 loss: 0.0020 lr: 0.02\n",
      "iteration: 41400 loss: 0.0022 lr: 0.02\n",
      "iteration: 41450 loss: 0.0015 lr: 0.02\n",
      "iteration: 41500 loss: 0.0015 lr: 0.02\n",
      "iteration: 41550 loss: 0.0015 lr: 0.02\n",
      "iteration: 41600 loss: 0.0018 lr: 0.02\n",
      "iteration: 41650 loss: 0.0015 lr: 0.02\n",
      "iteration: 41700 loss: 0.0016 lr: 0.02\n",
      "iteration: 41750 loss: 0.0018 lr: 0.02\n",
      "iteration: 41800 loss: 0.0019 lr: 0.02\n",
      "iteration: 41850 loss: 0.0014 lr: 0.02\n",
      "iteration: 41900 loss: 0.0016 lr: 0.02\n",
      "iteration: 41950 loss: 0.0018 lr: 0.02\n",
      "iteration: 42000 loss: 0.0018 lr: 0.02\n",
      "iteration: 42050 loss: 0.0016 lr: 0.02\n",
      "iteration: 42100 loss: 0.0016 lr: 0.02\n",
      "iteration: 42150 loss: 0.0018 lr: 0.02\n",
      "iteration: 42200 loss: 0.0018 lr: 0.02\n",
      "iteration: 42250 loss: 0.0020 lr: 0.02\n",
      "iteration: 42300 loss: 0.0019 lr: 0.02\n",
      "iteration: 42350 loss: 0.0015 lr: 0.02\n",
      "iteration: 42400 loss: 0.0016 lr: 0.02\n",
      "iteration: 42450 loss: 0.0015 lr: 0.02\n",
      "iteration: 42500 loss: 0.0019 lr: 0.02\n",
      "iteration: 42550 loss: 0.0015 lr: 0.02\n",
      "iteration: 42600 loss: 0.0020 lr: 0.02\n",
      "iteration: 42650 loss: 0.0016 lr: 0.02\n",
      "iteration: 42700 loss: 0.0016 lr: 0.02\n",
      "iteration: 42750 loss: 0.0017 lr: 0.02\n",
      "iteration: 42800 loss: 0.0016 lr: 0.02\n",
      "iteration: 42850 loss: 0.0014 lr: 0.02\n",
      "iteration: 42900 loss: 0.0018 lr: 0.02\n",
      "iteration: 42950 loss: 0.0016 lr: 0.02\n",
      "iteration: 43000 loss: 0.0017 lr: 0.02\n",
      "iteration: 43050 loss: 0.0016 lr: 0.02\n",
      "iteration: 43100 loss: 0.0015 lr: 0.02\n",
      "iteration: 43150 loss: 0.0016 lr: 0.02\n",
      "iteration: 43200 loss: 0.0014 lr: 0.02\n",
      "iteration: 43250 loss: 0.0016 lr: 0.02\n",
      "iteration: 43300 loss: 0.0025 lr: 0.02\n",
      "iteration: 43350 loss: 0.0019 lr: 0.02\n",
      "iteration: 43400 loss: 0.0018 lr: 0.02\n",
      "iteration: 43450 loss: 0.0020 lr: 0.02\n",
      "iteration: 43500 loss: 0.0018 lr: 0.02\n",
      "iteration: 43550 loss: 0.0018 lr: 0.02\n",
      "iteration: 43600 loss: 0.0016 lr: 0.02\n",
      "iteration: 43650 loss: 0.0014 lr: 0.02\n",
      "iteration: 43700 loss: 0.0020 lr: 0.02\n",
      "iteration: 43750 loss: 0.0015 lr: 0.02\n",
      "iteration: 43800 loss: 0.0015 lr: 0.02\n",
      "iteration: 43850 loss: 0.0023 lr: 0.02\n",
      "iteration: 43900 loss: 0.0019 lr: 0.02\n",
      "iteration: 43950 loss: 0.0018 lr: 0.02\n",
      "iteration: 44000 loss: 0.0016 lr: 0.02\n",
      "iteration: 44050 loss: 0.0021 lr: 0.02\n",
      "iteration: 44100 loss: 0.0016 lr: 0.02\n",
      "iteration: 44150 loss: 0.0016 lr: 0.02\n",
      "iteration: 44200 loss: 0.0014 lr: 0.02\n",
      "iteration: 44250 loss: 0.0017 lr: 0.02\n",
      "iteration: 44300 loss: 0.0016 lr: 0.02\n",
      "iteration: 44350 loss: 0.0019 lr: 0.02\n",
      "iteration: 44400 loss: 0.0019 lr: 0.02\n",
      "iteration: 44450 loss: 0.0016 lr: 0.02\n",
      "iteration: 44500 loss: 0.0018 lr: 0.02\n",
      "iteration: 44550 loss: 0.0018 lr: 0.02\n",
      "iteration: 44600 loss: 0.0018 lr: 0.02\n",
      "iteration: 44650 loss: 0.0015 lr: 0.02\n",
      "iteration: 44700 loss: 0.0015 lr: 0.02\n",
      "iteration: 44750 loss: 0.0014 lr: 0.02\n",
      "iteration: 44800 loss: 0.0017 lr: 0.02\n",
      "iteration: 44850 loss: 0.0014 lr: 0.02\n",
      "iteration: 44900 loss: 0.0015 lr: 0.02\n",
      "iteration: 44950 loss: 0.0017 lr: 0.02\n",
      "iteration: 45000 loss: 0.0015 lr: 0.02\n",
      "iteration: 45050 loss: 0.0015 lr: 0.02\n",
      "iteration: 45100 loss: 0.0017 lr: 0.02\n",
      "iteration: 45150 loss: 0.0015 lr: 0.02\n",
      "iteration: 45200 loss: 0.0013 lr: 0.02\n",
      "iteration: 45250 loss: 0.0016 lr: 0.02\n",
      "iteration: 45300 loss: 0.0015 lr: 0.02\n",
      "iteration: 45350 loss: 0.0014 lr: 0.02\n",
      "iteration: 45400 loss: 0.0015 lr: 0.02\n",
      "iteration: 45450 loss: 0.0014 lr: 0.02\n",
      "iteration: 45500 loss: 0.0015 lr: 0.02\n",
      "iteration: 45550 loss: 0.0019 lr: 0.02\n",
      "iteration: 45600 loss: 0.0016 lr: 0.02\n",
      "iteration: 45650 loss: 0.0013 lr: 0.02\n",
      "iteration: 45700 loss: 0.0020 lr: 0.02\n",
      "iteration: 45750 loss: 0.0021 lr: 0.02\n",
      "iteration: 45800 loss: 0.0015 lr: 0.02\n",
      "iteration: 45850 loss: 0.0014 lr: 0.02\n",
      "iteration: 45900 loss: 0.0015 lr: 0.02\n",
      "iteration: 45950 loss: 0.0019 lr: 0.02\n",
      "iteration: 46000 loss: 0.0022 lr: 0.02\n",
      "iteration: 46050 loss: 0.0017 lr: 0.02\n",
      "iteration: 46100 loss: 0.0016 lr: 0.02\n",
      "iteration: 46150 loss: 0.0014 lr: 0.02\n",
      "iteration: 46200 loss: 0.0016 lr: 0.02\n",
      "iteration: 46250 loss: 0.0015 lr: 0.02\n",
      "iteration: 46300 loss: 0.0020 lr: 0.02\n",
      "iteration: 46350 loss: 0.0018 lr: 0.02\n",
      "iteration: 46400 loss: 0.0014 lr: 0.02\n",
      "iteration: 46450 loss: 0.0020 lr: 0.02\n",
      "iteration: 46500 loss: 0.0013 lr: 0.02\n",
      "iteration: 46550 loss: 0.0014 lr: 0.02\n",
      "iteration: 46600 loss: 0.0015 lr: 0.02\n",
      "iteration: 46650 loss: 0.0017 lr: 0.02\n",
      "iteration: 46700 loss: 0.0017 lr: 0.02\n",
      "iteration: 46750 loss: 0.0020 lr: 0.02\n",
      "iteration: 46800 loss: 0.0018 lr: 0.02\n",
      "iteration: 46850 loss: 0.0019 lr: 0.02\n",
      "iteration: 46900 loss: 0.0017 lr: 0.02\n",
      "iteration: 46950 loss: 0.0016 lr: 0.02\n",
      "iteration: 47000 loss: 0.0015 lr: 0.02\n",
      "iteration: 47050 loss: 0.0016 lr: 0.02\n",
      "iteration: 47100 loss: 0.0015 lr: 0.02\n",
      "iteration: 47150 loss: 0.0017 lr: 0.02\n",
      "iteration: 47200 loss: 0.0015 lr: 0.02\n",
      "iteration: 47250 loss: 0.0022 lr: 0.02\n",
      "iteration: 47300 loss: 0.0016 lr: 0.02\n",
      "iteration: 47350 loss: 0.0016 lr: 0.02\n",
      "iteration: 47400 loss: 0.0014 lr: 0.02\n",
      "iteration: 47450 loss: 0.0014 lr: 0.02\n",
      "iteration: 47500 loss: 0.0018 lr: 0.02\n",
      "iteration: 47550 loss: 0.0011 lr: 0.02\n",
      "iteration: 47600 loss: 0.0017 lr: 0.02\n",
      "iteration: 47650 loss: 0.0015 lr: 0.02\n",
      "iteration: 47700 loss: 0.0015 lr: 0.02\n",
      "iteration: 47750 loss: 0.0016 lr: 0.02\n",
      "iteration: 47800 loss: 0.0018 lr: 0.02\n",
      "iteration: 47850 loss: 0.0017 lr: 0.02\n",
      "iteration: 47900 loss: 0.0016 lr: 0.02\n",
      "iteration: 47950 loss: 0.0015 lr: 0.02\n",
      "iteration: 48000 loss: 0.0020 lr: 0.02\n",
      "iteration: 48050 loss: 0.0015 lr: 0.02\n",
      "iteration: 48100 loss: 0.0018 lr: 0.02\n",
      "iteration: 48150 loss: 0.0016 lr: 0.02\n",
      "iteration: 48200 loss: 0.0015 lr: 0.02\n",
      "iteration: 48250 loss: 0.0018 lr: 0.02\n",
      "iteration: 48300 loss: 0.0021 lr: 0.02\n",
      "iteration: 48350 loss: 0.0016 lr: 0.02\n",
      "iteration: 48400 loss: 0.0012 lr: 0.02\n",
      "iteration: 48450 loss: 0.0014 lr: 0.02\n",
      "iteration: 48500 loss: 0.0016 lr: 0.02\n",
      "iteration: 48550 loss: 0.0014 lr: 0.02\n",
      "iteration: 48600 loss: 0.0013 lr: 0.02\n",
      "iteration: 48650 loss: 0.0016 lr: 0.02\n",
      "iteration: 48700 loss: 0.0016 lr: 0.02\n",
      "iteration: 48750 loss: 0.0014 lr: 0.02\n",
      "iteration: 48800 loss: 0.0015 lr: 0.02\n",
      "iteration: 48850 loss: 0.0014 lr: 0.02\n",
      "iteration: 48900 loss: 0.0012 lr: 0.02\n",
      "iteration: 48950 loss: 0.0013 lr: 0.02\n",
      "iteration: 49000 loss: 0.0016 lr: 0.02\n",
      "iteration: 49050 loss: 0.0014 lr: 0.02\n",
      "iteration: 49100 loss: 0.0017 lr: 0.02\n",
      "iteration: 49150 loss: 0.0014 lr: 0.02\n",
      "iteration: 49200 loss: 0.0015 lr: 0.02\n",
      "iteration: 49250 loss: 0.0015 lr: 0.02\n",
      "iteration: 49300 loss: 0.0015 lr: 0.02\n",
      "iteration: 49350 loss: 0.0015 lr: 0.02\n",
      "iteration: 49400 loss: 0.0015 lr: 0.02\n",
      "iteration: 49450 loss: 0.0012 lr: 0.02\n",
      "iteration: 49500 loss: 0.0014 lr: 0.02\n",
      "iteration: 49550 loss: 0.0017 lr: 0.02\n",
      "iteration: 49600 loss: 0.0014 lr: 0.02\n",
      "iteration: 49650 loss: 0.0016 lr: 0.02\n",
      "iteration: 49700 loss: 0.0016 lr: 0.02\n",
      "iteration: 49750 loss: 0.0019 lr: 0.02\n",
      "iteration: 49800 loss: 0.0016 lr: 0.02\n",
      "iteration: 49850 loss: 0.0017 lr: 0.02\n",
      "iteration: 49900 loss: 0.0016 lr: 0.02\n",
      "iteration: 49950 loss: 0.0011 lr: 0.02\n",
      "iteration: 50000 loss: 0.0015 lr: 0.02\n",
      "iteration: 50050 loss: 0.0016 lr: 0.02\n",
      "iteration: 50100 loss: 0.0018 lr: 0.02\n",
      "iteration: 50150 loss: 0.0018 lr: 0.02\n",
      "iteration: 50200 loss: 0.0016 lr: 0.02\n",
      "iteration: 50250 loss: 0.0019 lr: 0.02\n",
      "iteration: 50300 loss: 0.0018 lr: 0.02\n",
      "iteration: 50350 loss: 0.0016 lr: 0.02\n",
      "iteration: 50400 loss: 0.0013 lr: 0.02\n",
      "iteration: 50450 loss: 0.0013 lr: 0.02\n",
      "iteration: 50500 loss: 0.0017 lr: 0.02\n",
      "iteration: 50550 loss: 0.0017 lr: 0.02\n",
      "iteration: 50600 loss: 0.0016 lr: 0.02\n",
      "iteration: 50650 loss: 0.0016 lr: 0.02\n",
      "iteration: 50700 loss: 0.0016 lr: 0.02\n",
      "iteration: 50750 loss: 0.0013 lr: 0.02\n",
      "iteration: 50800 loss: 0.0016 lr: 0.02\n",
      "iteration: 50850 loss: 0.0015 lr: 0.02\n",
      "iteration: 50900 loss: 0.0016 lr: 0.02\n",
      "iteration: 50950 loss: 0.0017 lr: 0.02\n",
      "iteration: 51000 loss: 0.0015 lr: 0.02\n",
      "iteration: 51050 loss: 0.0014 lr: 0.02\n",
      "iteration: 51100 loss: 0.0015 lr: 0.02\n",
      "iteration: 51150 loss: 0.0014 lr: 0.02\n",
      "iteration: 51200 loss: 0.0013 lr: 0.02\n",
      "iteration: 51250 loss: 0.0015 lr: 0.02\n",
      "iteration: 51300 loss: 0.0017 lr: 0.02\n",
      "iteration: 51350 loss: 0.0013 lr: 0.02\n",
      "iteration: 51400 loss: 0.0014 lr: 0.02\n",
      "iteration: 51450 loss: 0.0013 lr: 0.02\n",
      "iteration: 51500 loss: 0.0016 lr: 0.02\n",
      "iteration: 51550 loss: 0.0011 lr: 0.02\n",
      "iteration: 51600 loss: 0.0018 lr: 0.02\n",
      "iteration: 51650 loss: 0.0019 lr: 0.02\n",
      "iteration: 51700 loss: 0.0016 lr: 0.02\n",
      "iteration: 51750 loss: 0.0014 lr: 0.02\n",
      "iteration: 51800 loss: 0.0016 lr: 0.02\n",
      "iteration: 51850 loss: 0.0018 lr: 0.02\n",
      "iteration: 51900 loss: 0.0015 lr: 0.02\n",
      "iteration: 51950 loss: 0.0013 lr: 0.02\n",
      "iteration: 52000 loss: 0.0012 lr: 0.02\n",
      "iteration: 52050 loss: 0.0017 lr: 0.02\n",
      "iteration: 52100 loss: 0.0012 lr: 0.02\n",
      "iteration: 52150 loss: 0.0018 lr: 0.02\n",
      "iteration: 52200 loss: 0.0018 lr: 0.02\n",
      "iteration: 52250 loss: 0.0017 lr: 0.02\n",
      "iteration: 52300 loss: 0.0018 lr: 0.02\n",
      "iteration: 52350 loss: 0.0014 lr: 0.02\n",
      "iteration: 52400 loss: 0.0016 lr: 0.02\n",
      "iteration: 52450 loss: 0.0015 lr: 0.02\n",
      "iteration: 52500 loss: 0.0013 lr: 0.02\n",
      "iteration: 52550 loss: 0.0014 lr: 0.02\n",
      "iteration: 52600 loss: 0.0014 lr: 0.02\n",
      "iteration: 52650 loss: 0.0016 lr: 0.02\n",
      "iteration: 52700 loss: 0.0013 lr: 0.02\n",
      "iteration: 52750 loss: 0.0015 lr: 0.02\n",
      "iteration: 52800 loss: 0.0015 lr: 0.02\n",
      "iteration: 52850 loss: 0.0015 lr: 0.02\n",
      "iteration: 52900 loss: 0.0016 lr: 0.02\n",
      "iteration: 52950 loss: 0.0016 lr: 0.02\n",
      "iteration: 53000 loss: 0.0012 lr: 0.02\n",
      "iteration: 53050 loss: 0.0016 lr: 0.02\n",
      "iteration: 53100 loss: 0.0013 lr: 0.02\n",
      "iteration: 53150 loss: 0.0017 lr: 0.02\n",
      "iteration: 53200 loss: 0.0015 lr: 0.02\n",
      "iteration: 53250 loss: 0.0014 lr: 0.02\n",
      "iteration: 53300 loss: 0.0016 lr: 0.02\n",
      "iteration: 53350 loss: 0.0018 lr: 0.02\n",
      "iteration: 53400 loss: 0.0016 lr: 0.02\n",
      "iteration: 53450 loss: 0.0017 lr: 0.02\n",
      "iteration: 53500 loss: 0.0015 lr: 0.02\n",
      "iteration: 53550 loss: 0.0014 lr: 0.02\n",
      "iteration: 53600 loss: 0.0015 lr: 0.02\n",
      "iteration: 53650 loss: 0.0017 lr: 0.02\n",
      "iteration: 53700 loss: 0.0014 lr: 0.02\n",
      "iteration: 53750 loss: 0.0014 lr: 0.02\n",
      "iteration: 53800 loss: 0.0013 lr: 0.02\n",
      "iteration: 53850 loss: 0.0015 lr: 0.02\n",
      "iteration: 53900 loss: 0.0014 lr: 0.02\n",
      "iteration: 53950 loss: 0.0016 lr: 0.02\n",
      "iteration: 54000 loss: 0.0016 lr: 0.02\n",
      "iteration: 54050 loss: 0.0013 lr: 0.02\n",
      "iteration: 54100 loss: 0.0017 lr: 0.02\n",
      "iteration: 54150 loss: 0.0015 lr: 0.02\n",
      "iteration: 54200 loss: 0.0014 lr: 0.02\n",
      "iteration: 54250 loss: 0.0016 lr: 0.02\n",
      "iteration: 54300 loss: 0.0015 lr: 0.02\n",
      "iteration: 54350 loss: 0.0015 lr: 0.02\n",
      "iteration: 54400 loss: 0.0013 lr: 0.02\n",
      "iteration: 54450 loss: 0.0015 lr: 0.02\n",
      "iteration: 54500 loss: 0.0014 lr: 0.02\n",
      "iteration: 54550 loss: 0.0013 lr: 0.02\n",
      "iteration: 54600 loss: 0.0016 lr: 0.02\n",
      "iteration: 54650 loss: 0.0015 lr: 0.02\n",
      "iteration: 54700 loss: 0.0012 lr: 0.02\n",
      "iteration: 54750 loss: 0.0013 lr: 0.02\n",
      "iteration: 54800 loss: 0.0014 lr: 0.02\n",
      "iteration: 54850 loss: 0.0016 lr: 0.02\n",
      "iteration: 54900 loss: 0.0013 lr: 0.02\n",
      "iteration: 54950 loss: 0.0014 lr: 0.02\n",
      "iteration: 55000 loss: 0.0013 lr: 0.02\n",
      "iteration: 55050 loss: 0.0014 lr: 0.02\n",
      "iteration: 55100 loss: 0.0012 lr: 0.02\n",
      "iteration: 55150 loss: 0.0015 lr: 0.02\n",
      "iteration: 55200 loss: 0.0017 lr: 0.02\n",
      "iteration: 55250 loss: 0.0013 lr: 0.02\n",
      "iteration: 55300 loss: 0.0013 lr: 0.02\n",
      "iteration: 55350 loss: 0.0012 lr: 0.02\n",
      "iteration: 55400 loss: 0.0014 lr: 0.02\n",
      "iteration: 55450 loss: 0.0014 lr: 0.02\n",
      "iteration: 55500 loss: 0.0014 lr: 0.02\n",
      "iteration: 55550 loss: 0.0016 lr: 0.02\n",
      "iteration: 55600 loss: 0.0014 lr: 0.02\n",
      "iteration: 55650 loss: 0.0016 lr: 0.02\n",
      "iteration: 55700 loss: 0.0013 lr: 0.02\n",
      "iteration: 55750 loss: 0.0014 lr: 0.02\n",
      "iteration: 55800 loss: 0.0014 lr: 0.02\n",
      "iteration: 55850 loss: 0.0014 lr: 0.02\n",
      "iteration: 55900 loss: 0.0015 lr: 0.02\n",
      "iteration: 55950 loss: 0.0013 lr: 0.02\n",
      "iteration: 56000 loss: 0.0014 lr: 0.02\n",
      "iteration: 56050 loss: 0.0012 lr: 0.02\n",
      "iteration: 56100 loss: 0.0013 lr: 0.02\n",
      "iteration: 56150 loss: 0.0013 lr: 0.02\n",
      "iteration: 56200 loss: 0.0014 lr: 0.02\n",
      "iteration: 56250 loss: 0.0013 lr: 0.02\n",
      "iteration: 56300 loss: 0.0014 lr: 0.02\n",
      "iteration: 56350 loss: 0.0017 lr: 0.02\n",
      "iteration: 56400 loss: 0.0014 lr: 0.02\n",
      "iteration: 56450 loss: 0.0014 lr: 0.02\n",
      "iteration: 56500 loss: 0.0015 lr: 0.02\n",
      "iteration: 56550 loss: 0.0013 lr: 0.02\n",
      "iteration: 56600 loss: 0.0015 lr: 0.02\n",
      "iteration: 56650 loss: 0.0016 lr: 0.02\n",
      "iteration: 56700 loss: 0.0016 lr: 0.02\n",
      "iteration: 56750 loss: 0.0017 lr: 0.02\n",
      "iteration: 56800 loss: 0.0013 lr: 0.02\n",
      "iteration: 56850 loss: 0.0013 lr: 0.02\n",
      "iteration: 56900 loss: 0.0017 lr: 0.02\n",
      "iteration: 56950 loss: 0.0013 lr: 0.02\n",
      "iteration: 57000 loss: 0.0014 lr: 0.02\n",
      "iteration: 57050 loss: 0.0016 lr: 0.02\n",
      "iteration: 57100 loss: 0.0014 lr: 0.02\n",
      "iteration: 57150 loss: 0.0016 lr: 0.02\n",
      "iteration: 57200 loss: 0.0015 lr: 0.02\n",
      "iteration: 57250 loss: 0.0012 lr: 0.02\n",
      "iteration: 57300 loss: 0.0013 lr: 0.02\n",
      "iteration: 57350 loss: 0.0014 lr: 0.02\n",
      "iteration: 57400 loss: 0.0013 lr: 0.02\n",
      "iteration: 57450 loss: 0.0013 lr: 0.02\n",
      "iteration: 57500 loss: 0.0015 lr: 0.02\n",
      "iteration: 57550 loss: 0.0014 lr: 0.02\n",
      "iteration: 57600 loss: 0.0012 lr: 0.02\n",
      "iteration: 57650 loss: 0.0013 lr: 0.02\n",
      "iteration: 57700 loss: 0.0012 lr: 0.02\n",
      "iteration: 57750 loss: 0.0012 lr: 0.02\n",
      "iteration: 57800 loss: 0.0013 lr: 0.02\n",
      "iteration: 57850 loss: 0.0015 lr: 0.02\n",
      "iteration: 57900 loss: 0.0011 lr: 0.02\n",
      "iteration: 57950 loss: 0.0012 lr: 0.02\n",
      "iteration: 58000 loss: 0.0013 lr: 0.02\n",
      "iteration: 58050 loss: 0.0014 lr: 0.02\n",
      "iteration: 58100 loss: 0.0014 lr: 0.02\n",
      "iteration: 58150 loss: 0.0015 lr: 0.02\n",
      "iteration: 58200 loss: 0.0012 lr: 0.02\n",
      "iteration: 58250 loss: 0.0014 lr: 0.02\n",
      "iteration: 58300 loss: 0.0016 lr: 0.02\n",
      "iteration: 58350 loss: 0.0014 lr: 0.02\n",
      "iteration: 58400 loss: 0.0011 lr: 0.02\n",
      "iteration: 58450 loss: 0.0014 lr: 0.02\n",
      "iteration: 58500 loss: 0.0013 lr: 0.02\n",
      "iteration: 58550 loss: 0.0013 lr: 0.02\n",
      "iteration: 58600 loss: 0.0014 lr: 0.02\n",
      "iteration: 58650 loss: 0.0015 lr: 0.02\n",
      "iteration: 58700 loss: 0.0015 lr: 0.02\n",
      "iteration: 58750 loss: 0.0014 lr: 0.02\n",
      "iteration: 58800 loss: 0.0013 lr: 0.02\n",
      "iteration: 58850 loss: 0.0017 lr: 0.02\n",
      "iteration: 58900 loss: 0.0015 lr: 0.02\n",
      "iteration: 58950 loss: 0.0013 lr: 0.02\n",
      "iteration: 59000 loss: 0.0012 lr: 0.02\n",
      "iteration: 59050 loss: 0.0016 lr: 0.02\n",
      "iteration: 59100 loss: 0.0015 lr: 0.02\n",
      "iteration: 59150 loss: 0.0015 lr: 0.02\n",
      "iteration: 59200 loss: 0.0014 lr: 0.02\n",
      "iteration: 59250 loss: 0.0013 lr: 0.02\n",
      "iteration: 59300 loss: 0.0013 lr: 0.02\n",
      "iteration: 59350 loss: 0.0012 lr: 0.02\n",
      "iteration: 59400 loss: 0.0012 lr: 0.02\n",
      "iteration: 59450 loss: 0.0012 lr: 0.02\n",
      "iteration: 59500 loss: 0.0014 lr: 0.02\n",
      "iteration: 59550 loss: 0.0013 lr: 0.02\n",
      "iteration: 59600 loss: 0.0014 lr: 0.02\n",
      "iteration: 59650 loss: 0.0013 lr: 0.02\n",
      "iteration: 59700 loss: 0.0014 lr: 0.02\n",
      "iteration: 59750 loss: 0.0015 lr: 0.02\n",
      "iteration: 59800 loss: 0.0013 lr: 0.02\n",
      "iteration: 59850 loss: 0.0014 lr: 0.02\n",
      "iteration: 59900 loss: 0.0016 lr: 0.02\n",
      "iteration: 59950 loss: 0.0015 lr: 0.02\n",
      "iteration: 60000 loss: 0.0020 lr: 0.02\n",
      "iteration: 60050 loss: 0.0011 lr: 0.02\n",
      "iteration: 60100 loss: 0.0016 lr: 0.02\n",
      "iteration: 60150 loss: 0.0015 lr: 0.02\n",
      "iteration: 60200 loss: 0.0014 lr: 0.02\n",
      "iteration: 60250 loss: 0.0011 lr: 0.02\n",
      "iteration: 60300 loss: 0.0013 lr: 0.02\n",
      "iteration: 60350 loss: 0.0015 lr: 0.02\n",
      "iteration: 60400 loss: 0.0014 lr: 0.02\n",
      "iteration: 60450 loss: 0.0015 lr: 0.02\n",
      "iteration: 60500 loss: 0.0015 lr: 0.02\n",
      "iteration: 60550 loss: 0.0014 lr: 0.02\n",
      "iteration: 60600 loss: 0.0013 lr: 0.02\n",
      "iteration: 60650 loss: 0.0016 lr: 0.02\n",
      "iteration: 60700 loss: 0.0015 lr: 0.02\n",
      "iteration: 60750 loss: 0.0012 lr: 0.02\n",
      "iteration: 60800 loss: 0.0018 lr: 0.02\n",
      "iteration: 60850 loss: 0.0011 lr: 0.02\n",
      "iteration: 60900 loss: 0.0013 lr: 0.02\n",
      "iteration: 60950 loss: 0.0015 lr: 0.02\n",
      "iteration: 61000 loss: 0.0012 lr: 0.02\n",
      "iteration: 61050 loss: 0.0015 lr: 0.02\n",
      "iteration: 61100 loss: 0.0013 lr: 0.02\n",
      "iteration: 61150 loss: 0.0016 lr: 0.02\n",
      "iteration: 61200 loss: 0.0014 lr: 0.02\n",
      "iteration: 61250 loss: 0.0012 lr: 0.02\n",
      "iteration: 61300 loss: 0.0013 lr: 0.02\n",
      "iteration: 61350 loss: 0.0014 lr: 0.02\n",
      "iteration: 61400 loss: 0.0016 lr: 0.02\n",
      "iteration: 61450 loss: 0.0015 lr: 0.02\n",
      "iteration: 61500 loss: 0.0012 lr: 0.02\n",
      "iteration: 61550 loss: 0.0015 lr: 0.02\n",
      "iteration: 61600 loss: 0.0015 lr: 0.02\n",
      "iteration: 61650 loss: 0.0020 lr: 0.02\n",
      "iteration: 61700 loss: 0.0015 lr: 0.02\n",
      "iteration: 61750 loss: 0.0015 lr: 0.02\n",
      "iteration: 61800 loss: 0.0016 lr: 0.02\n",
      "iteration: 61850 loss: 0.0014 lr: 0.02\n",
      "iteration: 61900 loss: 0.0014 lr: 0.02\n",
      "iteration: 61950 loss: 0.0013 lr: 0.02\n",
      "iteration: 62000 loss: 0.0011 lr: 0.02\n",
      "iteration: 62050 loss: 0.0012 lr: 0.02\n",
      "iteration: 62100 loss: 0.0013 lr: 0.02\n",
      "iteration: 62150 loss: 0.0012 lr: 0.02\n",
      "iteration: 62200 loss: 0.0014 lr: 0.02\n",
      "iteration: 62250 loss: 0.0016 lr: 0.02\n",
      "iteration: 62300 loss: 0.0014 lr: 0.02\n",
      "iteration: 62350 loss: 0.0014 lr: 0.02\n",
      "iteration: 62400 loss: 0.0016 lr: 0.02\n",
      "iteration: 62450 loss: 0.0013 lr: 0.02\n",
      "iteration: 62500 loss: 0.0013 lr: 0.02\n",
      "iteration: 62550 loss: 0.0013 lr: 0.02\n",
      "iteration: 62600 loss: 0.0015 lr: 0.02\n",
      "iteration: 62650 loss: 0.0012 lr: 0.02\n",
      "iteration: 62700 loss: 0.0014 lr: 0.02\n",
      "iteration: 62750 loss: 0.0013 lr: 0.02\n",
      "iteration: 62800 loss: 0.0012 lr: 0.02\n",
      "iteration: 62850 loss: 0.0013 lr: 0.02\n",
      "iteration: 62900 loss: 0.0013 lr: 0.02\n",
      "iteration: 62950 loss: 0.0015 lr: 0.02\n",
      "iteration: 63000 loss: 0.0018 lr: 0.02\n",
      "iteration: 63050 loss: 0.0015 lr: 0.02\n",
      "iteration: 63100 loss: 0.0014 lr: 0.02\n",
      "iteration: 63150 loss: 0.0014 lr: 0.02\n",
      "iteration: 63200 loss: 0.0014 lr: 0.02\n",
      "iteration: 63250 loss: 0.0015 lr: 0.02\n",
      "iteration: 63300 loss: 0.0014 lr: 0.02\n",
      "iteration: 63350 loss: 0.0014 lr: 0.02\n",
      "iteration: 63400 loss: 0.0013 lr: 0.02\n",
      "iteration: 63450 loss: 0.0014 lr: 0.02\n",
      "iteration: 63500 loss: 0.0014 lr: 0.02\n",
      "iteration: 63550 loss: 0.0014 lr: 0.02\n",
      "iteration: 63600 loss: 0.0013 lr: 0.02\n",
      "iteration: 63650 loss: 0.0013 lr: 0.02\n",
      "iteration: 63700 loss: 0.0013 lr: 0.02\n",
      "iteration: 63750 loss: 0.0014 lr: 0.02\n",
      "iteration: 63800 loss: 0.0012 lr: 0.02\n",
      "iteration: 63850 loss: 0.0012 lr: 0.02\n",
      "iteration: 63900 loss: 0.0013 lr: 0.02\n",
      "iteration: 63950 loss: 0.0014 lr: 0.02\n",
      "iteration: 64000 loss: 0.0014 lr: 0.02\n",
      "iteration: 64050 loss: 0.0012 lr: 0.02\n",
      "iteration: 64100 loss: 0.0014 lr: 0.02\n",
      "iteration: 64150 loss: 0.0013 lr: 0.02\n",
      "iteration: 64200 loss: 0.0015 lr: 0.02\n",
      "iteration: 64250 loss: 0.0014 lr: 0.02\n",
      "iteration: 64300 loss: 0.0013 lr: 0.02\n",
      "iteration: 64350 loss: 0.0012 lr: 0.02\n",
      "iteration: 64400 loss: 0.0013 lr: 0.02\n",
      "iteration: 64450 loss: 0.0018 lr: 0.02\n",
      "iteration: 64500 loss: 0.0014 lr: 0.02\n",
      "iteration: 64550 loss: 0.0013 lr: 0.02\n",
      "iteration: 64600 loss: 0.0012 lr: 0.02\n",
      "iteration: 64650 loss: 0.0014 lr: 0.02\n",
      "iteration: 64700 loss: 0.0016 lr: 0.02\n",
      "iteration: 64750 loss: 0.0014 lr: 0.02\n",
      "iteration: 64800 loss: 0.0013 lr: 0.02\n",
      "iteration: 64850 loss: 0.0016 lr: 0.02\n",
      "iteration: 64900 loss: 0.0014 lr: 0.02\n",
      "iteration: 64950 loss: 0.0016 lr: 0.02\n",
      "iteration: 65000 loss: 0.0012 lr: 0.02\n",
      "iteration: 65050 loss: 0.0014 lr: 0.02\n",
      "iteration: 65100 loss: 0.0016 lr: 0.02\n",
      "iteration: 65150 loss: 0.0014 lr: 0.02\n",
      "iteration: 65200 loss: 0.0011 lr: 0.02\n",
      "iteration: 65250 loss: 0.0015 lr: 0.02\n",
      "iteration: 65300 loss: 0.0012 lr: 0.02\n",
      "iteration: 65350 loss: 0.0013 lr: 0.02\n",
      "iteration: 65400 loss: 0.0015 lr: 0.02\n",
      "iteration: 65450 loss: 0.0012 lr: 0.02\n",
      "iteration: 65500 loss: 0.0014 lr: 0.02\n",
      "iteration: 65550 loss: 0.0014 lr: 0.02\n",
      "iteration: 65600 loss: 0.0016 lr: 0.02\n",
      "iteration: 65650 loss: 0.0012 lr: 0.02\n",
      "iteration: 65700 loss: 0.0012 lr: 0.02\n",
      "iteration: 65750 loss: 0.0014 lr: 0.02\n",
      "iteration: 65800 loss: 0.0013 lr: 0.02\n",
      "iteration: 65850 loss: 0.0014 lr: 0.02\n",
      "iteration: 65900 loss: 0.0013 lr: 0.02\n",
      "iteration: 65950 loss: 0.0012 lr: 0.02\n",
      "iteration: 66000 loss: 0.0014 lr: 0.02\n",
      "iteration: 66050 loss: 0.0016 lr: 0.02\n",
      "iteration: 66100 loss: 0.0017 lr: 0.02\n",
      "iteration: 66150 loss: 0.0014 lr: 0.02\n",
      "iteration: 66200 loss: 0.0014 lr: 0.02\n",
      "iteration: 66250 loss: 0.0014 lr: 0.02\n",
      "iteration: 66300 loss: 0.0014 lr: 0.02\n",
      "iteration: 66350 loss: 0.0015 lr: 0.02\n",
      "iteration: 66400 loss: 0.0016 lr: 0.02\n",
      "iteration: 66450 loss: 0.0014 lr: 0.02\n",
      "iteration: 66500 loss: 0.0013 lr: 0.02\n",
      "iteration: 66550 loss: 0.0014 lr: 0.02\n",
      "iteration: 66600 loss: 0.0015 lr: 0.02\n",
      "iteration: 66650 loss: 0.0013 lr: 0.02\n",
      "iteration: 66700 loss: 0.0016 lr: 0.02\n",
      "iteration: 66750 loss: 0.0014 lr: 0.02\n",
      "iteration: 66800 loss: 0.0013 lr: 0.02\n",
      "iteration: 66850 loss: 0.0013 lr: 0.02\n",
      "iteration: 66900 loss: 0.0012 lr: 0.02\n",
      "iteration: 66950 loss: 0.0013 lr: 0.02\n",
      "iteration: 67000 loss: 0.0012 lr: 0.02\n",
      "iteration: 67050 loss: 0.0013 lr: 0.02\n",
      "iteration: 67100 loss: 0.0014 lr: 0.02\n",
      "iteration: 67150 loss: 0.0015 lr: 0.02\n",
      "iteration: 67200 loss: 0.0014 lr: 0.02\n",
      "iteration: 67250 loss: 0.0013 lr: 0.02\n",
      "iteration: 67300 loss: 0.0013 lr: 0.02\n",
      "iteration: 67350 loss: 0.0015 lr: 0.02\n",
      "iteration: 67400 loss: 0.0015 lr: 0.02\n",
      "iteration: 67450 loss: 0.0012 lr: 0.02\n",
      "iteration: 67500 loss: 0.0016 lr: 0.02\n",
      "iteration: 67550 loss: 0.0012 lr: 0.02\n",
      "iteration: 67600 loss: 0.0013 lr: 0.02\n",
      "iteration: 67650 loss: 0.0014 lr: 0.02\n",
      "iteration: 67700 loss: 0.0013 lr: 0.02\n",
      "iteration: 67750 loss: 0.0015 lr: 0.02\n",
      "iteration: 67800 loss: 0.0012 lr: 0.02\n",
      "iteration: 67850 loss: 0.0012 lr: 0.02\n",
      "iteration: 67900 loss: 0.0012 lr: 0.02\n",
      "iteration: 67950 loss: 0.0016 lr: 0.02\n",
      "iteration: 68000 loss: 0.0012 lr: 0.02\n",
      "iteration: 68050 loss: 0.0014 lr: 0.02\n",
      "iteration: 68100 loss: 0.0012 lr: 0.02\n",
      "iteration: 68150 loss: 0.0013 lr: 0.02\n",
      "iteration: 68200 loss: 0.0010 lr: 0.02\n",
      "iteration: 68250 loss: 0.0013 lr: 0.02\n",
      "iteration: 68300 loss: 0.0012 lr: 0.02\n",
      "iteration: 68350 loss: 0.0014 lr: 0.02\n",
      "iteration: 68400 loss: 0.0011 lr: 0.02\n",
      "iteration: 68450 loss: 0.0012 lr: 0.02\n",
      "iteration: 68500 loss: 0.0016 lr: 0.02\n",
      "iteration: 68550 loss: 0.0013 lr: 0.02\n",
      "iteration: 68600 loss: 0.0012 lr: 0.02\n",
      "iteration: 68650 loss: 0.0012 lr: 0.02\n",
      "iteration: 68700 loss: 0.0013 lr: 0.02\n",
      "iteration: 68750 loss: 0.0013 lr: 0.02\n",
      "iteration: 68800 loss: 0.0012 lr: 0.02\n",
      "iteration: 68850 loss: 0.0012 lr: 0.02\n",
      "iteration: 68900 loss: 0.0014 lr: 0.02\n",
      "iteration: 68950 loss: 0.0012 lr: 0.02\n",
      "iteration: 69000 loss: 0.0014 lr: 0.02\n",
      "iteration: 69050 loss: 0.0016 lr: 0.02\n",
      "iteration: 69100 loss: 0.0016 lr: 0.02\n",
      "iteration: 69150 loss: 0.0014 lr: 0.02\n",
      "iteration: 69200 loss: 0.0014 lr: 0.02\n",
      "iteration: 69250 loss: 0.0013 lr: 0.02\n",
      "iteration: 69300 loss: 0.0014 lr: 0.02\n",
      "iteration: 69350 loss: 0.0016 lr: 0.02\n",
      "iteration: 69400 loss: 0.0015 lr: 0.02\n",
      "iteration: 69450 loss: 0.0017 lr: 0.02\n",
      "iteration: 69500 loss: 0.0012 lr: 0.02\n",
      "iteration: 69550 loss: 0.0012 lr: 0.02\n",
      "iteration: 69600 loss: 0.0016 lr: 0.02\n",
      "iteration: 69650 loss: 0.0015 lr: 0.02\n",
      "iteration: 69700 loss: 0.0014 lr: 0.02\n",
      "iteration: 69750 loss: 0.0011 lr: 0.02\n",
      "iteration: 69800 loss: 0.0014 lr: 0.02\n",
      "iteration: 69850 loss: 0.0015 lr: 0.02\n",
      "iteration: 69900 loss: 0.0013 lr: 0.02\n",
      "iteration: 69950 loss: 0.0017 lr: 0.02\n",
      "iteration: 70000 loss: 0.0014 lr: 0.02\n",
      "iteration: 70050 loss: 0.0014 lr: 0.02\n",
      "iteration: 70100 loss: 0.0014 lr: 0.02\n",
      "iteration: 70150 loss: 0.0012 lr: 0.02\n",
      "iteration: 70200 loss: 0.0012 lr: 0.02\n",
      "iteration: 70250 loss: 0.0013 lr: 0.02\n",
      "iteration: 70300 loss: 0.0012 lr: 0.02\n",
      "iteration: 70350 loss: 0.0013 lr: 0.02\n",
      "iteration: 70400 loss: 0.0011 lr: 0.02\n",
      "iteration: 70450 loss: 0.0013 lr: 0.02\n",
      "iteration: 70500 loss: 0.0012 lr: 0.02\n",
      "iteration: 70550 loss: 0.0016 lr: 0.02\n",
      "iteration: 70600 loss: 0.0013 lr: 0.02\n",
      "iteration: 70650 loss: 0.0011 lr: 0.02\n",
      "iteration: 70700 loss: 0.0014 lr: 0.02\n",
      "iteration: 70750 loss: 0.0014 lr: 0.02\n",
      "iteration: 70800 loss: 0.0014 lr: 0.02\n",
      "iteration: 70850 loss: 0.0015 lr: 0.02\n",
      "iteration: 70900 loss: 0.0013 lr: 0.02\n",
      "iteration: 70950 loss: 0.0013 lr: 0.02\n",
      "iteration: 71000 loss: 0.0019 lr: 0.02\n",
      "iteration: 71050 loss: 0.0014 lr: 0.02\n",
      "iteration: 71100 loss: 0.0016 lr: 0.02\n",
      "iteration: 71150 loss: 0.0015 lr: 0.02\n",
      "iteration: 71200 loss: 0.0013 lr: 0.02\n",
      "iteration: 71250 loss: 0.0014 lr: 0.02\n",
      "iteration: 71300 loss: 0.0013 lr: 0.02\n",
      "iteration: 71350 loss: 0.0015 lr: 0.02\n",
      "iteration: 71400 loss: 0.0014 lr: 0.02\n",
      "iteration: 71450 loss: 0.0014 lr: 0.02\n",
      "iteration: 71500 loss: 0.0010 lr: 0.02\n",
      "iteration: 71550 loss: 0.0014 lr: 0.02\n",
      "iteration: 71600 loss: 0.0014 lr: 0.02\n",
      "iteration: 71650 loss: 0.0013 lr: 0.02\n",
      "iteration: 71700 loss: 0.0013 lr: 0.02\n",
      "iteration: 71750 loss: 0.0012 lr: 0.02\n",
      "iteration: 71800 loss: 0.0013 lr: 0.02\n",
      "iteration: 71850 loss: 0.0016 lr: 0.02\n",
      "iteration: 71900 loss: 0.0013 lr: 0.02\n",
      "iteration: 71950 loss: 0.0014 lr: 0.02\n",
      "iteration: 72000 loss: 0.0013 lr: 0.02\n",
      "iteration: 72050 loss: 0.0014 lr: 0.02\n",
      "iteration: 72100 loss: 0.0013 lr: 0.02\n",
      "iteration: 72150 loss: 0.0013 lr: 0.02\n",
      "iteration: 72200 loss: 0.0012 lr: 0.02\n",
      "iteration: 72250 loss: 0.0011 lr: 0.02\n",
      "iteration: 72300 loss: 0.0014 lr: 0.02\n",
      "iteration: 72350 loss: 0.0013 lr: 0.02\n",
      "iteration: 72400 loss: 0.0016 lr: 0.02\n",
      "iteration: 72450 loss: 0.0012 lr: 0.02\n",
      "iteration: 72500 loss: 0.0012 lr: 0.02\n",
      "iteration: 72550 loss: 0.0015 lr: 0.02\n",
      "iteration: 72600 loss: 0.0013 lr: 0.02\n",
      "iteration: 72650 loss: 0.0012 lr: 0.02\n",
      "iteration: 72700 loss: 0.0012 lr: 0.02\n",
      "iteration: 72750 loss: 0.0012 lr: 0.02\n",
      "iteration: 72800 loss: 0.0014 lr: 0.02\n",
      "iteration: 72850 loss: 0.0012 lr: 0.02\n",
      "iteration: 72900 loss: 0.0014 lr: 0.02\n",
      "iteration: 72950 loss: 0.0014 lr: 0.02\n",
      "iteration: 73000 loss: 0.0012 lr: 0.02\n",
      "iteration: 73050 loss: 0.0012 lr: 0.02\n",
      "iteration: 73100 loss: 0.0012 lr: 0.02\n",
      "iteration: 73150 loss: 0.0013 lr: 0.02\n",
      "iteration: 73200 loss: 0.0014 lr: 0.02\n",
      "iteration: 73250 loss: 0.0012 lr: 0.02\n",
      "iteration: 73300 loss: 0.0012 lr: 0.02\n",
      "iteration: 73350 loss: 0.0012 lr: 0.02\n",
      "iteration: 73400 loss: 0.0014 lr: 0.02\n",
      "iteration: 73450 loss: 0.0016 lr: 0.02\n",
      "iteration: 73500 loss: 0.0011 lr: 0.02\n",
      "iteration: 73550 loss: 0.0011 lr: 0.02\n",
      "iteration: 73600 loss: 0.0014 lr: 0.02\n",
      "iteration: 73650 loss: 0.0011 lr: 0.02\n",
      "iteration: 73700 loss: 0.0014 lr: 0.02\n",
      "iteration: 73750 loss: 0.0012 lr: 0.02\n",
      "iteration: 73800 loss: 0.0012 lr: 0.02\n",
      "iteration: 73850 loss: 0.0011 lr: 0.02\n",
      "iteration: 73900 loss: 0.0011 lr: 0.02\n",
      "iteration: 73950 loss: 0.0012 lr: 0.02\n",
      "iteration: 74000 loss: 0.0012 lr: 0.02\n",
      "iteration: 74050 loss: 0.0014 lr: 0.02\n",
      "iteration: 74100 loss: 0.0013 lr: 0.02\n",
      "iteration: 74150 loss: 0.0012 lr: 0.02\n",
      "iteration: 74200 loss: 0.0013 lr: 0.02\n",
      "iteration: 74250 loss: 0.0013 lr: 0.02\n",
      "iteration: 74300 loss: 0.0012 lr: 0.02\n",
      "iteration: 74350 loss: 0.0012 lr: 0.02\n",
      "iteration: 74400 loss: 0.0013 lr: 0.02\n",
      "iteration: 74450 loss: 0.0015 lr: 0.02\n",
      "iteration: 74500 loss: 0.0014 lr: 0.02\n",
      "iteration: 74550 loss: 0.0013 lr: 0.02\n",
      "iteration: 74600 loss: 0.0020 lr: 0.02\n",
      "iteration: 74650 loss: 0.0015 lr: 0.02\n",
      "iteration: 74700 loss: 0.0013 lr: 0.02\n",
      "iteration: 74750 loss: 0.0013 lr: 0.02\n",
      "iteration: 74800 loss: 0.0012 lr: 0.02\n",
      "iteration: 74850 loss: 0.0013 lr: 0.02\n",
      "iteration: 74900 loss: 0.0012 lr: 0.02\n",
      "iteration: 74950 loss: 0.0012 lr: 0.02\n",
      "iteration: 75000 loss: 0.0010 lr: 0.02\n",
      "iteration: 75050 loss: 0.0013 lr: 0.02\n",
      "iteration: 75100 loss: 0.0014 lr: 0.02\n",
      "iteration: 75150 loss: 0.0015 lr: 0.02\n",
      "iteration: 75200 loss: 0.0012 lr: 0.02\n",
      "iteration: 75250 loss: 0.0014 lr: 0.02\n",
      "iteration: 75300 loss: 0.0012 lr: 0.02\n",
      "iteration: 75350 loss: 0.0015 lr: 0.02\n",
      "iteration: 75400 loss: 0.0012 lr: 0.02\n",
      "iteration: 75450 loss: 0.0015 lr: 0.02\n",
      "iteration: 75500 loss: 0.0015 lr: 0.02\n",
      "iteration: 75550 loss: 0.0012 lr: 0.02\n",
      "iteration: 75600 loss: 0.0013 lr: 0.02\n",
      "iteration: 75650 loss: 0.0013 lr: 0.02\n",
      "iteration: 75700 loss: 0.0014 lr: 0.02\n",
      "iteration: 75750 loss: 0.0013 lr: 0.02\n",
      "iteration: 75800 loss: 0.0013 lr: 0.02\n",
      "iteration: 75850 loss: 0.0014 lr: 0.02\n",
      "iteration: 75900 loss: 0.0015 lr: 0.02\n",
      "iteration: 75950 loss: 0.0013 lr: 0.02\n",
      "iteration: 76000 loss: 0.0013 lr: 0.02\n",
      "iteration: 76050 loss: 0.0015 lr: 0.02\n",
      "iteration: 76100 loss: 0.0016 lr: 0.02\n",
      "iteration: 76150 loss: 0.0012 lr: 0.02\n",
      "iteration: 76200 loss: 0.0011 lr: 0.02\n",
      "iteration: 76250 loss: 0.0014 lr: 0.02\n",
      "iteration: 76300 loss: 0.0019 lr: 0.02\n",
      "iteration: 76350 loss: 0.0015 lr: 0.02\n",
      "iteration: 76400 loss: 0.0011 lr: 0.02\n",
      "iteration: 76450 loss: 0.0013 lr: 0.02\n",
      "iteration: 76500 loss: 0.0014 lr: 0.02\n",
      "iteration: 76550 loss: 0.0015 lr: 0.02\n",
      "iteration: 76600 loss: 0.0012 lr: 0.02\n",
      "iteration: 76650 loss: 0.0012 lr: 0.02\n",
      "iteration: 76700 loss: 0.0013 lr: 0.02\n",
      "iteration: 76750 loss: 0.0015 lr: 0.02\n",
      "iteration: 76800 loss: 0.0013 lr: 0.02\n",
      "iteration: 76850 loss: 0.0011 lr: 0.02\n",
      "iteration: 76900 loss: 0.0012 lr: 0.02\n",
      "iteration: 76950 loss: 0.0012 lr: 0.02\n",
      "iteration: 77000 loss: 0.0013 lr: 0.02\n",
      "iteration: 77050 loss: 0.0014 lr: 0.02\n",
      "iteration: 77100 loss: 0.0014 lr: 0.02\n",
      "iteration: 77150 loss: 0.0014 lr: 0.02\n",
      "iteration: 77200 loss: 0.0013 lr: 0.02\n",
      "iteration: 77250 loss: 0.0012 lr: 0.02\n",
      "iteration: 77300 loss: 0.0016 lr: 0.02\n",
      "iteration: 77350 loss: 0.0012 lr: 0.02\n",
      "iteration: 77400 loss: 0.0011 lr: 0.02\n",
      "iteration: 77450 loss: 0.0014 lr: 0.02\n",
      "iteration: 77500 loss: 0.0013 lr: 0.02\n",
      "iteration: 77550 loss: 0.0015 lr: 0.02\n",
      "iteration: 77600 loss: 0.0013 lr: 0.02\n",
      "iteration: 77650 loss: 0.0014 lr: 0.02\n",
      "iteration: 77700 loss: 0.0014 lr: 0.02\n",
      "iteration: 77750 loss: 0.0014 lr: 0.02\n",
      "iteration: 77800 loss: 0.0011 lr: 0.02\n",
      "iteration: 77850 loss: 0.0012 lr: 0.02\n",
      "iteration: 77900 loss: 0.0013 lr: 0.02\n",
      "iteration: 77950 loss: 0.0011 lr: 0.02\n",
      "iteration: 78000 loss: 0.0014 lr: 0.02\n",
      "iteration: 78050 loss: 0.0013 lr: 0.02\n",
      "iteration: 78100 loss: 0.0013 lr: 0.02\n",
      "iteration: 78150 loss: 0.0010 lr: 0.02\n",
      "iteration: 78200 loss: 0.0012 lr: 0.02\n",
      "iteration: 78250 loss: 0.0012 lr: 0.02\n",
      "iteration: 78300 loss: 0.0014 lr: 0.02\n",
      "iteration: 78350 loss: 0.0013 lr: 0.02\n",
      "iteration: 78400 loss: 0.0014 lr: 0.02\n",
      "iteration: 78450 loss: 0.0012 lr: 0.02\n",
      "iteration: 78500 loss: 0.0015 lr: 0.02\n",
      "iteration: 78550 loss: 0.0011 lr: 0.02\n",
      "iteration: 78600 loss: 0.0012 lr: 0.02\n",
      "iteration: 78650 loss: 0.0012 lr: 0.02\n",
      "iteration: 78700 loss: 0.0012 lr: 0.02\n",
      "iteration: 78750 loss: 0.0015 lr: 0.02\n",
      "iteration: 78800 loss: 0.0013 lr: 0.02\n",
      "iteration: 78850 loss: 0.0012 lr: 0.02\n",
      "iteration: 78900 loss: 0.0015 lr: 0.02\n",
      "iteration: 78950 loss: 0.0012 lr: 0.02\n",
      "iteration: 79000 loss: 0.0013 lr: 0.02\n",
      "iteration: 79050 loss: 0.0014 lr: 0.02\n",
      "iteration: 79100 loss: 0.0014 lr: 0.02\n",
      "iteration: 79150 loss: 0.0011 lr: 0.02\n",
      "iteration: 79200 loss: 0.0014 lr: 0.02\n",
      "iteration: 79250 loss: 0.0014 lr: 0.02\n",
      "iteration: 79300 loss: 0.0012 lr: 0.02\n",
      "iteration: 79350 loss: 0.0011 lr: 0.02\n",
      "iteration: 79400 loss: 0.0012 lr: 0.02\n",
      "iteration: 79450 loss: 0.0014 lr: 0.02\n",
      "iteration: 79500 loss: 0.0012 lr: 0.02\n",
      "iteration: 79550 loss: 0.0015 lr: 0.02\n",
      "iteration: 79600 loss: 0.0014 lr: 0.02\n",
      "iteration: 79650 loss: 0.0012 lr: 0.02\n",
      "iteration: 79700 loss: 0.0012 lr: 0.02\n",
      "iteration: 79750 loss: 0.0012 lr: 0.02\n",
      "iteration: 79800 loss: 0.0012 lr: 0.02\n",
      "iteration: 79850 loss: 0.0012 lr: 0.02\n",
      "iteration: 79900 loss: 0.0013 lr: 0.02\n",
      "iteration: 79950 loss: 0.0013 lr: 0.02\n",
      "iteration: 80000 loss: 0.0012 lr: 0.02\n",
      "iteration: 80050 loss: 0.0011 lr: 0.02\n",
      "iteration: 80100 loss: 0.0013 lr: 0.02\n",
      "iteration: 80150 loss: 0.0012 lr: 0.02\n",
      "iteration: 80200 loss: 0.0011 lr: 0.02\n",
      "iteration: 80250 loss: 0.0012 lr: 0.02\n",
      "iteration: 80300 loss: 0.0012 lr: 0.02\n",
      "iteration: 80350 loss: 0.0016 lr: 0.02\n",
      "iteration: 80400 loss: 0.0013 lr: 0.02\n",
      "iteration: 80450 loss: 0.0013 lr: 0.02\n",
      "iteration: 80500 loss: 0.0016 lr: 0.02\n",
      "iteration: 80550 loss: 0.0013 lr: 0.02\n",
      "iteration: 80600 loss: 0.0012 lr: 0.02\n",
      "iteration: 80650 loss: 0.0011 lr: 0.02\n",
      "iteration: 80700 loss: 0.0013 lr: 0.02\n",
      "iteration: 80750 loss: 0.0014 lr: 0.02\n",
      "iteration: 80800 loss: 0.0011 lr: 0.02\n",
      "iteration: 80850 loss: 0.0014 lr: 0.02\n",
      "iteration: 80900 loss: 0.0011 lr: 0.02\n",
      "iteration: 80950 loss: 0.0014 lr: 0.02\n",
      "iteration: 81000 loss: 0.0012 lr: 0.02\n",
      "iteration: 81050 loss: 0.0011 lr: 0.02\n",
      "iteration: 81100 loss: 0.0013 lr: 0.02\n",
      "iteration: 81150 loss: 0.0012 lr: 0.02\n",
      "iteration: 81200 loss: 0.0014 lr: 0.02\n",
      "iteration: 81250 loss: 0.0012 lr: 0.02\n",
      "iteration: 81300 loss: 0.0013 lr: 0.02\n",
      "iteration: 81350 loss: 0.0011 lr: 0.02\n",
      "iteration: 81400 loss: 0.0013 lr: 0.02\n",
      "iteration: 81450 loss: 0.0014 lr: 0.02\n",
      "iteration: 81500 loss: 0.0012 lr: 0.02\n",
      "iteration: 81550 loss: 0.0012 lr: 0.02\n",
      "iteration: 81600 loss: 0.0012 lr: 0.02\n",
      "iteration: 81650 loss: 0.0011 lr: 0.02\n",
      "iteration: 81700 loss: 0.0012 lr: 0.02\n",
      "iteration: 81750 loss: 0.0014 lr: 0.02\n",
      "iteration: 81800 loss: 0.0013 lr: 0.02\n",
      "iteration: 81850 loss: 0.0013 lr: 0.02\n",
      "iteration: 81900 loss: 0.0015 lr: 0.02\n",
      "iteration: 81950 loss: 0.0012 lr: 0.02\n",
      "iteration: 82000 loss: 0.0013 lr: 0.02\n",
      "iteration: 82050 loss: 0.0015 lr: 0.02\n",
      "iteration: 82100 loss: 0.0011 lr: 0.02\n",
      "iteration: 82150 loss: 0.0012 lr: 0.02\n",
      "iteration: 82200 loss: 0.0010 lr: 0.02\n",
      "iteration: 82250 loss: 0.0011 lr: 0.02\n",
      "iteration: 82300 loss: 0.0012 lr: 0.02\n",
      "iteration: 82350 loss: 0.0014 lr: 0.02\n",
      "iteration: 82400 loss: 0.0015 lr: 0.02\n",
      "iteration: 82450 loss: 0.0012 lr: 0.02\n",
      "iteration: 82500 loss: 0.0016 lr: 0.02\n",
      "iteration: 82550 loss: 0.0012 lr: 0.02\n",
      "iteration: 82600 loss: 0.0013 lr: 0.02\n",
      "iteration: 82650 loss: 0.0012 lr: 0.02\n",
      "iteration: 82700 loss: 0.0014 lr: 0.02\n",
      "iteration: 82750 loss: 0.0014 lr: 0.02\n",
      "iteration: 82800 loss: 0.0013 lr: 0.02\n",
      "iteration: 82850 loss: 0.0013 lr: 0.02\n",
      "iteration: 82900 loss: 0.0013 lr: 0.02\n",
      "iteration: 82950 loss: 0.0017 lr: 0.02\n",
      "iteration: 83000 loss: 0.0013 lr: 0.02\n",
      "iteration: 83050 loss: 0.0010 lr: 0.02\n",
      "iteration: 83100 loss: 0.0012 lr: 0.02\n",
      "iteration: 83150 loss: 0.0014 lr: 0.02\n",
      "iteration: 83200 loss: 0.0011 lr: 0.02\n",
      "iteration: 83250 loss: 0.0013 lr: 0.02\n",
      "iteration: 83300 loss: 0.0013 lr: 0.02\n",
      "iteration: 83350 loss: 0.0013 lr: 0.02\n",
      "iteration: 83400 loss: 0.0012 lr: 0.02\n",
      "iteration: 83450 loss: 0.0015 lr: 0.02\n",
      "iteration: 83500 loss: 0.0012 lr: 0.02\n",
      "iteration: 83550 loss: 0.0012 lr: 0.02\n",
      "iteration: 83600 loss: 0.0014 lr: 0.02\n",
      "iteration: 83650 loss: 0.0012 lr: 0.02\n",
      "iteration: 83700 loss: 0.0014 lr: 0.02\n",
      "iteration: 83750 loss: 0.0015 lr: 0.02\n",
      "iteration: 83800 loss: 0.0012 lr: 0.02\n",
      "iteration: 83850 loss: 0.0014 lr: 0.02\n",
      "iteration: 83900 loss: 0.0013 lr: 0.02\n",
      "iteration: 83950 loss: 0.0011 lr: 0.02\n",
      "iteration: 84000 loss: 0.0014 lr: 0.02\n",
      "iteration: 84050 loss: 0.0013 lr: 0.02\n",
      "iteration: 84100 loss: 0.0012 lr: 0.02\n",
      "iteration: 84150 loss: 0.0013 lr: 0.02\n",
      "iteration: 84200 loss: 0.0011 lr: 0.02\n",
      "iteration: 84250 loss: 0.0012 lr: 0.02\n",
      "iteration: 84300 loss: 0.0014 lr: 0.02\n",
      "iteration: 84350 loss: 0.0012 lr: 0.02\n",
      "iteration: 84400 loss: 0.0011 lr: 0.02\n",
      "iteration: 84450 loss: 0.0015 lr: 0.02\n",
      "iteration: 84500 loss: 0.0011 lr: 0.02\n",
      "iteration: 84550 loss: 0.0013 lr: 0.02\n",
      "iteration: 84600 loss: 0.0011 lr: 0.02\n",
      "iteration: 84650 loss: 0.0012 lr: 0.02\n",
      "iteration: 84700 loss: 0.0013 lr: 0.02\n",
      "iteration: 84750 loss: 0.0012 lr: 0.02\n",
      "iteration: 84800 loss: 0.0013 lr: 0.02\n",
      "iteration: 84850 loss: 0.0013 lr: 0.02\n",
      "iteration: 84900 loss: 0.0011 lr: 0.02\n",
      "iteration: 84950 loss: 0.0011 lr: 0.02\n",
      "iteration: 85000 loss: 0.0012 lr: 0.02\n",
      "iteration: 85050 loss: 0.0012 lr: 0.02\n",
      "iteration: 85100 loss: 0.0012 lr: 0.02\n",
      "iteration: 85150 loss: 0.0011 lr: 0.02\n",
      "iteration: 85200 loss: 0.0013 lr: 0.02\n",
      "iteration: 85250 loss: 0.0014 lr: 0.02\n",
      "iteration: 85300 loss: 0.0013 lr: 0.02\n",
      "iteration: 85350 loss: 0.0013 lr: 0.02\n",
      "iteration: 85400 loss: 0.0013 lr: 0.02\n",
      "iteration: 85450 loss: 0.0012 lr: 0.02\n",
      "iteration: 85500 loss: 0.0015 lr: 0.02\n",
      "iteration: 85550 loss: 0.0015 lr: 0.02\n",
      "iteration: 85600 loss: 0.0013 lr: 0.02\n",
      "iteration: 85650 loss: 0.0012 lr: 0.02\n",
      "iteration: 85700 loss: 0.0013 lr: 0.02\n",
      "iteration: 85750 loss: 0.0013 lr: 0.02\n",
      "iteration: 85800 loss: 0.0013 lr: 0.02\n",
      "iteration: 85850 loss: 0.0015 lr: 0.02\n",
      "iteration: 85900 loss: 0.0013 lr: 0.02\n",
      "iteration: 85950 loss: 0.0012 lr: 0.02\n",
      "iteration: 86000 loss: 0.0014 lr: 0.02\n",
      "iteration: 86050 loss: 0.0012 lr: 0.02\n",
      "iteration: 86100 loss: 0.0013 lr: 0.02\n",
      "iteration: 86150 loss: 0.0010 lr: 0.02\n",
      "iteration: 86200 loss: 0.0012 lr: 0.02\n",
      "iteration: 86250 loss: 0.0012 lr: 0.02\n",
      "iteration: 86300 loss: 0.0014 lr: 0.02\n",
      "iteration: 86350 loss: 0.0016 lr: 0.02\n",
      "iteration: 86400 loss: 0.0013 lr: 0.02\n",
      "iteration: 86450 loss: 0.0012 lr: 0.02\n",
      "iteration: 86500 loss: 0.0012 lr: 0.02\n",
      "iteration: 86550 loss: 0.0011 lr: 0.02\n",
      "iteration: 86600 loss: 0.0016 lr: 0.02\n",
      "iteration: 86650 loss: 0.0014 lr: 0.02\n",
      "iteration: 86700 loss: 0.0011 lr: 0.02\n",
      "iteration: 86750 loss: 0.0013 lr: 0.02\n",
      "iteration: 86800 loss: 0.0016 lr: 0.02\n",
      "iteration: 86850 loss: 0.0014 lr: 0.02\n",
      "iteration: 86900 loss: 0.0014 lr: 0.02\n",
      "iteration: 86950 loss: 0.0014 lr: 0.02\n",
      "iteration: 87000 loss: 0.0011 lr: 0.02\n",
      "iteration: 87050 loss: 0.0013 lr: 0.02\n",
      "iteration: 87100 loss: 0.0016 lr: 0.02\n",
      "iteration: 87150 loss: 0.0012 lr: 0.02\n",
      "iteration: 87200 loss: 0.0013 lr: 0.02\n",
      "iteration: 87250 loss: 0.0014 lr: 0.02\n",
      "iteration: 87300 loss: 0.0011 lr: 0.02\n",
      "iteration: 87350 loss: 0.0013 lr: 0.02\n",
      "iteration: 87400 loss: 0.0013 lr: 0.02\n",
      "iteration: 87450 loss: 0.0012 lr: 0.02\n",
      "iteration: 87500 loss: 0.0012 lr: 0.02\n",
      "iteration: 87550 loss: 0.0012 lr: 0.02\n",
      "iteration: 87600 loss: 0.0013 lr: 0.02\n",
      "iteration: 87650 loss: 0.0010 lr: 0.02\n",
      "iteration: 87700 loss: 0.0013 lr: 0.02\n",
      "iteration: 87750 loss: 0.0012 lr: 0.02\n",
      "iteration: 87800 loss: 0.0013 lr: 0.02\n",
      "iteration: 87850 loss: 0.0014 lr: 0.02\n",
      "iteration: 87900 loss: 0.0016 lr: 0.02\n",
      "iteration: 87950 loss: 0.0014 lr: 0.02\n",
      "iteration: 88000 loss: 0.0013 lr: 0.02\n",
      "iteration: 88050 loss: 0.0013 lr: 0.02\n",
      "iteration: 88100 loss: 0.0012 lr: 0.02\n",
      "iteration: 88150 loss: 0.0011 lr: 0.02\n",
      "iteration: 88200 loss: 0.0010 lr: 0.02\n",
      "iteration: 88250 loss: 0.0013 lr: 0.02\n",
      "iteration: 88300 loss: 0.0012 lr: 0.02\n",
      "iteration: 88350 loss: 0.0013 lr: 0.02\n",
      "iteration: 88400 loss: 0.0012 lr: 0.02\n",
      "iteration: 88450 loss: 0.0012 lr: 0.02\n",
      "iteration: 88500 loss: 0.0013 lr: 0.02\n",
      "iteration: 88550 loss: 0.0011 lr: 0.02\n",
      "iteration: 88600 loss: 0.0013 lr: 0.02\n",
      "iteration: 88650 loss: 0.0013 lr: 0.02\n",
      "iteration: 88700 loss: 0.0012 lr: 0.02\n",
      "iteration: 88750 loss: 0.0012 lr: 0.02\n",
      "iteration: 88800 loss: 0.0011 lr: 0.02\n",
      "iteration: 88850 loss: 0.0012 lr: 0.02\n",
      "iteration: 88900 loss: 0.0011 lr: 0.02\n",
      "iteration: 88950 loss: 0.0012 lr: 0.02\n",
      "iteration: 89000 loss: 0.0015 lr: 0.02\n",
      "iteration: 89050 loss: 0.0013 lr: 0.02\n",
      "iteration: 89100 loss: 0.0012 lr: 0.02\n",
      "iteration: 89150 loss: 0.0012 lr: 0.02\n",
      "iteration: 89200 loss: 0.0015 lr: 0.02\n",
      "iteration: 89250 loss: 0.0014 lr: 0.02\n",
      "iteration: 89300 loss: 0.0013 lr: 0.02\n",
      "iteration: 89350 loss: 0.0014 lr: 0.02\n",
      "iteration: 89400 loss: 0.0014 lr: 0.02\n",
      "iteration: 89450 loss: 0.0015 lr: 0.02\n",
      "iteration: 89500 loss: 0.0013 lr: 0.02\n",
      "iteration: 89550 loss: 0.0014 lr: 0.02\n",
      "iteration: 89600 loss: 0.0015 lr: 0.02\n",
      "iteration: 89650 loss: 0.0012 lr: 0.02\n",
      "iteration: 89700 loss: 0.0012 lr: 0.02\n",
      "iteration: 89750 loss: 0.0011 lr: 0.02\n",
      "iteration: 89800 loss: 0.0012 lr: 0.02\n",
      "iteration: 89850 loss: 0.0011 lr: 0.02\n",
      "iteration: 89900 loss: 0.0012 lr: 0.02\n",
      "iteration: 89950 loss: 0.0012 lr: 0.02\n",
      "iteration: 90000 loss: 0.0011 lr: 0.02\n",
      "iteration: 90050 loss: 0.0016 lr: 0.02\n",
      "iteration: 90100 loss: 0.0012 lr: 0.02\n",
      "iteration: 90150 loss: 0.0016 lr: 0.02\n",
      "iteration: 90200 loss: 0.0011 lr: 0.02\n",
      "iteration: 90250 loss: 0.0012 lr: 0.02\n",
      "iteration: 90300 loss: 0.0010 lr: 0.02\n",
      "iteration: 90350 loss: 0.0010 lr: 0.02\n",
      "iteration: 90400 loss: 0.0013 lr: 0.02\n",
      "iteration: 90450 loss: 0.0013 lr: 0.02\n",
      "iteration: 90500 loss: 0.0013 lr: 0.02\n",
      "iteration: 90550 loss: 0.0014 lr: 0.02\n",
      "iteration: 90600 loss: 0.0011 lr: 0.02\n",
      "iteration: 90650 loss: 0.0013 lr: 0.02\n",
      "iteration: 90700 loss: 0.0014 lr: 0.02\n",
      "iteration: 90750 loss: 0.0010 lr: 0.02\n",
      "iteration: 90800 loss: 0.0012 lr: 0.02\n",
      "iteration: 90850 loss: 0.0012 lr: 0.02\n",
      "iteration: 90900 loss: 0.0012 lr: 0.02\n",
      "iteration: 90950 loss: 0.0013 lr: 0.02\n",
      "iteration: 91000 loss: 0.0012 lr: 0.02\n",
      "iteration: 91050 loss: 0.0014 lr: 0.02\n",
      "iteration: 91100 loss: 0.0014 lr: 0.02\n",
      "iteration: 91150 loss: 0.0011 lr: 0.02\n",
      "iteration: 91200 loss: 0.0014 lr: 0.02\n",
      "iteration: 91250 loss: 0.0015 lr: 0.02\n",
      "iteration: 91300 loss: 0.0012 lr: 0.02\n",
      "iteration: 91350 loss: 0.0012 lr: 0.02\n",
      "iteration: 91400 loss: 0.0012 lr: 0.02\n",
      "iteration: 91450 loss: 0.0012 lr: 0.02\n",
      "iteration: 91500 loss: 0.0013 lr: 0.02\n",
      "iteration: 91550 loss: 0.0012 lr: 0.02\n",
      "iteration: 91600 loss: 0.0012 lr: 0.02\n",
      "iteration: 91650 loss: 0.0014 lr: 0.02\n",
      "iteration: 91700 loss: 0.0018 lr: 0.02\n",
      "iteration: 91750 loss: 0.0013 lr: 0.02\n",
      "iteration: 91800 loss: 0.0012 lr: 0.02\n",
      "iteration: 91850 loss: 0.0012 lr: 0.02\n",
      "iteration: 91900 loss: 0.0012 lr: 0.02\n",
      "iteration: 91950 loss: 0.0012 lr: 0.02\n",
      "iteration: 92000 loss: 0.0013 lr: 0.02\n",
      "iteration: 92050 loss: 0.0012 lr: 0.02\n",
      "iteration: 92100 loss: 0.0012 lr: 0.02\n",
      "iteration: 92150 loss: 0.0013 lr: 0.02\n",
      "iteration: 92200 loss: 0.0013 lr: 0.02\n",
      "iteration: 92250 loss: 0.0010 lr: 0.02\n",
      "iteration: 92300 loss: 0.0012 lr: 0.02\n",
      "iteration: 92350 loss: 0.0014 lr: 0.02\n",
      "iteration: 92400 loss: 0.0013 lr: 0.02\n",
      "iteration: 92450 loss: 0.0015 lr: 0.02\n",
      "iteration: 92500 loss: 0.0012 lr: 0.02\n",
      "iteration: 92550 loss: 0.0012 lr: 0.02\n",
      "iteration: 92600 loss: 0.0012 lr: 0.02\n",
      "iteration: 92650 loss: 0.0014 lr: 0.02\n",
      "iteration: 92700 loss: 0.0013 lr: 0.02\n",
      "iteration: 92750 loss: 0.0010 lr: 0.02\n",
      "iteration: 92800 loss: 0.0013 lr: 0.02\n",
      "iteration: 92850 loss: 0.0011 lr: 0.02\n",
      "iteration: 92900 loss: 0.0012 lr: 0.02\n",
      "iteration: 92950 loss: 0.0015 lr: 0.02\n",
      "iteration: 93000 loss: 0.0014 lr: 0.02\n",
      "iteration: 93050 loss: 0.0011 lr: 0.02\n",
      "iteration: 93100 loss: 0.0011 lr: 0.02\n",
      "iteration: 93150 loss: 0.0011 lr: 0.02\n",
      "iteration: 93200 loss: 0.0014 lr: 0.02\n",
      "iteration: 93250 loss: 0.0011 lr: 0.02\n",
      "iteration: 93300 loss: 0.0012 lr: 0.02\n",
      "iteration: 93350 loss: 0.0011 lr: 0.02\n",
      "iteration: 93400 loss: 0.0013 lr: 0.02\n",
      "iteration: 93450 loss: 0.0011 lr: 0.02\n",
      "iteration: 93500 loss: 0.0012 lr: 0.02\n",
      "iteration: 93550 loss: 0.0011 lr: 0.02\n",
      "iteration: 93600 loss: 0.0014 lr: 0.02\n",
      "iteration: 93650 loss: 0.0011 lr: 0.02\n",
      "iteration: 93700 loss: 0.0014 lr: 0.02\n",
      "iteration: 93750 loss: 0.0014 lr: 0.02\n",
      "iteration: 93800 loss: 0.0012 lr: 0.02\n",
      "iteration: 93850 loss: 0.0012 lr: 0.02\n",
      "iteration: 93900 loss: 0.0012 lr: 0.02\n",
      "iteration: 93950 loss: 0.0012 lr: 0.02\n",
      "iteration: 94000 loss: 0.0014 lr: 0.02\n",
      "iteration: 94050 loss: 0.0013 lr: 0.02\n",
      "iteration: 94100 loss: 0.0011 lr: 0.02\n",
      "iteration: 94150 loss: 0.0012 lr: 0.02\n",
      "iteration: 94200 loss: 0.0014 lr: 0.02\n",
      "iteration: 94250 loss: 0.0013 lr: 0.02\n",
      "iteration: 94300 loss: 0.0012 lr: 0.02\n",
      "iteration: 94350 loss: 0.0015 lr: 0.02\n",
      "iteration: 94400 loss: 0.0010 lr: 0.02\n",
      "iteration: 94450 loss: 0.0012 lr: 0.02\n",
      "iteration: 94500 loss: 0.0011 lr: 0.02\n",
      "iteration: 94550 loss: 0.0013 lr: 0.02\n",
      "iteration: 94600 loss: 0.0013 lr: 0.02\n",
      "iteration: 94650 loss: 0.0013 lr: 0.02\n",
      "iteration: 94700 loss: 0.0013 lr: 0.02\n",
      "iteration: 94750 loss: 0.0012 lr: 0.02\n",
      "iteration: 94800 loss: 0.0011 lr: 0.02\n",
      "iteration: 94850 loss: 0.0012 lr: 0.02\n",
      "iteration: 94900 loss: 0.0011 lr: 0.02\n",
      "iteration: 94950 loss: 0.0010 lr: 0.02\n",
      "iteration: 95000 loss: 0.0012 lr: 0.02\n",
      "iteration: 95050 loss: 0.0012 lr: 0.02\n",
      "iteration: 95100 loss: 0.0012 lr: 0.02\n",
      "iteration: 95150 loss: 0.0013 lr: 0.02\n",
      "iteration: 95200 loss: 0.0011 lr: 0.02\n",
      "iteration: 95250 loss: 0.0012 lr: 0.02\n",
      "iteration: 95300 loss: 0.0010 lr: 0.02\n",
      "iteration: 95350 loss: 0.0012 lr: 0.02\n",
      "iteration: 95400 loss: 0.0012 lr: 0.02\n",
      "iteration: 95450 loss: 0.0011 lr: 0.02\n",
      "iteration: 95500 loss: 0.0011 lr: 0.02\n",
      "iteration: 95550 loss: 0.0012 lr: 0.02\n",
      "iteration: 95600 loss: 0.0011 lr: 0.02\n",
      "iteration: 95650 loss: 0.0011 lr: 0.02\n",
      "iteration: 95700 loss: 0.0012 lr: 0.02\n",
      "iteration: 95750 loss: 0.0012 lr: 0.02\n",
      "iteration: 95800 loss: 0.0015 lr: 0.02\n",
      "iteration: 95850 loss: 0.0013 lr: 0.02\n",
      "iteration: 95900 loss: 0.0011 lr: 0.02\n",
      "iteration: 95950 loss: 0.0012 lr: 0.02\n",
      "iteration: 96000 loss: 0.0013 lr: 0.02\n",
      "iteration: 96050 loss: 0.0015 lr: 0.02\n",
      "iteration: 96100 loss: 0.0015 lr: 0.02\n",
      "iteration: 96150 loss: 0.0010 lr: 0.02\n",
      "iteration: 96200 loss: 0.0011 lr: 0.02\n",
      "iteration: 96250 loss: 0.0014 lr: 0.02\n",
      "iteration: 96300 loss: 0.0012 lr: 0.02\n",
      "iteration: 96350 loss: 0.0013 lr: 0.02\n",
      "iteration: 96400 loss: 0.0012 lr: 0.02\n",
      "iteration: 96450 loss: 0.0011 lr: 0.02\n",
      "iteration: 96500 loss: 0.0013 lr: 0.02\n",
      "iteration: 96550 loss: 0.0013 lr: 0.02\n",
      "iteration: 96600 loss: 0.0011 lr: 0.02\n",
      "iteration: 96650 loss: 0.0012 lr: 0.02\n",
      "iteration: 96700 loss: 0.0012 lr: 0.02\n",
      "iteration: 96750 loss: 0.0011 lr: 0.02\n",
      "iteration: 96800 loss: 0.0013 lr: 0.02\n",
      "iteration: 96850 loss: 0.0011 lr: 0.02\n",
      "iteration: 96900 loss: 0.0014 lr: 0.02\n",
      "iteration: 96950 loss: 0.0016 lr: 0.02\n",
      "iteration: 97000 loss: 0.0012 lr: 0.02\n",
      "iteration: 97050 loss: 0.0010 lr: 0.02\n",
      "iteration: 97100 loss: 0.0011 lr: 0.02\n",
      "iteration: 97150 loss: 0.0014 lr: 0.02\n",
      "iteration: 97200 loss: 0.0010 lr: 0.02\n",
      "iteration: 97250 loss: 0.0014 lr: 0.02\n",
      "iteration: 97300 loss: 0.0012 lr: 0.02\n",
      "iteration: 97350 loss: 0.0011 lr: 0.02\n",
      "iteration: 97400 loss: 0.0011 lr: 0.02\n",
      "iteration: 97450 loss: 0.0011 lr: 0.02\n",
      "iteration: 97500 loss: 0.0013 lr: 0.02\n",
      "iteration: 97550 loss: 0.0010 lr: 0.02\n",
      "iteration: 97600 loss: 0.0011 lr: 0.02\n",
      "iteration: 97650 loss: 0.0012 lr: 0.02\n",
      "iteration: 97700 loss: 0.0014 lr: 0.02\n",
      "iteration: 97750 loss: 0.0011 lr: 0.02\n",
      "iteration: 97800 loss: 0.0015 lr: 0.02\n",
      "iteration: 97850 loss: 0.0011 lr: 0.02\n",
      "iteration: 97900 loss: 0.0011 lr: 0.02\n",
      "iteration: 97950 loss: 0.0012 lr: 0.02\n",
      "iteration: 98000 loss: 0.0013 lr: 0.02\n",
      "iteration: 98050 loss: 0.0015 lr: 0.02\n",
      "iteration: 98100 loss: 0.0012 lr: 0.02\n",
      "iteration: 98150 loss: 0.0015 lr: 0.02\n",
      "iteration: 98200 loss: 0.0012 lr: 0.02\n",
      "iteration: 98250 loss: 0.0015 lr: 0.02\n",
      "iteration: 98300 loss: 0.0010 lr: 0.02\n",
      "iteration: 98350 loss: 0.0011 lr: 0.02\n",
      "iteration: 98400 loss: 0.0017 lr: 0.02\n",
      "iteration: 98450 loss: 0.0014 lr: 0.02\n",
      "iteration: 98500 loss: 0.0013 lr: 0.02\n",
      "iteration: 98550 loss: 0.0012 lr: 0.02\n",
      "iteration: 98600 loss: 0.0013 lr: 0.02\n",
      "iteration: 98650 loss: 0.0013 lr: 0.02\n",
      "iteration: 98700 loss: 0.0011 lr: 0.02\n",
      "iteration: 98750 loss: 0.0010 lr: 0.02\n",
      "iteration: 98800 loss: 0.0013 lr: 0.02\n",
      "iteration: 98850 loss: 0.0012 lr: 0.02\n",
      "iteration: 98900 loss: 0.0011 lr: 0.02\n",
      "iteration: 98950 loss: 0.0013 lr: 0.02\n",
      "iteration: 99000 loss: 0.0012 lr: 0.02\n",
      "iteration: 99050 loss: 0.0012 lr: 0.02\n",
      "iteration: 99100 loss: 0.0012 lr: 0.02\n",
      "iteration: 99150 loss: 0.0010 lr: 0.02\n",
      "iteration: 99200 loss: 0.0011 lr: 0.02\n",
      "iteration: 99250 loss: 0.0012 lr: 0.02\n",
      "iteration: 99300 loss: 0.0013 lr: 0.02\n",
      "iteration: 99350 loss: 0.0012 lr: 0.02\n",
      "iteration: 99400 loss: 0.0010 lr: 0.02\n",
      "iteration: 99450 loss: 0.0011 lr: 0.02\n",
      "iteration: 99500 loss: 0.0012 lr: 0.02\n",
      "iteration: 99550 loss: 0.0011 lr: 0.02\n",
      "iteration: 99600 loss: 0.0012 lr: 0.02\n",
      "iteration: 99650 loss: 0.0012 lr: 0.02\n",
      "iteration: 99700 loss: 0.0012 lr: 0.02\n",
      "iteration: 99750 loss: 0.0012 lr: 0.02\n",
      "iteration: 99800 loss: 0.0012 lr: 0.02\n",
      "iteration: 99850 loss: 0.0011 lr: 0.02\n",
      "iteration: 99900 loss: 0.0015 lr: 0.02\n",
      "iteration: 99950 loss: 0.0013 lr: 0.02\n",
      "iteration: 100000 loss: 0.0013 lr: 0.02\n",
      "Exception in thread Thread-15:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1334, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1319, in _run_fn\n",
      "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1407, in _call_tf_sessionrun\n",
      "    run_metadata)\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Enqueue operation was cancelled\n",
      "\t [[{{node fifo_queue_enqueue}}]]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\train.py\", line 69, in load_and_enqueue\n",
      "    sess.run(enqueue_op, feed_dict=food)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 929, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1152, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1328, in _do_run\n",
      "    run_metadata)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1348, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Enqueue operation was cancelled\n",
      "\t [[node fifo_queue_enqueue (defined at C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\train.py:55) ]]\n",
      "\n",
      "Caused by op 'fifo_queue_enqueue', defined at:\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 563, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\asyncio\\base_events.py\", line 438, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\asyncio\\base_events.py\", line 1451, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\asyncio\\events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n",
      "    lambda f: self._run_callback(functools.partial(callback, future))\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 272, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 542, in execute_request\n",
      "    user_expressions, allow_stdin,\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2683, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2793, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2847, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-52-af075288bfcd>\", line 1, in <module>\n",
      "    deeplabcut.train_network(path_config_file, maxiters=100000, displayiters=50)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\training.py\", line 95, in train_network\n",
      "    train(str(poseconfigfile),displayiters,saveiters,maxiters,max_to_keep=max_snapshots_to_keep) #pass on path and file name for pose_cfg.yaml!\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\train.py\", line 104, in train\n",
      "    batch, enqueue_op, placeholders = setup_preloading(batch_spec)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\train.py\", line 55, in setup_preloading\n",
      "    enqueue_op = q.enqueue(placeholders_list)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\ops\\data_flow_ops.py\", line 345, in enqueue\n",
      "    self._queue_ref, vals, name=scope)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\ops\\gen_data_flow_ops.py\", line 4158, in queue_enqueue_v2\n",
      "    timeout_ms=timeout_ms, name=name)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3300, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1801, in __init__\n",
      "    self._traceback = tf_stack.extract_stack()\n",
      "\n",
      "CancelledError (see above for traceback): Enqueue operation was cancelled\n",
      "\t [[node fifo_queue_enqueue (defined at C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\train.py:55) ]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network is now trained and ready to evaluate. Use the function 'evaluate_network' to evaluate the network.\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.train_network(path_config_file, maxiters=100000, displayiters=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0], [1], [2]],\n",
      " 'all_joints_names': ['COM', 'wand_1', 'wand_2'],\n",
      " 'batch_size': 8,\n",
      " 'bottomheight': 400,\n",
      " 'crop': True,\n",
      " 'crop_pad': 0,\n",
      " 'cropratio': 0.4,\n",
      " 'dataset': 'training-datasets\\\\iteration-1\\\\UnaugmentedDataSet_arena_chaseOct10\\\\arena_chase_Andy95shuffle1.mat',\n",
      " 'dataset_type': 'default',\n",
      " 'deterministic': False,\n",
      " 'display_iters': 1000,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': 'C:\\\\Users\\\\LabAdmin\\\\.conda\\\\envs\\\\dlc-windowsGPU\\\\lib\\\\site-packages\\\\deeplabcut\\\\pose_estimation_tensorflow\\\\models\\\\pretrained\\\\resnet_v1_50.ckpt',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'leftwidth': 400,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 0.05,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'max_input_size': 1500,\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'metadataset': 'training-datasets\\\\iteration-1\\\\UnaugmentedDataSet_arena_chaseOct10\\\\Documentation_data-arena_chase_95shuffle1.pickle',\n",
      " 'min_input_size': 64,\n",
      " 'minsize': 100,\n",
      " 'mirror': False,\n",
      " 'multi_step': [[0.005, 10000],\n",
      "                [0.02, 430000],\n",
      "                [0.002, 730000],\n",
      "                [0.001, 1030000]],\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 3,\n",
      " 'num_outputs': 1,\n",
      " 'optimizer': 'sgd',\n",
      " 'pos_dist_thresh': 17,\n",
      " 'project_path': 'E:\\\\Users\\\\Phil\\\\DeepLabCut\\\\dev\\\\arena_chase-Andy-2019-10-10',\n",
      " 'regularize': False,\n",
      " 'rightwidth': 400,\n",
      " 'save_iters': 50000,\n",
      " 'scale_jitter_lo': 0.5,\n",
      " 'scale_jitter_up': 1.25,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': 'E:\\\\Users\\\\Phil\\\\DeepLabCut\\\\dev\\\\arena_chase-Andy-2019-10-10\\\\dlc-models\\\\iteration-1\\\\arena_chaseOct10-trainset95shuffle1\\\\test\\\\snapshot',\n",
      " 'stride': 8.0,\n",
      " 'topheight': 400,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using snapshot-100000 for model E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-10-10\\dlc-models\\iteration-1\\arena_chaseOct10-trainset95shuffle1\n",
      "num_outputs =  1\n",
      "INFO:tensorflow:Restoring parameters from E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-10-10\\dlc-models\\iteration-1\\arena_chaseOct10-trainset95shuffle1\\train\\snapshot-100000\n",
      "Starting to analyze %  E:\\Users\\Phil\\Jerboa\\0426_0221F_4-1.mp4\n",
      "Loading  E:\\Users\\Phil\\Jerboa\\0426_0221F_4-1.mp4\n",
      "Duration of video [s]:  57.17 , recorded with  119.88 fps!\n",
      "Overall # of frames:  6853  found with (before cropping) frame dimensions:  1280 720\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6868it [04:41, 23.87it/s]                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  6853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6868it [04:43, 24.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in E:\\Users\\Phil\\Jerboa...\n",
      "The videos are analyzed. Now your research can truly start! \n",
      " You can create labeled videos with 'create_labeled_video'.\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract any outlier frames!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'DeepCut_resnet50_arena_chaseOct10shuffle1_100000'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deeplabcut.analyze_videos(path_config_file,[vids_to_analyze[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting %  E:\\Users\\Phil\\Jerboa ['E:\\\\Users\\\\Phil\\\\Jerboa\\\\0426_0221F_4-1.mp4']\n",
      "Loading  E:\\Users\\Phil\\Jerboa\\0426_0221F_4-1.mp4 and data.\n",
      "False 0 1280 0 720\n",
      "6853\n",
      "Duration of video [s]:  57.17 , recorded with  119.88 fps!\n",
      "Overall # of frames:  6853 with cropped frame dimensions:  1280 720\n",
      "Generating frames and creating video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6853/6853 [00:40<00:00, 168.66it/s]\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.create_labeled_video(path_config_file,[vids_to_analyze[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pCrUvQIvoEKD"
   },
   "source": [
    "## Create labeled video\n",
    "This funtion is for visualiztion purpose and can be used to create a video in .mp4 format with labels predicted by the network. This video is saved in the same directory where the original video resides. \n",
    "\n",
    "THIS HAS MANY FUN OPTIONS! \n",
    "\n",
    "``deeplabcut.create_labeled_video(config, videos, videotype='avi', shuffle=1, trainingsetindex=0, filtered=False, save_frames=False, Frames2plot=None, delete=False, displayedbodyparts='all', codec='mp4v', outputframerate=None, destfolder=None, draw_skeleton=False, trailpoints=0, displaycropped=False)``\n",
    "\n",
    "So please check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.create_labeled_video?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6aDF7Q7KoEKE"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'videofile_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-2ef95a10875e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdeeplabcut\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_labeled_video\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_config_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvideofile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'videofile_path' is not defined"
     ]
    }
   ],
   "source": [
    "deeplabcut.create_labeled_video(path_config_file,videofile_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8GTiuJESoEKH"
   },
   "source": [
    "## Plot the trajectories of the analyzed videos\n",
    "This function plots the trajectories of all the body parts across the entire video. Each body part is identified by a unique color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gX21zZbXoEKJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: unrecognized arguments: #for making interactive plots.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook #for making interactive plots.\n",
    "deeplabcut.plot_trajectories(path_config_file,videofile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Demo-yourowndata.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
