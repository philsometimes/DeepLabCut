2019-10-10 14:50:34 Config:
{'all_joints': [[0], [1], [2]],
 'all_joints_names': ['COM', 'wand_1', 'wand_2'],
 'batch_size': 1,
 'bottomheight': 400,
 'crop': True,
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_arena_chaseOct10\\arena_chase_Andy95shuffle1.mat',
 'dataset_type': 'default',
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'leftwidth': 400,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_arena_chaseOct10\\Documentation_data-arena_chase_95shuffle1.pickle',
 'min_input_size': 64,
 'minsize': 100,
 'mirror': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 3,
 'optimizer': 'sgd',
 'pos_dist_thresh': 17,
 'project_path': 'E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-10-10',
 'regularize': False,
 'rightwidth': 400,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-10-10\\dlc-models\\iteration-0\\arena_chaseOct10-trainset95shuffle1\\train\\snapshot',
 'stride': 8.0,
 'topheight': 400,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2019-10-10 14:50:56 iteration: 50 loss: 0.0875 lr: 0.005
2019-10-10 14:51:07 iteration: 100 loss: 0.0224 lr: 0.005
2019-10-10 14:51:20 iteration: 150 loss: 0.0165 lr: 0.005
2019-10-10 14:51:40 iteration: 200 loss: 0.0156 lr: 0.005
2019-10-10 14:51:56 iteration: 250 loss: 0.0122 lr: 0.005
2019-10-10 14:52:11 iteration: 300 loss: 0.0112 lr: 0.005
2019-10-10 14:52:26 iteration: 350 loss: 0.0088 lr: 0.005
2019-10-10 14:52:44 iteration: 400 loss: 0.0080 lr: 0.005
2019-10-10 14:53:01 iteration: 450 loss: 0.0069 lr: 0.005
2019-10-10 14:53:14 iteration: 500 loss: 0.0055 lr: 0.005
2019-10-10 14:53:31 iteration: 550 loss: 0.0050 lr: 0.005
2019-10-10 14:53:52 iteration: 600 loss: 0.0049 lr: 0.005
2019-10-10 14:54:11 iteration: 650 loss: 0.0053 lr: 0.005
2019-10-10 14:54:25 iteration: 700 loss: 0.0046 lr: 0.005
2019-10-10 14:54:40 iteration: 750 loss: 0.0037 lr: 0.005
2019-10-10 14:54:53 iteration: 800 loss: 0.0040 lr: 0.005
2019-10-10 14:55:12 iteration: 850 loss: 0.0045 lr: 0.005
2019-10-10 14:55:22 iteration: 900 loss: 0.0036 lr: 0.005
2019-10-10 14:55:35 iteration: 950 loss: 0.0041 lr: 0.005
2019-10-10 14:55:49 iteration: 1000 loss: 0.0037 lr: 0.005
2019-10-10 14:55:59 iteration: 1050 loss: 0.0030 lr: 0.005
2019-10-10 14:56:14 iteration: 1100 loss: 0.0040 lr: 0.005
2019-10-10 14:56:28 iteration: 1150 loss: 0.0032 lr: 0.005
2019-10-10 14:56:44 iteration: 1200 loss: 0.0029 lr: 0.005
2019-10-10 14:56:59 iteration: 1250 loss: 0.0025 lr: 0.005
2019-10-10 14:57:13 iteration: 1300 loss: 0.0028 lr: 0.005
2019-10-10 14:57:27 iteration: 1350 loss: 0.0028 lr: 0.005
2019-10-10 14:57:36 iteration: 1400 loss: 0.0026 lr: 0.005
2019-10-10 14:57:51 iteration: 1450 loss: 0.0025 lr: 0.005
2019-10-10 14:58:04 iteration: 1500 loss: 0.0028 lr: 0.005
2019-10-10 14:58:21 iteration: 1550 loss: 0.0030 lr: 0.005
2019-10-10 14:58:34 iteration: 1600 loss: 0.0026 lr: 0.005
2019-10-10 14:58:52 iteration: 1650 loss: 0.0024 lr: 0.005
2019-10-10 14:59:10 iteration: 1700 loss: 0.0023 lr: 0.005
2019-10-10 14:59:24 iteration: 1750 loss: 0.0020 lr: 0.005
2019-10-10 14:59:35 iteration: 1800 loss: 0.0025 lr: 0.005
2019-10-10 14:59:49 iteration: 1850 loss: 0.0025 lr: 0.005
2019-10-10 15:00:07 iteration: 1900 loss: 0.0024 lr: 0.005
2019-10-10 15:00:21 iteration: 1950 loss: 0.0023 lr: 0.005
2019-10-10 15:00:33 iteration: 2000 loss: 0.0021 lr: 0.005
2019-10-10 15:00:43 iteration: 2050 loss: 0.0024 lr: 0.005
2019-10-10 15:00:56 iteration: 2100 loss: 0.0021 lr: 0.005
2019-10-10 15:01:07 iteration: 2150 loss: 0.0019 lr: 0.005
2019-10-10 15:01:20 iteration: 2200 loss: 0.0023 lr: 0.005
2019-10-10 15:01:37 iteration: 2250 loss: 0.0020 lr: 0.005
2019-10-10 15:01:55 iteration: 2300 loss: 0.0022 lr: 0.005
2019-10-10 15:02:11 iteration: 2350 loss: 0.0020 lr: 0.005
2019-10-10 15:02:21 iteration: 2400 loss: 0.0019 lr: 0.005
2019-10-10 15:02:33 iteration: 2450 loss: 0.0018 lr: 0.005
2019-10-10 15:02:46 iteration: 2500 loss: 0.0019 lr: 0.005
2019-10-10 15:03:04 iteration: 2550 loss: 0.0020 lr: 0.005
2019-10-10 15:03:14 iteration: 2600 loss: 0.0018 lr: 0.005
2019-10-10 15:03:29 iteration: 2650 loss: 0.0017 lr: 0.005
2019-10-10 15:03:42 iteration: 2700 loss: 0.0018 lr: 0.005
2019-10-10 15:03:58 iteration: 2750 loss: 0.0019 lr: 0.005
2019-10-10 15:04:07 iteration: 2800 loss: 0.0019 lr: 0.005
2019-10-10 15:04:24 iteration: 2850 loss: 0.0018 lr: 0.005
2019-10-10 15:04:41 iteration: 2900 loss: 0.0017 lr: 0.005
2019-10-10 15:04:54 iteration: 2950 loss: 0.0018 lr: 0.005
2019-10-10 15:05:09 iteration: 3000 loss: 0.0017 lr: 0.005
2019-10-10 15:05:21 iteration: 3050 loss: 0.0017 lr: 0.005
2019-10-10 15:05:32 iteration: 3100 loss: 0.0018 lr: 0.005
2019-10-10 15:05:43 iteration: 3150 loss: 0.0018 lr: 0.005
2019-10-10 15:05:54 iteration: 3200 loss: 0.0020 lr: 0.005
2019-10-10 15:06:11 iteration: 3250 loss: 0.0018 lr: 0.005
2019-10-10 15:06:24 iteration: 3300 loss: 0.0018 lr: 0.005
2019-10-10 15:06:35 iteration: 3350 loss: 0.0015 lr: 0.005
2019-10-10 15:06:47 iteration: 3400 loss: 0.0015 lr: 0.005
2019-10-10 15:07:01 iteration: 3450 loss: 0.0016 lr: 0.005
2019-10-10 15:07:12 iteration: 3500 loss: 0.0017 lr: 0.005
2019-10-10 15:07:26 iteration: 3550 loss: 0.0014 lr: 0.005
2019-10-10 15:07:39 iteration: 3600 loss: 0.0019 lr: 0.005
2019-10-10 15:07:50 iteration: 3650 loss: 0.0017 lr: 0.005
2019-10-10 15:08:00 iteration: 3700 loss: 0.0015 lr: 0.005
2019-10-10 15:08:14 iteration: 3750 loss: 0.0016 lr: 0.005
2019-10-10 15:08:29 iteration: 3800 loss: 0.0016 lr: 0.005
2019-10-10 15:08:43 iteration: 3850 loss: 0.0016 lr: 0.005
2019-10-10 15:08:59 iteration: 3900 loss: 0.0015 lr: 0.005
2019-10-10 15:09:13 iteration: 3950 loss: 0.0016 lr: 0.005
2019-10-10 15:09:27 iteration: 4000 loss: 0.0017 lr: 0.005
2019-10-10 15:09:37 iteration: 4050 loss: 0.0017 lr: 0.005
2019-10-10 15:09:50 iteration: 4100 loss: 0.0020 lr: 0.005
2019-10-10 15:10:03 iteration: 4150 loss: 0.0018 lr: 0.005
2019-10-10 15:10:14 iteration: 4200 loss: 0.0018 lr: 0.005
2019-10-10 15:10:28 iteration: 4250 loss: 0.0014 lr: 0.005
2019-10-10 15:10:40 iteration: 4300 loss: 0.0015 lr: 0.005
2019-10-10 15:10:51 iteration: 4350 loss: 0.0012 lr: 0.005
2019-10-10 15:11:03 iteration: 4400 loss: 0.0015 lr: 0.005
2019-10-10 15:11:15 iteration: 4450 loss: 0.0015 lr: 0.005
2019-10-10 15:11:26 iteration: 4500 loss: 0.0018 lr: 0.005
2019-10-10 15:11:43 iteration: 4550 loss: 0.0017 lr: 0.005
2019-10-10 15:11:56 iteration: 4600 loss: 0.0015 lr: 0.005
2019-10-10 15:12:09 iteration: 4650 loss: 0.0014 lr: 0.005
2019-10-10 15:12:23 iteration: 4700 loss: 0.0018 lr: 0.005
2019-10-10 15:12:36 iteration: 4750 loss: 0.0017 lr: 0.005
2019-10-10 15:12:48 iteration: 4800 loss: 0.0015 lr: 0.005
2019-10-10 15:13:07 iteration: 4850 loss: 0.0018 lr: 0.005
2019-10-10 15:13:20 iteration: 4900 loss: 0.0014 lr: 0.005
2019-10-10 15:13:35 iteration: 4950 loss: 0.0013 lr: 0.005
2019-10-10 15:13:51 iteration: 5000 loss: 0.0017 lr: 0.005
2019-10-10 15:14:02 iteration: 5050 loss: 0.0014 lr: 0.005
2019-10-10 15:14:14 iteration: 5100 loss: 0.0015 lr: 0.005
2019-10-10 15:14:23 iteration: 5150 loss: 0.0014 lr: 0.005
2019-10-10 15:14:37 iteration: 5200 loss: 0.0016 lr: 0.005
2019-10-10 15:14:53 iteration: 5250 loss: 0.0015 lr: 0.005
2019-10-10 15:15:05 iteration: 5300 loss: 0.0014 lr: 0.005
2019-10-10 15:15:15 iteration: 5350 loss: 0.0014 lr: 0.005
2019-10-10 15:15:27 iteration: 5400 loss: 0.0014 lr: 0.005
2019-10-10 15:15:38 iteration: 5450 loss: 0.0015 lr: 0.005
2019-10-10 15:15:53 iteration: 5500 loss: 0.0015 lr: 0.005
2019-10-10 15:16:05 iteration: 5550 loss: 0.0013 lr: 0.005
2019-10-10 15:16:18 iteration: 5600 loss: 0.0015 lr: 0.005
2019-10-10 15:16:33 iteration: 5650 loss: 0.0015 lr: 0.005
2019-10-10 15:16:47 iteration: 5700 loss: 0.0019 lr: 0.005
2019-10-10 15:16:58 iteration: 5750 loss: 0.0015 lr: 0.005
2019-10-10 15:17:10 iteration: 5800 loss: 0.0013 lr: 0.005
2019-10-10 15:17:22 iteration: 5850 loss: 0.0014 lr: 0.005
2019-10-10 15:17:32 iteration: 5900 loss: 0.0012 lr: 0.005
2019-10-10 15:17:43 iteration: 5950 loss: 0.0015 lr: 0.005
2019-10-10 15:17:54 iteration: 6000 loss: 0.0012 lr: 0.005
2019-10-10 15:18:06 iteration: 6050 loss: 0.0013 lr: 0.005
2019-10-10 15:18:20 iteration: 6100 loss: 0.0012 lr: 0.005
2019-10-10 15:18:31 iteration: 6150 loss: 0.0014 lr: 0.005
2019-10-10 15:18:40 iteration: 6200 loss: 0.0015 lr: 0.005
2019-10-10 15:18:51 iteration: 6250 loss: 0.0015 lr: 0.005
2019-10-10 15:19:05 iteration: 6300 loss: 0.0014 lr: 0.005
2019-10-10 15:19:18 iteration: 6350 loss: 0.0016 lr: 0.005
2019-10-10 15:19:29 iteration: 6400 loss: 0.0012 lr: 0.005
2019-10-10 15:19:42 iteration: 6450 loss: 0.0013 lr: 0.005
2019-10-10 15:19:52 iteration: 6500 loss: 0.0011 lr: 0.005
2019-10-10 15:20:03 iteration: 6550 loss: 0.0016 lr: 0.005
2019-10-10 15:20:13 iteration: 6600 loss: 0.0014 lr: 0.005
2019-10-10 15:20:26 iteration: 6650 loss: 0.0015 lr: 0.005
2019-10-10 15:20:35 iteration: 6700 loss: 0.0015 lr: 0.005
2019-10-10 15:20:44 iteration: 6750 loss: 0.0013 lr: 0.005
2019-10-10 15:20:58 iteration: 6800 loss: 0.0015 lr: 0.005
2019-10-10 15:21:11 iteration: 6850 loss: 0.0013 lr: 0.005
2019-10-10 15:21:25 iteration: 6900 loss: 0.0014 lr: 0.005
2019-10-10 15:21:35 iteration: 6950 loss: 0.0012 lr: 0.005
2019-10-10 15:21:48 iteration: 7000 loss: 0.0015 lr: 0.005
2019-10-10 15:21:58 iteration: 7050 loss: 0.0014 lr: 0.005
2019-10-10 15:22:11 iteration: 7100 loss: 0.0014 lr: 0.005
2019-10-10 15:22:24 iteration: 7150 loss: 0.0015 lr: 0.005
2019-10-10 15:22:35 iteration: 7200 loss: 0.0013 lr: 0.005
2019-10-10 15:22:47 iteration: 7250 loss: 0.0014 lr: 0.005
2019-10-10 15:22:57 iteration: 7300 loss: 0.0013 lr: 0.005
2019-10-10 15:23:07 iteration: 7350 loss: 0.0013 lr: 0.005
2019-10-10 15:23:19 iteration: 7400 loss: 0.0014 lr: 0.005
2019-10-10 15:23:32 iteration: 7450 loss: 0.0013 lr: 0.005
2019-10-10 15:23:43 iteration: 7500 loss: 0.0013 lr: 0.005
2019-10-10 15:23:55 iteration: 7550 loss: 0.0011 lr: 0.005
2019-10-10 15:24:05 iteration: 7600 loss: 0.0013 lr: 0.005
2019-10-10 15:24:17 iteration: 7650 loss: 0.0013 lr: 0.005
2019-10-10 15:24:29 iteration: 7700 loss: 0.0011 lr: 0.005
2019-10-10 15:24:43 iteration: 7750 loss: 0.0012 lr: 0.005
2019-10-10 15:24:54 iteration: 7800 loss: 0.0011 lr: 0.005
2019-10-10 15:25:07 iteration: 7850 loss: 0.0015 lr: 0.005
2019-10-10 15:25:19 iteration: 7900 loss: 0.0013 lr: 0.005
2019-10-10 15:25:30 iteration: 7950 loss: 0.0012 lr: 0.005
2019-10-10 15:25:38 iteration: 8000 loss: 0.0013 lr: 0.005
2019-10-10 15:25:47 iteration: 8050 loss: 0.0012 lr: 0.005
2019-10-10 15:25:58 iteration: 8100 loss: 0.0014 lr: 0.005
2019-10-10 15:26:09 iteration: 8150 loss: 0.0013 lr: 0.005
2019-10-10 15:26:19 iteration: 8200 loss: 0.0012 lr: 0.005
2019-10-10 15:26:27 iteration: 8250 loss: 0.0012 lr: 0.005
2019-10-10 15:26:38 iteration: 8300 loss: 0.0013 lr: 0.005
2019-10-10 15:26:46 iteration: 8350 loss: 0.0012 lr: 0.005
2019-10-10 15:26:57 iteration: 8400 loss: 0.0014 lr: 0.005
2019-10-10 15:27:07 iteration: 8450 loss: 0.0012 lr: 0.005
2019-10-10 15:27:16 iteration: 8500 loss: 0.0011 lr: 0.005
2019-10-10 15:27:25 iteration: 8550 loss: 0.0011 lr: 0.005
2019-10-10 15:27:35 iteration: 8600 loss: 0.0014 lr: 0.005
2019-10-10 15:27:44 iteration: 8650 loss: 0.0012 lr: 0.005
2019-10-10 15:27:54 iteration: 8700 loss: 0.0013 lr: 0.005
2019-10-10 15:28:03 iteration: 8750 loss: 0.0012 lr: 0.005
2019-10-10 15:28:14 iteration: 8800 loss: 0.0011 lr: 0.005
2019-10-10 15:28:24 iteration: 8850 loss: 0.0013 lr: 0.005
2019-10-10 15:28:39 iteration: 8900 loss: 0.0013 lr: 0.005
2019-10-10 15:28:49 iteration: 8950 loss: 0.0012 lr: 0.005
2019-10-10 15:29:02 iteration: 9000 loss: 0.0011 lr: 0.005
2019-10-10 15:29:12 iteration: 9050 loss: 0.0012 lr: 0.005
2019-10-10 15:29:24 iteration: 9100 loss: 0.0013 lr: 0.005
2019-10-10 15:29:32 iteration: 9150 loss: 0.0013 lr: 0.005
2019-10-10 15:29:45 iteration: 9200 loss: 0.0013 lr: 0.005
2019-10-10 15:29:57 iteration: 9250 loss: 0.0011 lr: 0.005
2019-10-10 15:30:07 iteration: 9300 loss: 0.0012 lr: 0.005
2019-10-10 15:30:21 iteration: 9350 loss: 0.0014 lr: 0.005
2019-10-10 15:30:32 iteration: 9400 loss: 0.0011 lr: 0.005
2019-10-10 15:30:44 iteration: 9450 loss: 0.0011 lr: 0.005
2019-10-10 15:30:54 iteration: 9500 loss: 0.0013 lr: 0.005
2019-10-10 15:31:05 iteration: 9550 loss: 0.0011 lr: 0.005
2019-10-10 15:31:14 iteration: 9600 loss: 0.0013 lr: 0.005
2019-10-10 15:31:27 iteration: 9650 loss: 0.0012 lr: 0.005
2019-10-10 15:31:39 iteration: 9700 loss: 0.0014 lr: 0.005
2019-10-10 15:31:48 iteration: 9750 loss: 0.0010 lr: 0.005
2019-10-10 15:31:59 iteration: 9800 loss: 0.0015 lr: 0.005
2019-10-10 15:32:09 iteration: 9850 loss: 0.0014 lr: 0.005
2019-10-10 15:32:23 iteration: 9900 loss: 0.0011 lr: 0.005
2019-10-10 15:32:32 iteration: 9950 loss: 0.0011 lr: 0.005
2019-10-10 15:32:41 iteration: 10000 loss: 0.0012 lr: 0.005
2019-10-10 15:32:49 iteration: 10050 loss: 0.0024 lr: 0.02
2019-10-10 15:32:59 iteration: 10100 loss: 0.0025 lr: 0.02
2019-10-10 15:33:09 iteration: 10150 loss: 0.0026 lr: 0.02
2019-10-10 15:33:20 iteration: 10200 loss: 0.0026 lr: 0.02
2019-10-10 15:33:28 iteration: 10250 loss: 0.0026 lr: 0.02
2019-10-10 15:33:39 iteration: 10300 loss: 0.0023 lr: 0.02
2019-10-10 15:33:47 iteration: 10350 loss: 0.0024 lr: 0.02
2019-10-10 15:33:57 iteration: 10400 loss: 0.0023 lr: 0.02
2019-10-10 15:34:04 iteration: 10450 loss: 0.0021 lr: 0.02
2019-10-10 15:34:14 iteration: 10500 loss: 0.0026 lr: 0.02
2019-10-10 15:34:21 iteration: 10550 loss: 0.0020 lr: 0.02
2019-10-10 15:34:29 iteration: 10600 loss: 0.0023 lr: 0.02
2019-10-10 15:34:38 iteration: 10650 loss: 0.0024 lr: 0.02
2019-10-10 15:34:50 iteration: 10700 loss: 0.0018 lr: 0.02
2019-10-10 15:35:00 iteration: 10750 loss: 0.0020 lr: 0.02
2019-10-10 15:35:12 iteration: 10800 loss: 0.0026 lr: 0.02
2019-10-10 15:35:25 iteration: 10850 loss: 0.0023 lr: 0.02
2019-10-10 15:35:35 iteration: 10900 loss: 0.0026 lr: 0.02
2019-10-10 15:35:46 iteration: 10950 loss: 0.0021 lr: 0.02
2019-10-10 15:35:55 iteration: 11000 loss: 0.0020 lr: 0.02
2019-10-10 15:36:03 iteration: 11050 loss: 0.0019 lr: 0.02
2019-10-10 15:36:14 iteration: 11100 loss: 0.0016 lr: 0.02
2019-10-10 15:36:25 iteration: 11150 loss: 0.0020 lr: 0.02
2019-10-10 15:36:35 iteration: 11200 loss: 0.0017 lr: 0.02
2019-10-10 15:36:47 iteration: 11250 loss: 0.0017 lr: 0.02
2019-10-10 15:36:57 iteration: 11300 loss: 0.0015 lr: 0.02
2019-10-10 15:37:06 iteration: 11350 loss: 0.0018 lr: 0.02
2019-10-10 15:37:15 iteration: 11400 loss: 0.0019 lr: 0.02
2019-10-10 15:37:26 iteration: 11450 loss: 0.0018 lr: 0.02
2019-10-10 15:37:37 iteration: 11500 loss: 0.0017 lr: 0.02
2019-10-10 15:37:50 iteration: 11550 loss: 0.0017 lr: 0.02
2019-10-10 15:38:02 iteration: 11600 loss: 0.0016 lr: 0.02
2019-10-10 15:38:10 iteration: 11650 loss: 0.0018 lr: 0.02
2019-10-10 15:38:22 iteration: 11700 loss: 0.0019 lr: 0.02
2019-10-10 15:38:34 iteration: 11750 loss: 0.0018 lr: 0.02
2019-10-10 15:38:44 iteration: 11800 loss: 0.0016 lr: 0.02
2019-10-10 15:38:54 iteration: 11850 loss: 0.0016 lr: 0.02
2019-10-10 15:39:04 iteration: 11900 loss: 0.0017 lr: 0.02
2019-10-10 15:39:15 iteration: 11950 loss: 0.0025 lr: 0.02
2019-10-10 15:39:26 iteration: 12000 loss: 0.0020 lr: 0.02
2019-10-10 15:39:37 iteration: 12050 loss: 0.0019 lr: 0.02
2019-10-10 15:39:47 iteration: 12100 loss: 0.0015 lr: 0.02
2019-10-10 15:39:59 iteration: 12150 loss: 0.0017 lr: 0.02
2019-10-10 15:40:08 iteration: 12200 loss: 0.0017 lr: 0.02
2019-10-10 15:40:19 iteration: 12250 loss: 0.0022 lr: 0.02
2019-10-10 15:40:26 iteration: 12300 loss: 0.0015 lr: 0.02
2019-10-10 15:40:37 iteration: 12350 loss: 0.0014 lr: 0.02
2019-10-10 15:40:47 iteration: 12400 loss: 0.0015 lr: 0.02
2019-10-10 15:40:57 iteration: 12450 loss: 0.0019 lr: 0.02
2019-10-10 15:41:10 iteration: 12500 loss: 0.0020 lr: 0.02
2019-10-10 15:41:18 iteration: 12550 loss: 0.0015 lr: 0.02
2019-10-10 15:41:27 iteration: 12600 loss: 0.0013 lr: 0.02
2019-10-10 15:41:40 iteration: 12650 loss: 0.0015 lr: 0.02
2019-10-10 15:41:49 iteration: 12700 loss: 0.0019 lr: 0.02
2019-10-10 15:41:59 iteration: 12750 loss: 0.0016 lr: 0.02
2019-10-10 15:42:08 iteration: 12800 loss: 0.0014 lr: 0.02
2019-10-10 15:42:19 iteration: 12850 loss: 0.0015 lr: 0.02
2019-10-10 15:42:28 iteration: 12900 loss: 0.0015 lr: 0.02
2019-10-10 15:42:38 iteration: 12950 loss: 0.0014 lr: 0.02
2019-10-10 15:42:48 iteration: 13000 loss: 0.0014 lr: 0.02
2019-10-10 15:42:57 iteration: 13050 loss: 0.0016 lr: 0.02
2019-10-10 15:43:08 iteration: 13100 loss: 0.0017 lr: 0.02
2019-10-10 15:43:15 iteration: 13150 loss: 0.0015 lr: 0.02
2019-10-10 15:43:25 iteration: 13200 loss: 0.0018 lr: 0.02
2019-10-10 15:43:33 iteration: 13250 loss: 0.0015 lr: 0.02
2019-10-10 15:43:42 iteration: 13300 loss: 0.0013 lr: 0.02
2019-10-10 15:43:52 iteration: 13350 loss: 0.0016 lr: 0.02
2019-10-10 15:44:04 iteration: 13400 loss: 0.0017 lr: 0.02
2019-10-10 15:44:15 iteration: 13450 loss: 0.0017 lr: 0.02
2019-10-10 15:44:28 iteration: 13500 loss: 0.0014 lr: 0.02
2019-10-10 15:44:36 iteration: 13550 loss: 0.0014 lr: 0.02
2019-10-10 15:44:47 iteration: 13600 loss: 0.0015 lr: 0.02
2019-10-10 15:44:58 iteration: 13650 loss: 0.0015 lr: 0.02
2019-10-10 15:45:10 iteration: 13700 loss: 0.0017 lr: 0.02
2019-10-10 15:45:19 iteration: 13750 loss: 0.0017 lr: 0.02
2019-10-10 15:45:27 iteration: 13800 loss: 0.0016 lr: 0.02
2019-10-10 15:45:38 iteration: 13850 loss: 0.0015 lr: 0.02
2019-10-10 15:45:49 iteration: 13900 loss: 0.0016 lr: 0.02
2019-10-10 15:45:59 iteration: 13950 loss: 0.0015 lr: 0.02
2019-10-10 15:46:09 iteration: 14000 loss: 0.0012 lr: 0.02
2019-10-10 15:46:20 iteration: 14050 loss: 0.0013 lr: 0.02
2019-10-10 15:46:33 iteration: 14100 loss: 0.0015 lr: 0.02
2019-10-10 15:46:43 iteration: 14150 loss: 0.0014 lr: 0.02
2019-10-10 15:46:54 iteration: 14200 loss: 0.0015 lr: 0.02
2019-10-10 15:47:04 iteration: 14250 loss: 0.0014 lr: 0.02
2019-10-10 15:47:12 iteration: 14300 loss: 0.0013 lr: 0.02
2019-10-10 15:47:19 iteration: 14350 loss: 0.0016 lr: 0.02
2019-10-10 15:47:24 iteration: 14400 loss: 0.0014 lr: 0.02
2019-10-10 15:47:36 iteration: 14450 loss: 0.0015 lr: 0.02
2019-10-10 15:47:45 iteration: 14500 loss: 0.0014 lr: 0.02
2019-10-10 15:47:58 iteration: 14550 loss: 0.0016 lr: 0.02
2019-10-10 15:48:06 iteration: 14600 loss: 0.0016 lr: 0.02
2019-10-10 15:48:15 iteration: 14650 loss: 0.0014 lr: 0.02
2019-10-10 15:48:26 iteration: 14700 loss: 0.0013 lr: 0.02
2019-10-10 15:48:37 iteration: 14750 loss: 0.0014 lr: 0.02
2019-10-10 15:48:50 iteration: 14800 loss: 0.0017 lr: 0.02
2019-10-10 15:49:01 iteration: 14850 loss: 0.0014 lr: 0.02
2019-10-10 15:49:12 iteration: 14900 loss: 0.0012 lr: 0.02
2019-10-10 15:49:19 iteration: 14950 loss: 0.0014 lr: 0.02
2019-10-10 15:49:28 iteration: 15000 loss: 0.0012 lr: 0.02
2019-10-10 15:49:37 iteration: 15050 loss: 0.0014 lr: 0.02
2019-10-10 15:49:46 iteration: 15100 loss: 0.0013 lr: 0.02
2019-10-10 15:49:57 iteration: 15150 loss: 0.0015 lr: 0.02
2019-10-10 15:50:08 iteration: 15200 loss: 0.0015 lr: 0.02
2019-10-10 15:50:16 iteration: 15250 loss: 0.0013 lr: 0.02
2019-10-10 15:50:24 iteration: 15300 loss: 0.0011 lr: 0.02
2019-10-10 15:50:35 iteration: 15350 loss: 0.0012 lr: 0.02
2019-10-10 15:50:43 iteration: 15400 loss: 0.0013 lr: 0.02
2019-10-10 15:50:49 iteration: 15450 loss: 0.0011 lr: 0.02
2019-10-10 15:50:59 iteration: 15500 loss: 0.0014 lr: 0.02
2019-10-10 15:51:09 iteration: 15550 loss: 0.0014 lr: 0.02
2019-10-10 15:51:19 iteration: 15600 loss: 0.0015 lr: 0.02
2019-10-10 15:51:28 iteration: 15650 loss: 0.0014 lr: 0.02
2019-10-10 15:51:37 iteration: 15700 loss: 0.0012 lr: 0.02
2019-10-10 15:51:49 iteration: 15750 loss: 0.0014 lr: 0.02
2019-10-10 15:51:58 iteration: 15800 loss: 0.0012 lr: 0.02
2019-10-10 15:52:09 iteration: 15850 loss: 0.0014 lr: 0.02
2019-10-10 15:52:18 iteration: 15900 loss: 0.0013 lr: 0.02
2019-10-10 15:52:27 iteration: 15950 loss: 0.0013 lr: 0.02
2019-10-10 15:52:35 iteration: 16000 loss: 0.0012 lr: 0.02
2019-10-10 15:52:44 iteration: 16050 loss: 0.0013 lr: 0.02
2019-10-10 15:52:54 iteration: 16100 loss: 0.0014 lr: 0.02
2019-10-10 15:53:02 iteration: 16150 loss: 0.0015 lr: 0.02
2019-10-10 15:53:13 iteration: 16200 loss: 0.0015 lr: 0.02
2019-10-10 15:53:22 iteration: 16250 loss: 0.0012 lr: 0.02
2019-10-10 15:53:31 iteration: 16300 loss: 0.0013 lr: 0.02
2019-10-10 15:53:41 iteration: 16350 loss: 0.0013 lr: 0.02
2019-10-10 15:53:51 iteration: 16400 loss: 0.0014 lr: 0.02
2019-10-10 15:54:01 iteration: 16450 loss: 0.0014 lr: 0.02
2019-10-10 15:54:12 iteration: 16500 loss: 0.0013 lr: 0.02
2019-10-10 15:54:25 iteration: 16550 loss: 0.0012 lr: 0.02
2019-10-10 15:54:32 iteration: 16600 loss: 0.0011 lr: 0.02
2019-10-10 15:54:40 iteration: 16650 loss: 0.0014 lr: 0.02
2019-10-10 15:54:48 iteration: 16700 loss: 0.0014 lr: 0.02
2019-10-10 15:54:57 iteration: 16750 loss: 0.0014 lr: 0.02
2019-10-10 15:55:06 iteration: 16800 loss: 0.0013 lr: 0.02
2019-10-10 15:55:15 iteration: 16850 loss: 0.0012 lr: 0.02
2019-10-10 15:55:25 iteration: 16900 loss: 0.0011 lr: 0.02
2019-10-10 15:55:37 iteration: 16950 loss: 0.0013 lr: 0.02
2019-10-10 15:55:45 iteration: 17000 loss: 0.0013 lr: 0.02
2019-10-10 15:55:54 iteration: 17050 loss: 0.0015 lr: 0.02
2019-10-10 15:56:03 iteration: 17100 loss: 0.0012 lr: 0.02
2019-10-10 15:56:12 iteration: 17150 loss: 0.0011 lr: 0.02
2019-10-10 15:56:20 iteration: 17200 loss: 0.0012 lr: 0.02
2019-10-10 15:56:30 iteration: 17250 loss: 0.0012 lr: 0.02
2019-10-10 15:56:38 iteration: 17300 loss: 0.0013 lr: 0.02
2019-10-10 15:56:47 iteration: 17350 loss: 0.0014 lr: 0.02
2019-10-10 15:56:58 iteration: 17400 loss: 0.0014 lr: 0.02
2019-10-10 15:57:06 iteration: 17450 loss: 0.0015 lr: 0.02
2019-10-10 15:57:15 iteration: 17500 loss: 0.0012 lr: 0.02
2019-10-10 15:57:25 iteration: 17550 loss: 0.0015 lr: 0.02
2019-10-10 15:57:33 iteration: 17600 loss: 0.0015 lr: 0.02
2019-10-10 15:57:42 iteration: 17650 loss: 0.0014 lr: 0.02
2019-10-10 15:57:52 iteration: 17700 loss: 0.0016 lr: 0.02
2019-10-10 15:58:01 iteration: 17750 loss: 0.0014 lr: 0.02
2019-10-10 15:58:12 iteration: 17800 loss: 0.0013 lr: 0.02
2019-10-10 15:58:23 iteration: 17850 loss: 0.0012 lr: 0.02
2019-10-10 15:58:33 iteration: 17900 loss: 0.0013 lr: 0.02
2019-10-10 15:58:41 iteration: 17950 loss: 0.0014 lr: 0.02
2019-10-10 15:58:52 iteration: 18000 loss: 0.0012 lr: 0.02
2019-10-10 15:59:02 iteration: 18050 loss: 0.0013 lr: 0.02
2019-10-10 15:59:09 iteration: 18100 loss: 0.0012 lr: 0.02
2019-10-10 15:59:18 iteration: 18150 loss: 0.0013 lr: 0.02
2019-10-10 15:59:25 iteration: 18200 loss: 0.0011 lr: 0.02
2019-10-10 15:59:34 iteration: 18250 loss: 0.0012 lr: 0.02
2019-10-10 15:59:42 iteration: 18300 loss: 0.0013 lr: 0.02
2019-10-10 15:59:52 iteration: 18350 loss: 0.0012 lr: 0.02
2019-10-10 15:59:59 iteration: 18400 loss: 0.0011 lr: 0.02
2019-10-10 16:00:08 iteration: 18450 loss: 0.0012 lr: 0.02
2019-10-10 16:00:15 iteration: 18500 loss: 0.0012 lr: 0.02
2019-10-10 16:00:26 iteration: 18550 loss: 0.0012 lr: 0.02
2019-10-10 16:00:35 iteration: 18600 loss: 0.0011 lr: 0.02
2019-10-10 16:00:43 iteration: 18650 loss: 0.0012 lr: 0.02
2019-10-10 16:00:53 iteration: 18700 loss: 0.0014 lr: 0.02
2019-10-10 16:01:03 iteration: 18750 loss: 0.0012 lr: 0.02
2019-10-10 16:01:12 iteration: 18800 loss: 0.0013 lr: 0.02
2019-10-10 16:01:21 iteration: 18850 loss: 0.0012 lr: 0.02
2019-10-10 16:01:31 iteration: 18900 loss: 0.0013 lr: 0.02
2019-10-10 16:01:40 iteration: 18950 loss: 0.0012 lr: 0.02
2019-10-10 16:01:49 iteration: 19000 loss: 0.0012 lr: 0.02
2019-10-10 16:01:58 iteration: 19050 loss: 0.0012 lr: 0.02
2019-10-10 16:02:06 iteration: 19100 loss: 0.0011 lr: 0.02
2019-10-10 16:02:14 iteration: 19150 loss: 0.0014 lr: 0.02
2019-10-10 16:02:20 iteration: 19200 loss: 0.0012 lr: 0.02
2019-10-10 16:02:29 iteration: 19250 loss: 0.0011 lr: 0.02
2019-10-10 16:02:39 iteration: 19300 loss: 0.0011 lr: 0.02
2019-10-10 16:02:48 iteration: 19350 loss: 0.0011 lr: 0.02
2019-10-10 16:02:56 iteration: 19400 loss: 0.0013 lr: 0.02
2019-10-10 16:03:06 iteration: 19450 loss: 0.0013 lr: 0.02
2019-10-10 16:03:14 iteration: 19500 loss: 0.0011 lr: 0.02
2019-10-10 16:03:22 iteration: 19550 loss: 0.0016 lr: 0.02
2019-10-10 16:03:32 iteration: 19600 loss: 0.0014 lr: 0.02
2019-10-10 16:03:39 iteration: 19650 loss: 0.0014 lr: 0.02
2019-10-10 16:03:47 iteration: 19700 loss: 0.0012 lr: 0.02
2019-10-10 16:03:57 iteration: 19750 loss: 0.0011 lr: 0.02
2019-10-10 16:04:05 iteration: 19800 loss: 0.0011 lr: 0.02
2019-10-10 16:04:15 iteration: 19850 loss: 0.0013 lr: 0.02
2019-10-10 16:04:24 iteration: 19900 loss: 0.0011 lr: 0.02
2019-10-10 16:04:35 iteration: 19950 loss: 0.0013 lr: 0.02
2019-10-10 16:04:45 iteration: 20000 loss: 0.0013 lr: 0.02
2019-10-10 16:04:53 iteration: 20050 loss: 0.0012 lr: 0.02
2019-10-10 16:05:03 iteration: 20100 loss: 0.0012 lr: 0.02
2019-10-10 16:05:11 iteration: 20150 loss: 0.0012 lr: 0.02
2019-10-10 16:05:19 iteration: 20200 loss: 0.0011 lr: 0.02
2019-10-10 16:05:28 iteration: 20250 loss: 0.0012 lr: 0.02
2019-10-10 16:05:37 iteration: 20300 loss: 0.0014 lr: 0.02
2019-10-10 16:05:47 iteration: 20350 loss: 0.0014 lr: 0.02
2019-10-10 16:05:53 iteration: 20400 loss: 0.0012 lr: 0.02
2019-10-10 16:06:05 iteration: 20450 loss: 0.0012 lr: 0.02
2019-10-10 16:06:15 iteration: 20500 loss: 0.0013 lr: 0.02
2019-10-10 16:06:25 iteration: 20550 loss: 0.0012 lr: 0.02
2019-10-10 16:06:34 iteration: 20600 loss: 0.0012 lr: 0.02
2019-10-10 16:06:43 iteration: 20650 loss: 0.0012 lr: 0.02
2019-10-10 16:06:52 iteration: 20700 loss: 0.0011 lr: 0.02
2019-10-10 16:07:01 iteration: 20750 loss: 0.0010 lr: 0.02
2019-10-10 16:07:11 iteration: 20800 loss: 0.0012 lr: 0.02
2019-10-10 16:07:20 iteration: 20850 loss: 0.0012 lr: 0.02
2019-10-10 16:07:29 iteration: 20900 loss: 0.0011 lr: 0.02
2019-10-10 16:07:38 iteration: 20950 loss: 0.0011 lr: 0.02
2019-10-10 16:07:47 iteration: 21000 loss: 0.0012 lr: 0.02
2019-10-10 16:07:53 iteration: 21050 loss: 0.0012 lr: 0.02
2019-10-10 16:08:01 iteration: 21100 loss: 0.0011 lr: 0.02
2019-10-10 16:08:09 iteration: 21150 loss: 0.0011 lr: 0.02
2019-10-10 16:08:17 iteration: 21200 loss: 0.0011 lr: 0.02
2019-10-10 16:08:25 iteration: 21250 loss: 0.0011 lr: 0.02
2019-10-10 16:08:32 iteration: 21300 loss: 0.0011 lr: 0.02
2019-10-10 16:08:43 iteration: 21350 loss: 0.0012 lr: 0.02
2019-10-10 16:08:53 iteration: 21400 loss: 0.0012 lr: 0.02
2019-10-10 16:09:01 iteration: 21450 loss: 0.0010 lr: 0.02
2019-10-10 16:09:10 iteration: 21500 loss: 0.0012 lr: 0.02
2019-10-10 16:09:19 iteration: 21550 loss: 0.0011 lr: 0.02
2019-10-10 16:09:28 iteration: 21600 loss: 0.0011 lr: 0.02
2019-10-10 16:09:38 iteration: 21650 loss: 0.0012 lr: 0.02
2019-10-10 16:09:48 iteration: 21700 loss: 0.0013 lr: 0.02
2019-10-10 16:09:57 iteration: 21750 loss: 0.0011 lr: 0.02
2019-10-10 16:10:04 iteration: 21800 loss: 0.0011 lr: 0.02
2019-10-10 16:10:12 iteration: 21850 loss: 0.0012 lr: 0.02
2019-10-10 16:10:19 iteration: 21900 loss: 0.0010 lr: 0.02
2019-10-10 16:10:27 iteration: 21950 loss: 0.0011 lr: 0.02
2019-10-10 16:10:37 iteration: 22000 loss: 0.0011 lr: 0.02
2019-10-10 16:10:44 iteration: 22050 loss: 0.0010 lr: 0.02
2019-10-10 16:10:50 iteration: 22100 loss: 0.0012 lr: 0.02
2019-10-10 16:10:59 iteration: 22150 loss: 0.0011 lr: 0.02
2019-10-10 16:11:05 iteration: 22200 loss: 0.0011 lr: 0.02
2019-10-10 16:11:12 iteration: 22250 loss: 0.0011 lr: 0.02
2019-10-10 16:11:18 iteration: 22300 loss: 0.0011 lr: 0.02
2019-10-10 16:11:26 iteration: 22350 loss: 0.0012 lr: 0.02
2019-10-10 16:11:34 iteration: 22400 loss: 0.0012 lr: 0.02
2019-10-10 16:11:41 iteration: 22450 loss: 0.0011 lr: 0.02
2019-10-10 16:11:48 iteration: 22500 loss: 0.0012 lr: 0.02
2019-10-10 16:11:56 iteration: 22550 loss: 0.0013 lr: 0.02
2019-10-10 16:12:03 iteration: 22600 loss: 0.0012 lr: 0.02
2019-10-10 16:12:17 iteration: 22650 loss: 0.0012 lr: 0.02
2019-10-10 16:12:25 iteration: 22700 loss: 0.0011 lr: 0.02
2019-10-10 16:12:32 iteration: 22750 loss: 0.0012 lr: 0.02
2019-10-10 16:12:41 iteration: 22800 loss: 0.0012 lr: 0.02
2019-10-10 16:12:51 iteration: 22850 loss: 0.0012 lr: 0.02
2019-10-10 16:13:00 iteration: 22900 loss: 0.0011 lr: 0.02
2019-10-10 16:13:08 iteration: 22950 loss: 0.0010 lr: 0.02
2019-10-10 16:13:16 iteration: 23000 loss: 0.0010 lr: 0.02
2019-10-10 16:13:23 iteration: 23050 loss: 0.0010 lr: 0.02
2019-10-10 16:13:32 iteration: 23100 loss: 0.0013 lr: 0.02
2019-10-10 16:13:39 iteration: 23150 loss: 0.0012 lr: 0.02
2019-10-10 16:13:50 iteration: 23200 loss: 0.0013 lr: 0.02
2019-10-10 16:14:01 iteration: 23250 loss: 0.0010 lr: 0.02
2019-10-10 16:14:10 iteration: 23300 loss: 0.0012 lr: 0.02
2019-10-10 16:14:17 iteration: 23350 loss: 0.0011 lr: 0.02
2019-10-10 16:14:25 iteration: 23400 loss: 0.0011 lr: 0.02
2019-10-10 16:14:35 iteration: 23450 loss: 0.0009 lr: 0.02
2019-10-10 16:14:43 iteration: 23500 loss: 0.0011 lr: 0.02
2019-10-10 16:14:49 iteration: 23550 loss: 0.0010 lr: 0.02
2019-10-10 16:14:58 iteration: 23600 loss: 0.0013 lr: 0.02
2019-10-10 16:15:05 iteration: 23650 loss: 0.0013 lr: 0.02
2019-10-10 16:15:13 iteration: 23700 loss: 0.0010 lr: 0.02
2019-10-10 16:15:20 iteration: 23750 loss: 0.0013 lr: 0.02
2019-10-10 16:15:28 iteration: 23800 loss: 0.0012 lr: 0.02
2019-10-10 16:15:34 iteration: 23850 loss: 0.0013 lr: 0.02
2019-10-10 16:15:43 iteration: 23900 loss: 0.0010 lr: 0.02
2019-10-10 16:15:52 iteration: 23950 loss: 0.0011 lr: 0.02
2019-10-10 16:16:01 iteration: 24000 loss: 0.0011 lr: 0.02
2019-10-10 16:16:10 iteration: 24050 loss: 0.0010 lr: 0.02
2019-10-10 16:16:20 iteration: 24100 loss: 0.0011 lr: 0.02
2019-10-10 16:16:26 iteration: 24150 loss: 0.0013 lr: 0.02
2019-10-10 16:16:34 iteration: 24200 loss: 0.0010 lr: 0.02
2019-10-10 16:16:43 iteration: 24250 loss: 0.0010 lr: 0.02
2019-10-10 16:16:51 iteration: 24300 loss: 0.0012 lr: 0.02
2019-10-10 16:16:59 iteration: 24350 loss: 0.0011 lr: 0.02
2019-10-10 16:17:08 iteration: 24400 loss: 0.0011 lr: 0.02
2019-10-10 16:17:18 iteration: 24450 loss: 0.0010 lr: 0.02
2019-10-10 16:17:27 iteration: 24500 loss: 0.0011 lr: 0.02
2019-10-10 16:17:36 iteration: 24550 loss: 0.0010 lr: 0.02
2019-10-10 16:17:42 iteration: 24600 loss: 0.0010 lr: 0.02
2019-10-10 16:17:51 iteration: 24650 loss: 0.0011 lr: 0.02
2019-10-10 16:17:58 iteration: 24700 loss: 0.0012 lr: 0.02
2019-10-10 16:18:07 iteration: 24750 loss: 0.0009 lr: 0.02
2019-10-10 16:18:14 iteration: 24800 loss: 0.0011 lr: 0.02
2019-10-10 16:18:22 iteration: 24850 loss: 0.0010 lr: 0.02
2019-10-10 16:18:30 iteration: 24900 loss: 0.0010 lr: 0.02
2019-10-10 16:18:39 iteration: 24950 loss: 0.0011 lr: 0.02
2019-10-10 16:18:46 iteration: 25000 loss: 0.0011 lr: 0.02
2019-10-10 16:18:55 iteration: 25050 loss: 0.0012 lr: 0.02
2019-10-10 16:19:05 iteration: 25100 loss: 0.0013 lr: 0.02
2019-10-10 16:19:12 iteration: 25150 loss: 0.0011 lr: 0.02
2019-10-10 16:19:20 iteration: 25200 loss: 0.0011 lr: 0.02
2019-10-10 16:19:28 iteration: 25250 loss: 0.0012 lr: 0.02
2019-10-10 16:19:38 iteration: 25300 loss: 0.0011 lr: 0.02
2019-10-10 16:19:47 iteration: 25350 loss: 0.0011 lr: 0.02
2019-10-10 16:19:56 iteration: 25400 loss: 0.0011 lr: 0.02
2019-10-10 16:20:04 iteration: 25450 loss: 0.0012 lr: 0.02
2019-10-10 16:20:11 iteration: 25500 loss: 0.0011 lr: 0.02
2019-10-10 16:20:19 iteration: 25550 loss: 0.0010 lr: 0.02
2019-10-10 16:20:26 iteration: 25600 loss: 0.0013 lr: 0.02
2019-10-10 16:20:32 iteration: 25650 loss: 0.0010 lr: 0.02
2019-10-10 16:20:43 iteration: 25700 loss: 0.0010 lr: 0.02
2019-10-10 16:20:52 iteration: 25750 loss: 0.0010 lr: 0.02
2019-10-10 16:21:00 iteration: 25800 loss: 0.0009 lr: 0.02
2019-10-10 16:21:06 iteration: 25850 loss: 0.0012 lr: 0.02
2019-10-10 16:21:15 iteration: 25900 loss: 0.0012 lr: 0.02
2019-10-10 16:21:27 iteration: 25950 loss: 0.0011 lr: 0.02
2019-10-10 16:21:34 iteration: 26000 loss: 0.0012 lr: 0.02
2019-10-10 16:21:41 iteration: 26050 loss: 0.0011 lr: 0.02
2019-10-10 16:21:49 iteration: 26100 loss: 0.0011 lr: 0.02
2019-10-10 16:21:55 iteration: 26150 loss: 0.0010 lr: 0.02
2019-10-10 16:22:04 iteration: 26200 loss: 0.0010 lr: 0.02
2019-10-10 16:22:13 iteration: 26250 loss: 0.0012 lr: 0.02
2019-10-10 16:22:21 iteration: 26300 loss: 0.0011 lr: 0.02
2019-10-10 16:22:28 iteration: 26350 loss: 0.0010 lr: 0.02
2019-10-10 16:22:35 iteration: 26400 loss: 0.0011 lr: 0.02
2019-10-10 16:22:42 iteration: 26450 loss: 0.0011 lr: 0.02
2019-10-10 16:22:50 iteration: 26500 loss: 0.0010 lr: 0.02
2019-10-10 16:22:58 iteration: 26550 loss: 0.0011 lr: 0.02
2019-10-10 16:23:05 iteration: 26600 loss: 0.0010 lr: 0.02
2019-10-10 16:23:13 iteration: 26650 loss: 0.0010 lr: 0.02
2019-10-10 16:23:23 iteration: 26700 loss: 0.0010 lr: 0.02
2019-10-10 16:23:32 iteration: 26750 loss: 0.0011 lr: 0.02
2019-10-10 16:23:39 iteration: 26800 loss: 0.0015 lr: 0.02
2019-10-10 16:23:47 iteration: 26850 loss: 0.0011 lr: 0.02
2019-10-10 16:23:56 iteration: 26900 loss: 0.0012 lr: 0.02
2019-10-10 16:24:04 iteration: 26950 loss: 0.0011 lr: 0.02
2019-10-10 16:24:12 iteration: 27000 loss: 0.0012 lr: 0.02
2019-10-10 16:24:19 iteration: 27050 loss: 0.0011 lr: 0.02
2019-10-10 16:24:27 iteration: 27100 loss: 0.0010 lr: 0.02
2019-10-10 16:24:34 iteration: 27150 loss: 0.0010 lr: 0.02
2019-10-10 16:24:40 iteration: 27200 loss: 0.0011 lr: 0.02
2019-10-10 16:24:48 iteration: 27250 loss: 0.0010 lr: 0.02
2019-10-10 16:24:55 iteration: 27300 loss: 0.0010 lr: 0.02
2019-10-10 16:25:02 iteration: 27350 loss: 0.0010 lr: 0.02
2019-10-10 16:25:09 iteration: 27400 loss: 0.0010 lr: 0.02
2019-10-10 16:25:14 iteration: 27450 loss: 0.0010 lr: 0.02
2019-10-10 16:25:21 iteration: 27500 loss: 0.0011 lr: 0.02
2019-10-10 16:25:29 iteration: 27550 loss: 0.0012 lr: 0.02
2019-10-10 16:25:37 iteration: 27600 loss: 0.0011 lr: 0.02
2019-10-10 16:25:47 iteration: 27650 loss: 0.0011 lr: 0.02
2019-10-10 16:25:54 iteration: 27700 loss: 0.0010 lr: 0.02
2019-10-10 16:26:02 iteration: 27750 loss: 0.0011 lr: 0.02
2019-10-10 16:26:09 iteration: 27800 loss: 0.0011 lr: 0.02
2019-10-10 16:26:17 iteration: 27850 loss: 0.0011 lr: 0.02
2019-10-10 16:26:25 iteration: 27900 loss: 0.0009 lr: 0.02
2019-10-10 16:26:33 iteration: 27950 loss: 0.0011 lr: 0.02
2019-10-10 16:26:41 iteration: 28000 loss: 0.0011 lr: 0.02
2019-10-10 16:26:50 iteration: 28050 loss: 0.0010 lr: 0.02
2019-10-10 16:26:58 iteration: 28100 loss: 0.0011 lr: 0.02
2019-10-10 16:27:05 iteration: 28150 loss: 0.0010 lr: 0.02
2019-10-10 16:27:12 iteration: 28200 loss: 0.0011 lr: 0.02
2019-10-10 16:27:19 iteration: 28250 loss: 0.0010 lr: 0.02
2019-10-10 16:27:29 iteration: 28300 loss: 0.0010 lr: 0.02
2019-10-10 16:27:37 iteration: 28350 loss: 0.0010 lr: 0.02
2019-10-10 16:27:45 iteration: 28400 loss: 0.0011 lr: 0.02
2019-10-10 16:27:54 iteration: 28450 loss: 0.0011 lr: 0.02
2019-10-10 16:28:01 iteration: 28500 loss: 0.0011 lr: 0.02
2019-10-10 16:28:07 iteration: 28550 loss: 0.0010 lr: 0.02
2019-10-10 16:28:18 iteration: 28600 loss: 0.0013 lr: 0.02
2019-10-10 16:28:25 iteration: 28650 loss: 0.0011 lr: 0.02
2019-10-10 16:28:33 iteration: 28700 loss: 0.0013 lr: 0.02
2019-10-10 16:28:43 iteration: 28750 loss: 0.0010 lr: 0.02
2019-10-10 16:28:49 iteration: 28800 loss: 0.0010 lr: 0.02
2019-10-10 16:28:57 iteration: 28850 loss: 0.0011 lr: 0.02
2019-10-10 16:29:05 iteration: 28900 loss: 0.0009 lr: 0.02
2019-10-10 16:29:14 iteration: 28950 loss: 0.0009 lr: 0.02
2019-10-10 16:29:20 iteration: 29000 loss: 0.0010 lr: 0.02
2019-10-10 16:29:28 iteration: 29050 loss: 0.0009 lr: 0.02
2019-10-10 16:29:33 iteration: 29100 loss: 0.0009 lr: 0.02
2019-10-10 16:29:40 iteration: 29150 loss: 0.0010 lr: 0.02
2019-10-10 16:29:47 iteration: 29200 loss: 0.0011 lr: 0.02
2019-10-10 16:29:53 iteration: 29250 loss: 0.0011 lr: 0.02
2019-10-10 16:30:03 iteration: 29300 loss: 0.0012 lr: 0.02
2019-10-10 16:30:10 iteration: 29350 loss: 0.0013 lr: 0.02
2019-10-10 16:30:17 iteration: 29400 loss: 0.0011 lr: 0.02
2019-10-10 16:30:26 iteration: 29450 loss: 0.0010 lr: 0.02
2019-10-10 16:30:33 iteration: 29500 loss: 0.0011 lr: 0.02
2019-10-10 16:30:41 iteration: 29550 loss: 0.0010 lr: 0.02
2019-10-10 16:30:49 iteration: 29600 loss: 0.0011 lr: 0.02
2019-10-10 16:30:56 iteration: 29650 loss: 0.0012 lr: 0.02
2019-10-10 16:31:04 iteration: 29700 loss: 0.0012 lr: 0.02
2019-10-10 16:31:12 iteration: 29750 loss: 0.0011 lr: 0.02
2019-10-10 16:31:18 iteration: 29800 loss: 0.0009 lr: 0.02
2019-10-10 16:31:24 iteration: 29850 loss: 0.0012 lr: 0.02
2019-10-10 16:31:32 iteration: 29900 loss: 0.0010 lr: 0.02
2019-10-10 16:31:39 iteration: 29950 loss: 0.0010 lr: 0.02
2019-10-10 16:31:48 iteration: 30000 loss: 0.0010 lr: 0.02
2019-10-10 16:31:58 iteration: 30050 loss: 0.0011 lr: 0.02
2019-10-10 16:32:07 iteration: 30100 loss: 0.0009 lr: 0.02
2019-10-10 16:32:14 iteration: 30150 loss: 0.0010 lr: 0.02
2019-10-10 16:32:21 iteration: 30200 loss: 0.0011 lr: 0.02
2019-10-10 16:32:28 iteration: 30250 loss: 0.0011 lr: 0.02
2019-10-10 16:32:37 iteration: 30300 loss: 0.0010 lr: 0.02
2019-10-10 16:32:45 iteration: 30350 loss: 0.0010 lr: 0.02
2019-10-10 16:32:52 iteration: 30400 loss: 0.0011 lr: 0.02
2019-10-10 16:33:00 iteration: 30450 loss: 0.0010 lr: 0.02
2019-10-10 16:33:08 iteration: 30500 loss: 0.0010 lr: 0.02
2019-10-10 16:33:16 iteration: 30550 loss: 0.0010 lr: 0.02
2019-10-10 16:33:22 iteration: 30600 loss: 0.0009 lr: 0.02
2019-10-10 16:33:29 iteration: 30650 loss: 0.0010 lr: 0.02
2019-10-10 16:33:35 iteration: 30700 loss: 0.0010 lr: 0.02
2019-10-10 16:33:42 iteration: 30750 loss: 0.0010 lr: 0.02
2019-10-10 16:33:50 iteration: 30800 loss: 0.0010 lr: 0.02
2019-10-10 16:33:57 iteration: 30850 loss: 0.0010 lr: 0.02
2019-10-10 16:34:05 iteration: 30900 loss: 0.0010 lr: 0.02
2019-10-10 16:34:14 iteration: 30950 loss: 0.0009 lr: 0.02
2019-10-10 16:34:21 iteration: 31000 loss: 0.0010 lr: 0.02
2019-10-10 16:34:27 iteration: 31050 loss: 0.0009 lr: 0.02
2019-10-10 16:34:34 iteration: 31100 loss: 0.0012 lr: 0.02
2019-10-10 16:34:42 iteration: 31150 loss: 0.0010 lr: 0.02
2019-10-10 16:34:49 iteration: 31200 loss: 0.0008 lr: 0.02
2019-10-10 16:34:55 iteration: 31250 loss: 0.0010 lr: 0.02
2019-10-10 16:35:01 iteration: 31300 loss: 0.0010 lr: 0.02
2019-10-10 16:35:08 iteration: 31350 loss: 0.0010 lr: 0.02
2019-10-10 16:35:18 iteration: 31400 loss: 0.0011 lr: 0.02
2019-10-10 16:35:25 iteration: 31450 loss: 0.0011 lr: 0.02
2019-10-10 16:35:32 iteration: 31500 loss: 0.0010 lr: 0.02
2019-10-10 16:35:38 iteration: 31550 loss: 0.0010 lr: 0.02
2019-10-10 16:35:46 iteration: 31600 loss: 0.0010 lr: 0.02
2019-10-10 16:35:53 iteration: 31650 loss: 0.0011 lr: 0.02
2019-10-10 16:36:02 iteration: 31700 loss: 0.0012 lr: 0.02
2019-10-10 16:36:10 iteration: 31750 loss: 0.0010 lr: 0.02
2019-10-10 16:36:19 iteration: 31800 loss: 0.0010 lr: 0.02
2019-10-10 16:36:27 iteration: 31850 loss: 0.0010 lr: 0.02
2019-10-10 16:36:34 iteration: 31900 loss: 0.0009 lr: 0.02
2019-10-10 16:36:43 iteration: 31950 loss: 0.0011 lr: 0.02
2019-10-10 16:36:48 iteration: 32000 loss: 0.0011 lr: 0.02
2019-10-10 16:36:57 iteration: 32050 loss: 0.0011 lr: 0.02
2019-10-10 16:37:05 iteration: 32100 loss: 0.0009 lr: 0.02
2019-10-10 16:37:11 iteration: 32150 loss: 0.0010 lr: 0.02
2019-10-10 16:37:19 iteration: 32200 loss: 0.0009 lr: 0.02
2019-10-10 16:37:27 iteration: 32250 loss: 0.0011 lr: 0.02
2019-10-10 16:37:35 iteration: 32300 loss: 0.0009 lr: 0.02
2019-10-10 16:37:42 iteration: 32350 loss: 0.0011 lr: 0.02
2019-10-10 16:37:47 iteration: 32400 loss: 0.0010 lr: 0.02
2019-10-10 16:37:54 iteration: 32450 loss: 0.0010 lr: 0.02
2019-10-10 16:38:00 iteration: 32500 loss: 0.0009 lr: 0.02
2019-10-10 16:38:06 iteration: 32550 loss: 0.0010 lr: 0.02
2019-10-10 16:38:14 iteration: 32600 loss: 0.0012 lr: 0.02
2019-10-10 16:38:21 iteration: 32650 loss: 0.0010 lr: 0.02
2019-10-10 16:38:30 iteration: 32700 loss: 0.0010 lr: 0.02
2019-10-10 16:38:38 iteration: 32750 loss: 0.0010 lr: 0.02
2019-10-10 16:38:45 iteration: 32800 loss: 0.0010 lr: 0.02
2019-10-10 16:38:53 iteration: 32850 loss: 0.0010 lr: 0.02
2019-10-10 16:39:00 iteration: 32900 loss: 0.0010 lr: 0.02
2019-10-10 16:39:08 iteration: 32950 loss: 0.0010 lr: 0.02
2019-10-10 16:39:15 iteration: 33000 loss: 0.0010 lr: 0.02
2019-10-10 16:39:21 iteration: 33050 loss: 0.0010 lr: 0.02
2019-10-10 16:39:30 iteration: 33100 loss: 0.0012 lr: 0.02
2019-10-10 16:39:37 iteration: 33150 loss: 0.0009 lr: 0.02
2019-10-10 16:39:43 iteration: 33200 loss: 0.0010 lr: 0.02
2019-10-10 16:39:51 iteration: 33250 loss: 0.0011 lr: 0.02
2019-10-10 16:39:58 iteration: 33300 loss: 0.0009 lr: 0.02
2019-10-10 16:40:05 iteration: 33350 loss: 0.0011 lr: 0.02
2019-10-10 16:40:12 iteration: 33400 loss: 0.0009 lr: 0.02
2019-10-10 16:40:19 iteration: 33450 loss: 0.0012 lr: 0.02
2019-10-10 16:40:27 iteration: 33500 loss: 0.0010 lr: 0.02
2019-10-10 16:40:34 iteration: 33550 loss: 0.0011 lr: 0.02
2019-10-10 16:40:41 iteration: 33600 loss: 0.0011 lr: 0.02
2019-10-10 16:40:48 iteration: 33650 loss: 0.0010 lr: 0.02
2019-10-10 16:40:55 iteration: 33700 loss: 0.0010 lr: 0.02
2019-10-10 16:41:02 iteration: 33750 loss: 0.0011 lr: 0.02
2019-10-10 16:41:09 iteration: 33800 loss: 0.0011 lr: 0.02
2019-10-10 16:41:18 iteration: 33850 loss: 0.0010 lr: 0.02
2019-10-10 16:41:26 iteration: 33900 loss: 0.0009 lr: 0.02
2019-10-10 16:41:33 iteration: 33950 loss: 0.0009 lr: 0.02
2019-10-10 16:41:39 iteration: 34000 loss: 0.0010 lr: 0.02
2019-10-10 16:41:47 iteration: 34050 loss: 0.0010 lr: 0.02
2019-10-10 16:41:54 iteration: 34100 loss: 0.0011 lr: 0.02
2019-10-10 16:42:00 iteration: 34150 loss: 0.0010 lr: 0.02
2019-10-10 16:42:08 iteration: 34200 loss: 0.0009 lr: 0.02
2019-10-10 16:42:16 iteration: 34250 loss: 0.0008 lr: 0.02
2019-10-10 16:42:23 iteration: 34300 loss: 0.0011 lr: 0.02
2019-10-10 16:42:33 iteration: 34350 loss: 0.0012 lr: 0.02
2019-10-10 16:42:40 iteration: 34400 loss: 0.0009 lr: 0.02
2019-10-10 16:42:47 iteration: 34450 loss: 0.0010 lr: 0.02
2019-10-10 16:42:55 iteration: 34500 loss: 0.0012 lr: 0.02
2019-10-10 16:43:02 iteration: 34550 loss: 0.0010 lr: 0.02
2019-10-10 16:43:11 iteration: 34600 loss: 0.0009 lr: 0.02
2019-10-10 16:43:20 iteration: 34650 loss: 0.0011 lr: 0.02
2019-10-10 16:43:27 iteration: 34700 loss: 0.0011 lr: 0.02
2019-10-10 16:43:33 iteration: 34750 loss: 0.0012 lr: 0.02
2019-10-10 16:43:39 iteration: 34800 loss: 0.0010 lr: 0.02
2019-10-10 16:43:45 iteration: 34850 loss: 0.0009 lr: 0.02
2019-10-10 16:43:53 iteration: 34900 loss: 0.0010 lr: 0.02
2019-10-10 16:44:01 iteration: 34950 loss: 0.0010 lr: 0.02
2019-10-10 16:44:08 iteration: 35000 loss: 0.0010 lr: 0.02
2019-10-10 16:44:16 iteration: 35050 loss: 0.0010 lr: 0.02
2019-10-10 16:44:23 iteration: 35100 loss: 0.0011 lr: 0.02
2019-10-10 16:44:30 iteration: 35150 loss: 0.0009 lr: 0.02
2019-10-10 16:44:36 iteration: 35200 loss: 0.0010 lr: 0.02
2019-10-10 16:44:44 iteration: 35250 loss: 0.0009 lr: 0.02
2019-10-10 16:44:52 iteration: 35300 loss: 0.0010 lr: 0.02
2019-10-10 16:45:02 iteration: 35350 loss: 0.0010 lr: 0.02
2019-10-10 16:45:09 iteration: 35400 loss: 0.0010 lr: 0.02
2019-10-10 16:45:16 iteration: 35450 loss: 0.0009 lr: 0.02
2019-10-10 16:45:23 iteration: 35500 loss: 0.0013 lr: 0.02
2019-10-10 16:45:32 iteration: 35550 loss: 0.0011 lr: 0.02
2019-10-10 16:45:38 iteration: 35600 loss: 0.0009 lr: 0.02
2019-10-10 16:45:45 iteration: 35650 loss: 0.0008 lr: 0.02
2019-10-10 16:45:52 iteration: 35700 loss: 0.0010 lr: 0.02
2019-10-10 16:46:00 iteration: 35750 loss: 0.0010 lr: 0.02
2019-10-10 16:46:09 iteration: 35800 loss: 0.0010 lr: 0.02
2019-10-10 16:46:16 iteration: 35850 loss: 0.0012 lr: 0.02
2019-10-10 16:46:22 iteration: 35900 loss: 0.0009 lr: 0.02
2019-10-10 16:46:28 iteration: 35950 loss: 0.0010 lr: 0.02
2019-10-10 16:46:35 iteration: 36000 loss: 0.0009 lr: 0.02
2019-10-10 16:46:41 iteration: 36050 loss: 0.0008 lr: 0.02
2019-10-10 16:46:48 iteration: 36100 loss: 0.0010 lr: 0.02
2019-10-10 16:46:53 iteration: 36150 loss: 0.0012 lr: 0.02
2019-10-10 16:47:00 iteration: 36200 loss: 0.0009 lr: 0.02
2019-10-10 16:47:08 iteration: 36250 loss: 0.0010 lr: 0.02
2019-10-10 16:47:15 iteration: 36300 loss: 0.0010 lr: 0.02
2019-10-10 16:47:23 iteration: 36350 loss: 0.0011 lr: 0.02
2019-10-10 16:47:30 iteration: 36400 loss: 0.0011 lr: 0.02
2019-10-10 16:47:37 iteration: 36450 loss: 0.0010 lr: 0.02
2019-10-10 16:47:46 iteration: 36500 loss: 0.0010 lr: 0.02
2019-10-10 16:47:55 iteration: 36550 loss: 0.0010 lr: 0.02
2019-10-10 16:48:01 iteration: 36600 loss: 0.0009 lr: 0.02
2019-10-10 16:48:08 iteration: 36650 loss: 0.0012 lr: 0.02
2019-10-10 16:48:15 iteration: 36700 loss: 0.0008 lr: 0.02
2019-10-10 16:48:22 iteration: 36750 loss: 0.0009 lr: 0.02
2019-10-10 16:48:30 iteration: 36800 loss: 0.0011 lr: 0.02
2019-10-10 16:48:35 iteration: 36850 loss: 0.0010 lr: 0.02
2019-10-10 16:48:44 iteration: 36900 loss: 0.0010 lr: 0.02
2019-10-10 16:48:50 iteration: 36950 loss: 0.0009 lr: 0.02
2019-10-10 16:48:55 iteration: 37000 loss: 0.0010 lr: 0.02
2019-10-10 16:49:01 iteration: 37050 loss: 0.0010 lr: 0.02
2019-10-10 16:49:09 iteration: 37100 loss: 0.0011 lr: 0.02
2019-10-10 16:49:18 iteration: 37150 loss: 0.0010 lr: 0.02
2019-10-10 16:49:25 iteration: 37200 loss: 0.0010 lr: 0.02
2019-10-10 16:49:31 iteration: 37250 loss: 0.0008 lr: 0.02
2019-10-10 16:49:39 iteration: 37300 loss: 0.0010 lr: 0.02
2019-10-10 16:49:46 iteration: 37350 loss: 0.0010 lr: 0.02
2019-10-10 16:49:55 iteration: 37400 loss: 0.0009 lr: 0.02
2019-10-10 16:50:02 iteration: 37450 loss: 0.0010 lr: 0.02
2019-10-10 16:50:09 iteration: 37500 loss: 0.0009 lr: 0.02
2019-10-10 16:50:17 iteration: 37550 loss: 0.0011 lr: 0.02
2019-10-10 16:50:25 iteration: 37600 loss: 0.0009 lr: 0.02
2019-10-10 16:50:32 iteration: 37650 loss: 0.0009 lr: 0.02
2019-10-10 16:50:41 iteration: 37700 loss: 0.0012 lr: 0.02
2019-10-10 16:50:50 iteration: 37750 loss: 0.0010 lr: 0.02
2019-10-10 16:50:56 iteration: 37800 loss: 0.0009 lr: 0.02
2019-10-10 16:51:02 iteration: 37850 loss: 0.0010 lr: 0.02
2019-10-10 16:51:08 iteration: 37900 loss: 0.0010 lr: 0.02
2019-10-10 16:51:16 iteration: 37950 loss: 0.0012 lr: 0.02
2019-10-10 16:51:24 iteration: 38000 loss: 0.0009 lr: 0.02
2019-10-10 16:51:30 iteration: 38050 loss: 0.0010 lr: 0.02
2019-10-10 16:51:39 iteration: 38100 loss: 0.0010 lr: 0.02
2019-10-10 16:51:48 iteration: 38150 loss: 0.0010 lr: 0.02
2019-10-10 16:51:54 iteration: 38200 loss: 0.0010 lr: 0.02
2019-10-10 16:52:01 iteration: 38250 loss: 0.0009 lr: 0.02
2019-10-10 16:52:08 iteration: 38300 loss: 0.0010 lr: 0.02
2019-10-10 16:52:15 iteration: 38350 loss: 0.0010 lr: 0.02
2019-10-10 16:52:20 iteration: 38400 loss: 0.0011 lr: 0.02
2019-10-10 16:52:28 iteration: 38450 loss: 0.0011 lr: 0.02
2019-10-10 16:52:34 iteration: 38500 loss: 0.0010 lr: 0.02
2019-10-10 16:52:41 iteration: 38550 loss: 0.0009 lr: 0.02
2019-10-10 16:52:46 iteration: 38600 loss: 0.0010 lr: 0.02
2019-10-10 16:52:52 iteration: 38650 loss: 0.0011 lr: 0.02
2019-10-10 16:52:59 iteration: 38700 loss: 0.0009 lr: 0.02
2019-10-10 16:53:06 iteration: 38750 loss: 0.0010 lr: 0.02
2019-10-10 16:53:12 iteration: 38800 loss: 0.0010 lr: 0.02
2019-10-10 16:53:19 iteration: 38850 loss: 0.0009 lr: 0.02
2019-10-10 16:53:28 iteration: 38900 loss: 0.0010 lr: 0.02
2019-10-10 16:53:36 iteration: 38950 loss: 0.0010 lr: 0.02
2019-10-10 16:53:44 iteration: 39000 loss: 0.0010 lr: 0.02
2019-10-10 16:53:51 iteration: 39050 loss: 0.0011 lr: 0.02
2019-10-10 16:53:56 iteration: 39100 loss: 0.0010 lr: 0.02
2019-10-10 16:54:03 iteration: 39150 loss: 0.0011 lr: 0.02
2019-10-10 16:54:10 iteration: 39200 loss: 0.0012 lr: 0.02
2019-10-10 16:54:17 iteration: 39250 loss: 0.0011 lr: 0.02
2019-10-10 16:54:24 iteration: 39300 loss: 0.0010 lr: 0.02
2019-10-10 16:54:33 iteration: 39350 loss: 0.0011 lr: 0.02
2019-10-10 16:54:41 iteration: 39400 loss: 0.0010 lr: 0.02
2019-10-10 16:54:49 iteration: 39450 loss: 0.0009 lr: 0.02
2019-10-10 16:54:56 iteration: 39500 loss: 0.0009 lr: 0.02
2019-10-10 16:55:03 iteration: 39550 loss: 0.0009 lr: 0.02
2019-10-10 16:55:13 iteration: 39600 loss: 0.0012 lr: 0.02
2019-10-10 16:55:19 iteration: 39650 loss: 0.0010 lr: 0.02
2019-10-10 16:55:26 iteration: 39700 loss: 0.0010 lr: 0.02
2019-10-10 16:55:31 iteration: 39750 loss: 0.0011 lr: 0.02
2019-10-10 16:55:38 iteration: 39800 loss: 0.0010 lr: 0.02
2019-10-10 16:55:42 iteration: 39850 loss: 0.0011 lr: 0.02
2019-10-10 16:55:50 iteration: 39900 loss: 0.0009 lr: 0.02
2019-10-10 16:55:57 iteration: 39950 loss: 0.0009 lr: 0.02
2019-10-10 16:56:01 iteration: 40000 loss: 0.0009 lr: 0.02
2019-10-10 16:56:09 iteration: 40050 loss: 0.0010 lr: 0.02
2019-10-10 16:56:15 iteration: 40100 loss: 0.0009 lr: 0.02
2019-10-10 16:56:22 iteration: 40150 loss: 0.0010 lr: 0.02
2019-10-10 16:56:29 iteration: 40200 loss: 0.0010 lr: 0.02
2019-10-10 16:56:35 iteration: 40250 loss: 0.0011 lr: 0.02
2019-10-10 16:56:44 iteration: 40300 loss: 0.0010 lr: 0.02
2019-10-10 16:56:52 iteration: 40350 loss: 0.0011 lr: 0.02
2019-10-10 16:57:00 iteration: 40400 loss: 0.0010 lr: 0.02
2019-10-10 16:57:05 iteration: 40450 loss: 0.0009 lr: 0.02
2019-10-10 16:57:12 iteration: 40500 loss: 0.0008 lr: 0.02
2019-10-10 16:57:20 iteration: 40550 loss: 0.0010 lr: 0.02
2019-10-10 16:57:28 iteration: 40600 loss: 0.0010 lr: 0.02
2019-10-10 16:57:34 iteration: 40650 loss: 0.0010 lr: 0.02
2019-10-10 16:57:40 iteration: 40700 loss: 0.0011 lr: 0.02
2019-10-10 16:57:48 iteration: 40750 loss: 0.0010 lr: 0.02
2019-10-10 16:57:55 iteration: 40800 loss: 0.0009 lr: 0.02
2019-10-10 16:58:01 iteration: 40850 loss: 0.0011 lr: 0.02
2019-10-10 16:58:07 iteration: 40900 loss: 0.0010 lr: 0.02
2019-10-10 16:58:13 iteration: 40950 loss: 0.0011 lr: 0.02
2019-10-10 16:58:19 iteration: 41000 loss: 0.0010 lr: 0.02
2019-10-10 16:58:26 iteration: 41050 loss: 0.0008 lr: 0.02
2019-10-10 16:58:32 iteration: 41100 loss: 0.0009 lr: 0.02
2019-10-10 16:58:40 iteration: 41150 loss: 0.0010 lr: 0.02
2019-10-10 16:58:47 iteration: 41200 loss: 0.0009 lr: 0.02
2019-10-10 16:58:54 iteration: 41250 loss: 0.0010 lr: 0.02
2019-10-10 16:59:01 iteration: 41300 loss: 0.0009 lr: 0.02
2019-10-10 16:59:07 iteration: 41350 loss: 0.0009 lr: 0.02
2019-10-10 16:59:14 iteration: 41400 loss: 0.0010 lr: 0.02
2019-10-10 16:59:21 iteration: 41450 loss: 0.0011 lr: 0.02
2019-10-10 16:59:27 iteration: 41500 loss: 0.0009 lr: 0.02
2019-10-10 16:59:35 iteration: 41550 loss: 0.0010 lr: 0.02
2019-10-10 16:59:43 iteration: 41600 loss: 0.0009 lr: 0.02
2019-10-10 16:59:50 iteration: 41650 loss: 0.0010 lr: 0.02
2019-10-10 16:59:56 iteration: 41700 loss: 0.0009 lr: 0.02
2019-10-10 17:00:03 iteration: 41750 loss: 0.0009 lr: 0.02
2019-10-10 17:00:09 iteration: 41800 loss: 0.0011 lr: 0.02
2019-10-10 17:00:16 iteration: 41850 loss: 0.0011 lr: 0.02
2019-10-10 17:00:24 iteration: 41900 loss: 0.0010 lr: 0.02
2019-10-10 17:00:30 iteration: 41950 loss: 0.0009 lr: 0.02
2019-10-10 17:00:37 iteration: 42000 loss: 0.0009 lr: 0.02
2019-10-10 17:00:42 iteration: 42050 loss: 0.0009 lr: 0.02
2019-10-10 17:00:49 iteration: 42100 loss: 0.0009 lr: 0.02
2019-10-10 17:00:55 iteration: 42150 loss: 0.0011 lr: 0.02
2019-10-10 17:01:02 iteration: 42200 loss: 0.0009 lr: 0.02
2019-10-10 17:01:08 iteration: 42250 loss: 0.0009 lr: 0.02
2019-10-10 17:01:14 iteration: 42300 loss: 0.0010 lr: 0.02
2019-10-10 17:01:21 iteration: 42350 loss: 0.0010 lr: 0.02
2019-10-10 17:01:26 iteration: 42400 loss: 0.0010 lr: 0.02
2019-10-10 17:01:33 iteration: 42450 loss: 0.0009 lr: 0.02
2019-10-10 17:01:38 iteration: 42500 loss: 0.0010 lr: 0.02
2019-10-10 17:01:45 iteration: 42550 loss: 0.0010 lr: 0.02
2019-10-10 17:01:51 iteration: 42600 loss: 0.0011 lr: 0.02
2019-10-10 17:01:58 iteration: 42650 loss: 0.0010 lr: 0.02
2019-10-10 17:02:04 iteration: 42700 loss: 0.0009 lr: 0.02
2019-10-10 17:02:11 iteration: 42750 loss: 0.0010 lr: 0.02
2019-10-10 17:02:18 iteration: 42800 loss: 0.0010 lr: 0.02
2019-10-10 17:02:24 iteration: 42850 loss: 0.0009 lr: 0.02
2019-10-10 17:02:32 iteration: 42900 loss: 0.0011 lr: 0.02
2019-10-10 17:02:40 iteration: 42950 loss: 0.0010 lr: 0.02
2019-10-10 17:02:46 iteration: 43000 loss: 0.0012 lr: 0.02
2019-10-10 17:02:52 iteration: 43050 loss: 0.0009 lr: 0.02
2019-10-10 17:02:59 iteration: 43100 loss: 0.0010 lr: 0.02
2019-10-10 17:03:05 iteration: 43150 loss: 0.0010 lr: 0.02
2019-10-10 17:03:11 iteration: 43200 loss: 0.0009 lr: 0.02
2019-10-10 17:03:16 iteration: 43250 loss: 0.0011 lr: 0.02
2019-10-10 17:03:21 iteration: 43300 loss: 0.0010 lr: 0.02
2019-10-10 17:03:29 iteration: 43350 loss: 0.0009 lr: 0.02
2019-10-10 17:03:36 iteration: 43400 loss: 0.0010 lr: 0.02
2019-10-10 17:03:41 iteration: 43450 loss: 0.0010 lr: 0.02
2019-10-10 17:03:47 iteration: 43500 loss: 0.0010 lr: 0.02
2019-10-10 17:03:53 iteration: 43550 loss: 0.0010 lr: 0.02
2019-10-10 17:04:00 iteration: 43600 loss: 0.0009 lr: 0.02
2019-10-10 17:04:06 iteration: 43650 loss: 0.0010 lr: 0.02
2019-10-10 17:04:14 iteration: 43700 loss: 0.0011 lr: 0.02
2019-10-10 17:04:21 iteration: 43750 loss: 0.0010 lr: 0.02
2019-10-10 17:04:27 iteration: 43800 loss: 0.0009 lr: 0.02
2019-10-10 17:04:32 iteration: 43850 loss: 0.0011 lr: 0.02
2019-10-10 17:04:38 iteration: 43900 loss: 0.0010 lr: 0.02
2019-10-10 17:04:45 iteration: 43950 loss: 0.0009 lr: 0.02
2019-10-10 17:04:51 iteration: 44000 loss: 0.0010 lr: 0.02
2019-10-10 17:04:59 iteration: 44050 loss: 0.0011 lr: 0.02
2019-10-10 17:05:05 iteration: 44100 loss: 0.0011 lr: 0.02
2019-10-10 17:05:13 iteration: 44150 loss: 0.0009 lr: 0.02
2019-10-10 17:05:20 iteration: 44200 loss: 0.0010 lr: 0.02
2019-10-10 17:05:26 iteration: 44250 loss: 0.0010 lr: 0.02
2019-10-10 17:05:32 iteration: 44300 loss: 0.0010 lr: 0.02
2019-10-10 17:05:39 iteration: 44350 loss: 0.0009 lr: 0.02
2019-10-10 17:05:46 iteration: 44400 loss: 0.0010 lr: 0.02
2019-10-10 17:05:53 iteration: 44450 loss: 0.0010 lr: 0.02
2019-10-10 17:06:03 iteration: 44500 loss: 0.0009 lr: 0.02
2019-10-10 17:06:09 iteration: 44550 loss: 0.0009 lr: 0.02
2019-10-10 17:06:15 iteration: 44600 loss: 0.0009 lr: 0.02
2019-10-10 17:06:22 iteration: 44650 loss: 0.0008 lr: 0.02
2019-10-10 17:06:29 iteration: 44700 loss: 0.0009 lr: 0.02
2019-10-10 17:06:36 iteration: 44750 loss: 0.0009 lr: 0.02
2019-10-10 17:06:44 iteration: 44800 loss: 0.0009 lr: 0.02
2019-10-10 17:06:51 iteration: 44850 loss: 0.0010 lr: 0.02
2019-10-10 17:06:58 iteration: 44900 loss: 0.0009 lr: 0.02
2019-10-10 17:07:03 iteration: 44950 loss: 0.0009 lr: 0.02
2019-10-10 17:07:10 iteration: 45000 loss: 0.0009 lr: 0.02
2019-10-10 17:07:17 iteration: 45050 loss: 0.0009 lr: 0.02
2019-10-10 17:07:24 iteration: 45100 loss: 0.0010 lr: 0.02
2019-10-10 17:07:31 iteration: 45150 loss: 0.0010 lr: 0.02
2019-10-10 17:07:39 iteration: 45200 loss: 0.0010 lr: 0.02
2019-10-10 17:07:45 iteration: 45250 loss: 0.0009 lr: 0.02
2019-10-10 17:07:50 iteration: 45300 loss: 0.0009 lr: 0.02
2019-10-10 17:07:58 iteration: 45350 loss: 0.0009 lr: 0.02
2019-10-10 17:08:05 iteration: 45400 loss: 0.0010 lr: 0.02
2019-10-10 17:08:11 iteration: 45450 loss: 0.0009 lr: 0.02
2019-10-10 17:08:19 iteration: 45500 loss: 0.0008 lr: 0.02
2019-10-10 17:08:25 iteration: 45550 loss: 0.0009 lr: 0.02
2019-10-10 17:08:30 iteration: 45600 loss: 0.0009 lr: 0.02
2019-10-10 17:08:36 iteration: 45650 loss: 0.0011 lr: 0.02
2019-10-10 17:08:43 iteration: 45700 loss: 0.0010 lr: 0.02
2019-10-10 17:08:49 iteration: 45750 loss: 0.0008 lr: 0.02
2019-10-10 17:08:55 iteration: 45800 loss: 0.0010 lr: 0.02
2019-10-10 17:09:00 iteration: 45850 loss: 0.0010 lr: 0.02
2019-10-10 17:09:07 iteration: 45900 loss: 0.0009 lr: 0.02
2019-10-10 17:09:14 iteration: 45950 loss: 0.0011 lr: 0.02
2019-10-10 17:09:20 iteration: 46000 loss: 0.0009 lr: 0.02
2019-10-10 17:09:26 iteration: 46050 loss: 0.0009 lr: 0.02
2019-10-10 17:09:31 iteration: 46100 loss: 0.0010 lr: 0.02
2019-10-10 17:09:38 iteration: 46150 loss: 0.0011 lr: 0.02
2019-10-10 17:09:44 iteration: 46200 loss: 0.0009 lr: 0.02
2019-10-10 17:09:49 iteration: 46250 loss: 0.0009 lr: 0.02
2019-10-10 17:09:55 iteration: 46300 loss: 0.0009 lr: 0.02
2019-10-10 17:10:01 iteration: 46350 loss: 0.0010 lr: 0.02
2019-10-10 17:10:08 iteration: 46400 loss: 0.0008 lr: 0.02
2019-10-10 17:10:15 iteration: 46450 loss: 0.0009 lr: 0.02
2019-10-10 17:10:21 iteration: 46500 loss: 0.0010 lr: 0.02
2019-10-10 17:10:27 iteration: 46550 loss: 0.0010 lr: 0.02
2019-10-10 17:10:34 iteration: 46600 loss: 0.0009 lr: 0.02
2019-10-10 17:10:41 iteration: 46650 loss: 0.0010 lr: 0.02
2019-10-10 17:10:49 iteration: 46700 loss: 0.0009 lr: 0.02
2019-10-10 17:10:57 iteration: 46750 loss: 0.0010 lr: 0.02
2019-10-10 17:11:03 iteration: 46800 loss: 0.0009 lr: 0.02
2019-10-10 17:11:10 iteration: 46850 loss: 0.0008 lr: 0.02
2019-10-10 17:11:16 iteration: 46900 loss: 0.0008 lr: 0.02
2019-10-10 17:11:21 iteration: 46950 loss: 0.0010 lr: 0.02
2019-10-10 17:11:28 iteration: 47000 loss: 0.0010 lr: 0.02
2019-10-10 17:11:36 iteration: 47050 loss: 0.0010 lr: 0.02
2019-10-10 17:11:44 iteration: 47100 loss: 0.0011 lr: 0.02
2019-10-10 17:11:50 iteration: 47150 loss: 0.0010 lr: 0.02
2019-10-10 17:11:58 iteration: 47200 loss: 0.0009 lr: 0.02
2019-10-10 17:12:06 iteration: 47250 loss: 0.0009 lr: 0.02
2019-10-10 17:12:11 iteration: 47300 loss: 0.0009 lr: 0.02
2019-10-10 17:12:16 iteration: 47350 loss: 0.0010 lr: 0.02
2019-10-10 17:12:22 iteration: 47400 loss: 0.0010 lr: 0.02
2019-10-10 17:12:30 iteration: 47450 loss: 0.0010 lr: 0.02
2019-10-10 17:12:36 iteration: 47500 loss: 0.0010 lr: 0.02
2019-10-10 17:12:42 iteration: 47550 loss: 0.0010 lr: 0.02
2019-10-10 17:12:48 iteration: 47600 loss: 0.0009 lr: 0.02
2019-10-10 17:12:53 iteration: 47650 loss: 0.0009 lr: 0.02
2019-10-10 17:12:59 iteration: 47700 loss: 0.0009 lr: 0.02
2019-10-10 17:13:06 iteration: 47750 loss: 0.0009 lr: 0.02
2019-10-10 17:13:17 iteration: 47800 loss: 0.0010 lr: 0.02
2019-10-10 17:13:23 iteration: 47850 loss: 0.0008 lr: 0.02
2019-10-10 17:13:30 iteration: 47900 loss: 0.0009 lr: 0.02
2019-10-10 17:13:37 iteration: 47950 loss: 0.0008 lr: 0.02
2019-10-10 17:13:44 iteration: 48000 loss: 0.0011 lr: 0.02
2019-10-10 17:13:52 iteration: 48050 loss: 0.0010 lr: 0.02
2019-10-10 17:13:58 iteration: 48100 loss: 0.0011 lr: 0.02
2019-10-10 17:14:05 iteration: 48150 loss: 0.0010 lr: 0.02
2019-10-10 17:14:12 iteration: 48200 loss: 0.0009 lr: 0.02
2019-10-10 17:14:19 iteration: 48250 loss: 0.0010 lr: 0.02
2019-10-10 17:14:24 iteration: 48300 loss: 0.0010 lr: 0.02
2019-10-10 17:14:30 iteration: 48350 loss: 0.0010 lr: 0.02
2019-10-10 17:14:37 iteration: 48400 loss: 0.0009 lr: 0.02
2019-10-10 17:14:44 iteration: 48450 loss: 0.0010 lr: 0.02
2019-10-10 17:14:53 iteration: 48500 loss: 0.0009 lr: 0.02
2019-10-10 17:15:00 iteration: 48550 loss: 0.0009 lr: 0.02
2019-10-10 17:15:06 iteration: 48600 loss: 0.0008 lr: 0.02
2019-10-10 17:15:13 iteration: 48650 loss: 0.0009 lr: 0.02
2019-10-10 17:15:19 iteration: 48700 loss: 0.0010 lr: 0.02
2019-10-10 17:15:27 iteration: 48750 loss: 0.0011 lr: 0.02
2019-10-10 17:15:33 iteration: 48800 loss: 0.0008 lr: 0.02
2019-10-10 17:15:40 iteration: 48850 loss: 0.0010 lr: 0.02
2019-10-10 17:15:44 iteration: 48900 loss: 0.0009 lr: 0.02
2019-10-10 17:15:50 iteration: 48950 loss: 0.0010 lr: 0.02
2019-10-10 17:15:55 iteration: 49000 loss: 0.0010 lr: 0.02
2019-10-10 17:16:02 iteration: 49050 loss: 0.0010 lr: 0.02
2019-10-10 17:16:06 iteration: 49100 loss: 0.0009 lr: 0.02
2019-10-10 17:16:12 iteration: 49150 loss: 0.0008 lr: 0.02
2019-10-10 17:16:19 iteration: 49200 loss: 0.0011 lr: 0.02
2019-10-10 17:16:26 iteration: 49250 loss: 0.0011 lr: 0.02
2019-10-10 17:16:32 iteration: 49300 loss: 0.0009 lr: 0.02
2019-10-10 17:16:38 iteration: 49350 loss: 0.0010 lr: 0.02
2019-10-10 17:16:45 iteration: 49400 loss: 0.0009 lr: 0.02
2019-10-10 17:16:52 iteration: 49450 loss: 0.0011 lr: 0.02
2019-10-10 17:16:58 iteration: 49500 loss: 0.0009 lr: 0.02
2019-10-10 17:17:05 iteration: 49550 loss: 0.0009 lr: 0.02
2019-10-10 17:17:12 iteration: 49600 loss: 0.0010 lr: 0.02
2019-10-10 17:17:18 iteration: 49650 loss: 0.0009 lr: 0.02
2019-10-10 17:17:24 iteration: 49700 loss: 0.0011 lr: 0.02
2019-10-10 17:17:31 iteration: 49750 loss: 0.0009 lr: 0.02
2019-10-10 17:17:37 iteration: 49800 loss: 0.0009 lr: 0.02
2019-10-10 17:17:43 iteration: 49850 loss: 0.0010 lr: 0.02
2019-10-10 17:17:48 iteration: 49900 loss: 0.0010 lr: 0.02
2019-10-10 17:17:54 iteration: 49950 loss: 0.0010 lr: 0.02
2019-10-10 17:18:00 iteration: 50000 loss: 0.0008 lr: 0.02
2019-10-10 17:18:09 iteration: 50050 loss: 0.0009 lr: 0.02
2019-10-10 17:18:16 iteration: 50100 loss: 0.0009 lr: 0.02
2019-10-10 17:18:22 iteration: 50150 loss: 0.0008 lr: 0.02
2019-10-10 17:18:28 iteration: 50200 loss: 0.0009 lr: 0.02
2019-10-10 17:18:35 iteration: 50250 loss: 0.0010 lr: 0.02
2019-10-10 17:18:42 iteration: 50300 loss: 0.0010 lr: 0.02
2019-10-10 17:18:49 iteration: 50350 loss: 0.0010 lr: 0.02
2019-10-10 17:18:57 iteration: 50400 loss: 0.0008 lr: 0.02
2019-10-10 17:19:02 iteration: 50450 loss: 0.0009 lr: 0.02
2019-10-10 17:19:08 iteration: 50500 loss: 0.0009 lr: 0.02
2019-10-10 17:19:12 iteration: 50550 loss: 0.0010 lr: 0.02
2019-10-10 17:19:20 iteration: 50600 loss: 0.0010 lr: 0.02
2019-10-10 17:19:28 iteration: 50650 loss: 0.0009 lr: 0.02
2019-10-10 17:19:35 iteration: 50700 loss: 0.0009 lr: 0.02
2019-10-10 17:19:41 iteration: 50750 loss: 0.0009 lr: 0.02
2019-10-10 17:19:48 iteration: 50800 loss: 0.0008 lr: 0.02
2019-10-10 17:19:54 iteration: 50850 loss: 0.0008 lr: 0.02
2019-10-10 17:20:01 iteration: 50900 loss: 0.0009 lr: 0.02
2019-10-10 17:20:07 iteration: 50950 loss: 0.0009 lr: 0.02
2019-10-10 17:20:12 iteration: 51000 loss: 0.0010 lr: 0.02
2019-10-10 17:20:22 iteration: 51050 loss: 0.0009 lr: 0.02
2019-10-10 17:20:28 iteration: 51100 loss: 0.0009 lr: 0.02
2019-10-10 17:20:33 iteration: 51150 loss: 0.0009 lr: 0.02
2019-10-10 17:20:39 iteration: 51200 loss: 0.0010 lr: 0.02
2019-10-10 17:20:47 iteration: 51250 loss: 0.0008 lr: 0.02
2019-10-10 17:20:55 iteration: 51300 loss: 0.0009 lr: 0.02
2019-10-10 17:21:01 iteration: 51350 loss: 0.0010 lr: 0.02
2019-10-10 17:21:07 iteration: 51400 loss: 0.0008 lr: 0.02
2019-10-10 17:21:14 iteration: 51450 loss: 0.0009 lr: 0.02
2019-10-10 17:21:20 iteration: 51500 loss: 0.0009 lr: 0.02
2019-10-10 17:21:27 iteration: 51550 loss: 0.0009 lr: 0.02
2019-10-10 17:21:33 iteration: 51600 loss: 0.0008 lr: 0.02
2019-10-10 17:21:40 iteration: 51650 loss: 0.0009 lr: 0.02
2019-10-10 17:21:45 iteration: 51700 loss: 0.0008 lr: 0.02
2019-10-10 17:21:53 iteration: 51750 loss: 0.0010 lr: 0.02
2019-10-10 17:21:59 iteration: 51800 loss: 0.0011 lr: 0.02
2019-10-10 17:22:06 iteration: 51850 loss: 0.0009 lr: 0.02
2019-10-10 17:22:11 iteration: 51900 loss: 0.0011 lr: 0.02
2019-10-10 17:22:17 iteration: 51950 loss: 0.0009 lr: 0.02
2019-10-10 17:22:23 iteration: 52000 loss: 0.0008 lr: 0.02
2019-10-10 17:22:29 iteration: 52050 loss: 0.0010 lr: 0.02
2019-10-10 17:22:37 iteration: 52100 loss: 0.0009 lr: 0.02
2019-10-10 17:22:44 iteration: 52150 loss: 0.0008 lr: 0.02
2019-10-10 17:22:52 iteration: 52200 loss: 0.0009 lr: 0.02
2019-10-10 17:22:57 iteration: 52250 loss: 0.0010 lr: 0.02
2019-10-10 17:23:03 iteration: 52300 loss: 0.0009 lr: 0.02
2019-10-10 17:23:08 iteration: 52350 loss: 0.0009 lr: 0.02
2019-10-10 17:23:14 iteration: 52400 loss: 0.0008 lr: 0.02
2019-10-10 17:23:21 iteration: 52450 loss: 0.0008 lr: 0.02
2019-10-10 17:23:26 iteration: 52500 loss: 0.0010 lr: 0.02
2019-10-10 17:23:33 iteration: 52550 loss: 0.0010 lr: 0.02
2019-10-10 17:23:40 iteration: 52600 loss: 0.0009 lr: 0.02
2019-10-10 17:23:48 iteration: 52650 loss: 0.0008 lr: 0.02
2019-10-10 17:23:54 iteration: 52700 loss: 0.0009 lr: 0.02
2019-10-10 17:24:00 iteration: 52750 loss: 0.0009 lr: 0.02
2019-10-10 17:24:06 iteration: 52800 loss: 0.0009 lr: 0.02
2019-10-10 17:24:12 iteration: 52850 loss: 0.0009 lr: 0.02
2019-10-10 17:24:18 iteration: 52900 loss: 0.0011 lr: 0.02
2019-10-10 17:24:23 iteration: 52950 loss: 0.0010 lr: 0.02
2019-10-10 17:24:30 iteration: 53000 loss: 0.0008 lr: 0.02
2019-10-10 17:24:37 iteration: 53050 loss: 0.0009 lr: 0.02
2019-10-10 17:24:43 iteration: 53100 loss: 0.0010 lr: 0.02
2019-10-10 17:24:49 iteration: 53150 loss: 0.0010 lr: 0.02
2019-10-10 17:24:55 iteration: 53200 loss: 0.0008 lr: 0.02
2019-10-10 17:25:01 iteration: 53250 loss: 0.0011 lr: 0.02
2019-10-10 17:25:07 iteration: 53300 loss: 0.0008 lr: 0.02
2019-10-10 17:25:12 iteration: 53350 loss: 0.0009 lr: 0.02
2019-10-10 17:25:17 iteration: 53400 loss: 0.0009 lr: 0.02
2019-10-10 17:25:23 iteration: 53450 loss: 0.0009 lr: 0.02
2019-10-10 17:25:29 iteration: 53500 loss: 0.0009 lr: 0.02
2019-10-10 17:25:36 iteration: 53550 loss: 0.0008 lr: 0.02
2019-10-10 17:25:42 iteration: 53600 loss: 0.0009 lr: 0.02
2019-10-10 17:25:47 iteration: 53650 loss: 0.0010 lr: 0.02
2019-10-10 17:25:56 iteration: 53700 loss: 0.0011 lr: 0.02
2019-10-10 17:26:01 iteration: 53750 loss: 0.0009 lr: 0.02
2019-10-10 17:26:06 iteration: 53800 loss: 0.0008 lr: 0.02
2019-10-10 17:26:12 iteration: 53850 loss: 0.0009 lr: 0.02
2019-10-10 17:26:17 iteration: 53900 loss: 0.0008 lr: 0.02
2019-10-10 17:26:24 iteration: 53950 loss: 0.0009 lr: 0.02
2019-10-10 17:26:31 iteration: 54000 loss: 0.0009 lr: 0.02
2019-10-10 17:26:38 iteration: 54050 loss: 0.0010 lr: 0.02
2019-10-10 17:26:45 iteration: 54100 loss: 0.0010 lr: 0.02
2019-10-10 17:26:52 iteration: 54150 loss: 0.0009 lr: 0.02
2019-10-10 17:26:59 iteration: 54200 loss: 0.0010 lr: 0.02
2019-10-10 17:27:05 iteration: 54250 loss: 0.0008 lr: 0.02
2019-10-10 17:27:10 iteration: 54300 loss: 0.0008 lr: 0.02
2019-10-10 17:27:17 iteration: 54350 loss: 0.0009 lr: 0.02
2019-10-10 17:27:23 iteration: 54400 loss: 0.0009 lr: 0.02
2019-10-10 17:27:31 iteration: 54450 loss: 0.0008 lr: 0.02
2019-10-10 17:27:38 iteration: 54500 loss: 0.0009 lr: 0.02
2019-10-10 17:27:44 iteration: 54550 loss: 0.0010 lr: 0.02
2019-10-10 17:27:50 iteration: 54600 loss: 0.0010 lr: 0.02
2019-10-10 17:27:55 iteration: 54650 loss: 0.0011 lr: 0.02
2019-10-10 17:28:01 iteration: 54700 loss: 0.0009 lr: 0.02
2019-10-10 17:28:08 iteration: 54750 loss: 0.0010 lr: 0.02
2019-10-10 17:28:13 iteration: 54800 loss: 0.0009 lr: 0.02
2019-10-10 17:28:20 iteration: 54850 loss: 0.0010 lr: 0.02
2019-10-10 17:28:25 iteration: 54900 loss: 0.0010 lr: 0.02
2019-10-10 17:28:33 iteration: 54950 loss: 0.0009 lr: 0.02
2019-10-10 17:28:39 iteration: 55000 loss: 0.0010 lr: 0.02
2019-10-10 17:28:44 iteration: 55050 loss: 0.0009 lr: 0.02
2019-10-10 17:28:53 iteration: 55100 loss: 0.0011 lr: 0.02
2019-10-10 17:29:00 iteration: 55150 loss: 0.0008 lr: 0.02
2019-10-10 17:29:07 iteration: 55200 loss: 0.0009 lr: 0.02
2019-10-10 17:29:13 iteration: 55250 loss: 0.0010 lr: 0.02
2019-10-10 17:29:18 iteration: 55300 loss: 0.0010 lr: 0.02
2019-10-10 17:29:24 iteration: 55350 loss: 0.0010 lr: 0.02
2019-10-10 17:29:30 iteration: 55400 loss: 0.0009 lr: 0.02
2019-10-10 17:29:35 iteration: 55450 loss: 0.0009 lr: 0.02
2019-10-10 17:29:41 iteration: 55500 loss: 0.0009 lr: 0.02
2019-10-10 17:29:47 iteration: 55550 loss: 0.0009 lr: 0.02
2019-10-10 17:29:55 iteration: 55600 loss: 0.0009 lr: 0.02
2019-10-10 17:29:59 iteration: 55650 loss: 0.0009 lr: 0.02
2019-10-10 17:30:05 iteration: 55700 loss: 0.0009 lr: 0.02
2019-10-10 17:30:11 iteration: 55750 loss: 0.0008 lr: 0.02
2019-10-10 17:30:18 iteration: 55800 loss: 0.0009 lr: 0.02
2019-10-10 17:30:25 iteration: 55850 loss: 0.0010 lr: 0.02
2019-10-10 17:30:32 iteration: 55900 loss: 0.0010 lr: 0.02
2019-10-10 17:30:37 iteration: 55950 loss: 0.0010 lr: 0.02
2019-10-10 17:30:43 iteration: 56000 loss: 0.0009 lr: 0.02
2019-10-10 17:30:51 iteration: 56050 loss: 0.0009 lr: 0.02
2019-10-10 17:30:58 iteration: 56100 loss: 0.0009 lr: 0.02
2019-10-10 17:31:05 iteration: 56150 loss: 0.0009 lr: 0.02
2019-10-10 17:31:12 iteration: 56200 loss: 0.0010 lr: 0.02
2019-10-10 17:31:18 iteration: 56250 loss: 0.0009 lr: 0.02
2019-10-10 17:31:23 iteration: 56300 loss: 0.0009 lr: 0.02
2019-10-10 17:31:30 iteration: 56350 loss: 0.0009 lr: 0.02
2019-10-10 17:31:38 iteration: 56400 loss: 0.0009 lr: 0.02
2019-10-10 17:31:42 iteration: 56450 loss: 0.0008 lr: 0.02
2019-10-10 17:31:47 iteration: 56500 loss: 0.0011 lr: 0.02
2019-10-10 17:31:53 iteration: 56550 loss: 0.0009 lr: 0.02
2019-10-10 17:31:59 iteration: 56600 loss: 0.0010 lr: 0.02
2019-10-10 17:32:05 iteration: 56650 loss: 0.0009 lr: 0.02
2019-10-10 17:32:09 iteration: 56700 loss: 0.0009 lr: 0.02
2019-10-10 17:32:16 iteration: 56750 loss: 0.0010 lr: 0.02
2019-10-10 17:32:22 iteration: 56800 loss: 0.0009 lr: 0.02
2019-10-10 17:32:28 iteration: 56850 loss: 0.0010 lr: 0.02
2019-10-10 17:32:36 iteration: 56900 loss: 0.0010 lr: 0.02
2019-10-10 17:32:42 iteration: 56950 loss: 0.0008 lr: 0.02
2019-10-10 17:32:48 iteration: 57000 loss: 0.0009 lr: 0.02
2019-10-10 17:32:54 iteration: 57050 loss: 0.0009 lr: 0.02
2019-10-10 17:33:01 iteration: 57100 loss: 0.0009 lr: 0.02
2019-10-10 17:33:07 iteration: 57150 loss: 0.0008 lr: 0.02
2019-10-10 17:33:15 iteration: 57200 loss: 0.0009 lr: 0.02
2019-10-10 17:33:23 iteration: 57250 loss: 0.0010 lr: 0.02
2019-10-10 17:33:29 iteration: 57300 loss: 0.0009 lr: 0.02
2019-10-10 17:33:35 iteration: 57350 loss: 0.0010 lr: 0.02
2019-10-10 17:33:41 iteration: 57400 loss: 0.0010 lr: 0.02
2019-10-10 17:33:49 iteration: 57450 loss: 0.0009 lr: 0.02
2019-10-10 17:33:58 iteration: 57500 loss: 0.0008 lr: 0.02
2019-10-10 17:34:03 iteration: 57550 loss: 0.0009 lr: 0.02
2019-10-10 17:34:09 iteration: 57600 loss: 0.0009 lr: 0.02
2019-10-10 17:34:15 iteration: 57650 loss: 0.0008 lr: 0.02
2019-10-10 17:34:21 iteration: 57700 loss: 0.0010 lr: 0.02
2019-10-10 17:34:27 iteration: 57750 loss: 0.0009 lr: 0.02
2019-10-10 17:34:34 iteration: 57800 loss: 0.0008 lr: 0.02
2019-10-10 17:34:40 iteration: 57850 loss: 0.0009 lr: 0.02
2019-10-10 17:34:45 iteration: 57900 loss: 0.0010 lr: 0.02
2019-10-10 17:34:55 iteration: 57950 loss: 0.0010 lr: 0.02
2019-10-10 17:35:01 iteration: 58000 loss: 0.0008 lr: 0.02
2019-10-10 17:35:08 iteration: 58050 loss: 0.0010 lr: 0.02
2019-10-10 17:35:14 iteration: 58100 loss: 0.0010 lr: 0.02
2019-10-10 17:35:22 iteration: 58150 loss: 0.0011 lr: 0.02
2019-10-10 17:35:29 iteration: 58200 loss: 0.0010 lr: 0.02
2019-10-10 17:35:35 iteration: 58250 loss: 0.0009 lr: 0.02
2019-10-10 17:35:41 iteration: 58300 loss: 0.0010 lr: 0.02
2019-10-10 17:35:48 iteration: 58350 loss: 0.0009 lr: 0.02
2019-10-10 17:35:53 iteration: 58400 loss: 0.0009 lr: 0.02
2019-10-10 17:36:00 iteration: 58450 loss: 0.0007 lr: 0.02
2019-10-10 17:36:06 iteration: 58500 loss: 0.0010 lr: 0.02
2019-10-10 17:36:13 iteration: 58550 loss: 0.0009 lr: 0.02
2019-10-10 17:36:20 iteration: 58600 loss: 0.0008 lr: 0.02
2019-10-10 17:36:25 iteration: 58650 loss: 0.0010 lr: 0.02
2019-10-10 17:36:31 iteration: 58700 loss: 0.0009 lr: 0.02
2019-10-10 17:36:36 iteration: 58750 loss: 0.0009 lr: 0.02
2019-10-10 17:36:42 iteration: 58800 loss: 0.0008 lr: 0.02
2019-10-10 17:36:48 iteration: 58850 loss: 0.0010 lr: 0.02
2019-10-10 17:36:53 iteration: 58900 loss: 0.0010 lr: 0.02
2019-10-10 17:36:59 iteration: 58950 loss: 0.0009 lr: 0.02
2019-10-10 17:37:05 iteration: 59000 loss: 0.0009 lr: 0.02
2019-10-10 17:37:10 iteration: 59050 loss: 0.0008 lr: 0.02
2019-10-10 17:37:16 iteration: 59100 loss: 0.0009 lr: 0.02
2019-10-10 17:37:22 iteration: 59150 loss: 0.0010 lr: 0.02
2019-10-10 17:37:29 iteration: 59200 loss: 0.0008 lr: 0.02
2019-10-10 17:37:34 iteration: 59250 loss: 0.0008 lr: 0.02
2019-10-10 17:37:41 iteration: 59300 loss: 0.0010 lr: 0.02
2019-10-10 17:37:47 iteration: 59350 loss: 0.0010 lr: 0.02
2019-10-10 17:37:51 iteration: 59400 loss: 0.0008 lr: 0.02
2019-10-10 17:37:58 iteration: 59450 loss: 0.0010 lr: 0.02
2019-10-10 17:38:05 iteration: 59500 loss: 0.0009 lr: 0.02
2019-10-10 17:38:13 iteration: 59550 loss: 0.0010 lr: 0.02
2019-10-10 17:38:19 iteration: 59600 loss: 0.0008 lr: 0.02
2019-10-10 17:38:26 iteration: 59650 loss: 0.0009 lr: 0.02
2019-10-10 17:38:31 iteration: 59700 loss: 0.0009 lr: 0.02
2019-10-10 17:38:38 iteration: 59750 loss: 0.0009 lr: 0.02
2019-10-10 17:38:44 iteration: 59800 loss: 0.0008 lr: 0.02
2019-10-10 17:38:50 iteration: 59850 loss: 0.0010 lr: 0.02
2019-10-10 17:38:57 iteration: 59900 loss: 0.0009 lr: 0.02
2019-10-10 17:39:05 iteration: 59950 loss: 0.0009 lr: 0.02
2019-10-10 17:39:10 iteration: 60000 loss: 0.0009 lr: 0.02
2019-10-10 17:39:17 iteration: 60050 loss: 0.0010 lr: 0.02
2019-10-10 17:39:24 iteration: 60100 loss: 0.0010 lr: 0.02
2019-10-10 17:39:29 iteration: 60150 loss: 0.0010 lr: 0.02
2019-10-10 17:39:37 iteration: 60200 loss: 0.0009 lr: 0.02
2019-10-10 17:39:43 iteration: 60250 loss: 0.0009 lr: 0.02
2019-10-10 17:39:49 iteration: 60300 loss: 0.0009 lr: 0.02
2019-10-10 17:39:55 iteration: 60350 loss: 0.0009 lr: 0.02
2019-10-10 17:40:00 iteration: 60400 loss: 0.0009 lr: 0.02
2019-10-10 17:40:05 iteration: 60450 loss: 0.0010 lr: 0.02
2019-10-10 17:40:14 iteration: 60500 loss: 0.0010 lr: 0.02
2019-10-10 17:40:21 iteration: 60550 loss: 0.0008 lr: 0.02
2019-10-10 17:40:27 iteration: 60600 loss: 0.0009 lr: 0.02
2019-10-10 17:40:32 iteration: 60650 loss: 0.0008 lr: 0.02
2019-10-10 17:40:39 iteration: 60700 loss: 0.0009 lr: 0.02
2019-10-10 17:40:44 iteration: 60750 loss: 0.0009 lr: 0.02
2019-10-10 17:40:51 iteration: 60800 loss: 0.0008 lr: 0.02
2019-10-10 17:40:58 iteration: 60850 loss: 0.0008 lr: 0.02
2019-10-10 17:41:03 iteration: 60900 loss: 0.0009 lr: 0.02
2019-10-10 17:41:11 iteration: 60950 loss: 0.0010 lr: 0.02
2019-10-10 17:41:18 iteration: 61000 loss: 0.0009 lr: 0.02
2019-10-10 17:41:23 iteration: 61050 loss: 0.0008 lr: 0.02
2019-10-10 17:41:29 iteration: 61100 loss: 0.0010 lr: 0.02
2019-10-10 17:41:37 iteration: 61150 loss: 0.0008 lr: 0.02
2019-10-10 17:41:42 iteration: 61200 loss: 0.0008 lr: 0.02
2019-10-10 17:41:48 iteration: 61250 loss: 0.0010 lr: 0.02
2019-10-10 17:41:54 iteration: 61300 loss: 0.0009 lr: 0.02
2019-10-10 17:42:00 iteration: 61350 loss: 0.0010 lr: 0.02
2019-10-10 17:42:05 iteration: 61400 loss: 0.0009 lr: 0.02
2019-10-10 17:42:12 iteration: 61450 loss: 0.0009 lr: 0.02
2019-10-10 17:42:17 iteration: 61500 loss: 0.0009 lr: 0.02
2019-10-10 17:42:24 iteration: 61550 loss: 0.0008 lr: 0.02
2019-10-10 17:42:29 iteration: 61600 loss: 0.0008 lr: 0.02
2019-10-10 17:42:37 iteration: 61650 loss: 0.0008 lr: 0.02
2019-10-10 17:42:43 iteration: 61700 loss: 0.0008 lr: 0.02
2019-10-10 17:42:48 iteration: 61750 loss: 0.0008 lr: 0.02
2019-10-10 17:42:53 iteration: 61800 loss: 0.0009 lr: 0.02
2019-10-10 17:43:00 iteration: 61850 loss: 0.0008 lr: 0.02
2019-10-10 17:43:05 iteration: 61900 loss: 0.0010 lr: 0.02
2019-10-10 17:43:11 iteration: 61950 loss: 0.0009 lr: 0.02
2019-10-10 17:43:17 iteration: 62000 loss: 0.0008 lr: 0.02
2019-10-10 17:43:23 iteration: 62050 loss: 0.0010 lr: 0.02
2019-10-10 17:43:27 iteration: 62100 loss: 0.0010 lr: 0.02
2019-10-10 17:43:35 iteration: 62150 loss: 0.0010 lr: 0.02
2019-10-10 17:43:40 iteration: 62200 loss: 0.0008 lr: 0.02
2019-10-10 17:43:46 iteration: 62250 loss: 0.0009 lr: 0.02
2019-10-10 17:43:52 iteration: 62300 loss: 0.0009 lr: 0.02
2019-10-10 17:43:59 iteration: 62350 loss: 0.0009 lr: 0.02
2019-10-10 17:44:06 iteration: 62400 loss: 0.0008 lr: 0.02
2019-10-10 17:44:13 iteration: 62450 loss: 0.0008 lr: 0.02
2019-10-10 17:44:20 iteration: 62500 loss: 0.0009 lr: 0.02
2019-10-10 17:44:25 iteration: 62550 loss: 0.0009 lr: 0.02
2019-10-10 17:44:31 iteration: 62600 loss: 0.0009 lr: 0.02
2019-10-10 17:44:38 iteration: 62650 loss: 0.0009 lr: 0.02
2019-10-10 17:44:44 iteration: 62700 loss: 0.0008 lr: 0.02
2019-10-10 17:44:49 iteration: 62750 loss: 0.0009 lr: 0.02
2019-10-10 17:44:55 iteration: 62800 loss: 0.0009 lr: 0.02
2019-10-10 17:45:01 iteration: 62850 loss: 0.0008 lr: 0.02
2019-10-10 17:45:08 iteration: 62900 loss: 0.0009 lr: 0.02
2019-10-10 17:45:16 iteration: 62950 loss: 0.0009 lr: 0.02
2019-10-10 17:45:22 iteration: 63000 loss: 0.0009 lr: 0.02
2019-10-10 17:45:29 iteration: 63050 loss: 0.0009 lr: 0.02
2019-10-10 17:45:34 iteration: 63100 loss: 0.0009 lr: 0.02
2019-10-10 17:45:40 iteration: 63150 loss: 0.0009 lr: 0.02
2019-10-10 17:45:47 iteration: 63200 loss: 0.0009 lr: 0.02
2019-10-10 17:45:52 iteration: 63250 loss: 0.0008 lr: 0.02
2019-10-10 17:45:57 iteration: 63300 loss: 0.0008 lr: 0.02
2019-10-10 17:46:03 iteration: 63350 loss: 0.0008 lr: 0.02
2019-10-10 17:46:11 iteration: 63400 loss: 0.0009 lr: 0.02
2019-10-10 17:46:15 iteration: 63450 loss: 0.0009 lr: 0.02
2019-10-10 17:46:20 iteration: 63500 loss: 0.0009 lr: 0.02
2019-10-10 17:46:26 iteration: 63550 loss: 0.0009 lr: 0.02
2019-10-10 17:46:32 iteration: 63600 loss: 0.0008 lr: 0.02
2019-10-10 17:46:37 iteration: 63650 loss: 0.0009 lr: 0.02
2019-10-10 17:46:42 iteration: 63700 loss: 0.0009 lr: 0.02
2019-10-10 17:46:48 iteration: 63750 loss: 0.0010 lr: 0.02
2019-10-10 17:46:55 iteration: 63800 loss: 0.0009 lr: 0.02
2019-10-10 17:47:01 iteration: 63850 loss: 0.0008 lr: 0.02
2019-10-10 17:47:06 iteration: 63900 loss: 0.0009 lr: 0.02
2019-10-10 17:47:14 iteration: 63950 loss: 0.0008 lr: 0.02
2019-10-10 17:47:21 iteration: 64000 loss: 0.0010 lr: 0.02
2019-10-10 17:47:27 iteration: 64050 loss: 0.0009 lr: 0.02
2019-10-10 17:47:34 iteration: 64100 loss: 0.0009 lr: 0.02
2019-10-10 17:47:39 iteration: 64150 loss: 0.0010 lr: 0.02
2019-10-10 17:47:44 iteration: 64200 loss: 0.0009 lr: 0.02
2019-10-10 17:47:49 iteration: 64250 loss: 0.0008 lr: 0.02
2019-10-10 17:47:56 iteration: 64300 loss: 0.0010 lr: 0.02
2019-10-10 17:48:02 iteration: 64350 loss: 0.0009 lr: 0.02
2019-10-10 17:48:08 iteration: 64400 loss: 0.0008 lr: 0.02
2019-10-10 17:48:15 iteration: 64450 loss: 0.0010 lr: 0.02
2019-10-10 17:48:22 iteration: 64500 loss: 0.0009 lr: 0.02
2019-10-10 17:48:27 iteration: 64550 loss: 0.0009 lr: 0.02
2019-10-10 17:48:35 iteration: 64600 loss: 0.0008 lr: 0.02
2019-10-10 17:48:40 iteration: 64650 loss: 0.0009 lr: 0.02
2019-10-10 17:48:47 iteration: 64700 loss: 0.0008 lr: 0.02
2019-10-10 17:48:55 iteration: 64750 loss: 0.0009 lr: 0.02
2019-10-10 17:49:02 iteration: 64800 loss: 0.0010 lr: 0.02
2019-10-10 17:49:09 iteration: 64850 loss: 0.0008 lr: 0.02
2019-10-10 17:49:14 iteration: 64900 loss: 0.0008 lr: 0.02
2019-10-10 17:49:19 iteration: 64950 loss: 0.0009 lr: 0.02
2019-10-10 17:49:25 iteration: 65000 loss: 0.0009 lr: 0.02
2019-10-10 17:49:31 iteration: 65050 loss: 0.0009 lr: 0.02
2019-10-10 17:49:38 iteration: 65100 loss: 0.0008 lr: 0.02
2019-10-10 17:49:44 iteration: 65150 loss: 0.0008 lr: 0.02
2019-10-10 17:49:50 iteration: 65200 loss: 0.0008 lr: 0.02
2019-10-10 17:49:57 iteration: 65250 loss: 0.0008 lr: 0.02
2019-10-10 17:50:03 iteration: 65300 loss: 0.0009 lr: 0.02
2019-10-10 17:50:09 iteration: 65350 loss: 0.0009 lr: 0.02
2019-10-10 17:50:14 iteration: 65400 loss: 0.0008 lr: 0.02
2019-10-10 17:50:21 iteration: 65450 loss: 0.0008 lr: 0.02
2019-10-10 17:50:26 iteration: 65500 loss: 0.0010 lr: 0.02
2019-10-10 17:50:30 iteration: 65550 loss: 0.0009 lr: 0.02
2019-10-10 17:50:38 iteration: 65600 loss: 0.0010 lr: 0.02
2019-10-10 17:50:44 iteration: 65650 loss: 0.0010 lr: 0.02
2019-10-10 17:50:51 iteration: 65700 loss: 0.0009 lr: 0.02
2019-10-10 17:50:57 iteration: 65750 loss: 0.0009 lr: 0.02
2019-10-10 17:51:03 iteration: 65800 loss: 0.0008 lr: 0.02
2019-10-10 17:51:09 iteration: 65850 loss: 0.0009 lr: 0.02
2019-10-10 17:51:14 iteration: 65900 loss: 0.0009 lr: 0.02
2019-10-10 17:51:21 iteration: 65950 loss: 0.0010 lr: 0.02
2019-10-10 17:51:28 iteration: 66000 loss: 0.0009 lr: 0.02
2019-10-10 17:51:33 iteration: 66050 loss: 0.0009 lr: 0.02
2019-10-10 17:51:38 iteration: 66100 loss: 0.0009 lr: 0.02
2019-10-10 17:51:43 iteration: 66150 loss: 0.0008 lr: 0.02
2019-10-10 17:51:50 iteration: 66200 loss: 0.0010 lr: 0.02
2019-10-10 17:51:57 iteration: 66250 loss: 0.0009 lr: 0.02
2019-10-10 17:52:02 iteration: 66300 loss: 0.0010 lr: 0.02
2019-10-10 17:52:08 iteration: 66350 loss: 0.0008 lr: 0.02
2019-10-10 17:52:14 iteration: 66400 loss: 0.0008 lr: 0.02
2019-10-10 17:52:20 iteration: 66450 loss: 0.0009 lr: 0.02
2019-10-10 17:52:26 iteration: 66500 loss: 0.0010 lr: 0.02
2019-10-10 17:52:34 iteration: 66550 loss: 0.0009 lr: 0.02
2019-10-10 17:52:39 iteration: 66600 loss: 0.0008 lr: 0.02
2019-10-10 17:52:45 iteration: 66650 loss: 0.0008 lr: 0.02
2019-10-10 17:52:51 iteration: 66700 loss: 0.0008 lr: 0.02
2019-10-10 17:52:57 iteration: 66750 loss: 0.0008 lr: 0.02
2019-10-10 17:53:02 iteration: 66800 loss: 0.0008 lr: 0.02
2019-10-10 17:53:07 iteration: 66850 loss: 0.0009 lr: 0.02
2019-10-10 17:53:13 iteration: 66900 loss: 0.0009 lr: 0.02
2019-10-10 17:53:20 iteration: 66950 loss: 0.0009 lr: 0.02
2019-10-10 17:53:28 iteration: 67000 loss: 0.0008 lr: 0.02
2019-10-10 17:53:34 iteration: 67050 loss: 0.0009 lr: 0.02
2019-10-10 17:53:39 iteration: 67100 loss: 0.0009 lr: 0.02
2019-10-10 17:53:45 iteration: 67150 loss: 0.0009 lr: 0.02
2019-10-10 17:53:52 iteration: 67200 loss: 0.0009 lr: 0.02
2019-10-10 17:53:57 iteration: 67250 loss: 0.0008 lr: 0.02
2019-10-10 17:54:03 iteration: 67300 loss: 0.0010 lr: 0.02
2019-10-10 17:54:09 iteration: 67350 loss: 0.0010 lr: 0.02
2019-10-10 17:54:15 iteration: 67400 loss: 0.0009 lr: 0.02
2019-10-10 17:54:20 iteration: 67450 loss: 0.0010 lr: 0.02
2019-10-10 17:54:27 iteration: 67500 loss: 0.0009 lr: 0.02
2019-10-10 17:54:32 iteration: 67550 loss: 0.0009 lr: 0.02
2019-10-10 17:54:38 iteration: 67600 loss: 0.0008 lr: 0.02
2019-10-10 17:54:45 iteration: 67650 loss: 0.0009 lr: 0.02
2019-10-10 17:54:50 iteration: 67700 loss: 0.0009 lr: 0.02
2019-10-10 17:54:55 iteration: 67750 loss: 0.0008 lr: 0.02
2019-10-10 17:55:01 iteration: 67800 loss: 0.0009 lr: 0.02
2019-10-10 17:55:08 iteration: 67850 loss: 0.0008 lr: 0.02
2019-10-10 17:55:17 iteration: 67900 loss: 0.0008 lr: 0.02
2019-10-10 17:55:21 iteration: 67950 loss: 0.0009 lr: 0.02
2019-10-10 17:55:27 iteration: 68000 loss: 0.0008 lr: 0.02
2019-10-10 17:55:32 iteration: 68050 loss: 0.0008 lr: 0.02
2019-10-10 17:55:37 iteration: 68100 loss: 0.0009 lr: 0.02
2019-10-10 17:55:44 iteration: 68150 loss: 0.0008 lr: 0.02
2019-10-10 17:55:51 iteration: 68200 loss: 0.0008 lr: 0.02
2019-10-10 17:55:56 iteration: 68250 loss: 0.0010 lr: 0.02
2019-10-10 17:56:01 iteration: 68300 loss: 0.0010 lr: 0.02
2019-10-10 17:56:07 iteration: 68350 loss: 0.0009 lr: 0.02
2019-10-10 17:56:13 iteration: 68400 loss: 0.0009 lr: 0.02
2019-10-10 17:56:19 iteration: 68450 loss: 0.0009 lr: 0.02
2019-10-10 17:56:25 iteration: 68500 loss: 0.0009 lr: 0.02
2019-10-10 17:56:31 iteration: 68550 loss: 0.0009 lr: 0.02
2019-10-10 17:56:37 iteration: 68600 loss: 0.0009 lr: 0.02
2019-10-10 17:56:43 iteration: 68650 loss: 0.0009 lr: 0.02
2019-10-10 17:56:49 iteration: 68700 loss: 0.0010 lr: 0.02
2019-10-10 17:56:56 iteration: 68750 loss: 0.0009 lr: 0.02
2019-10-10 17:57:02 iteration: 68800 loss: 0.0009 lr: 0.02
2019-10-10 17:57:08 iteration: 68850 loss: 0.0009 lr: 0.02
2019-10-10 17:57:14 iteration: 68900 loss: 0.0008 lr: 0.02
2019-10-10 17:57:21 iteration: 68950 loss: 0.0008 lr: 0.02
2019-10-10 17:57:27 iteration: 69000 loss: 0.0009 lr: 0.02
2019-10-10 17:57:32 iteration: 69050 loss: 0.0010 lr: 0.02
2019-10-10 17:57:38 iteration: 69100 loss: 0.0010 lr: 0.02
2019-10-10 17:57:43 iteration: 69150 loss: 0.0010 lr: 0.02
2019-10-10 17:57:48 iteration: 69200 loss: 0.0009 lr: 0.02
2019-10-10 17:57:53 iteration: 69250 loss: 0.0009 lr: 0.02
2019-10-10 17:58:00 iteration: 69300 loss: 0.0009 lr: 0.02
2019-10-10 17:58:06 iteration: 69350 loss: 0.0010 lr: 0.02
2019-10-10 17:58:11 iteration: 69400 loss: 0.0009 lr: 0.02
2019-10-10 17:58:16 iteration: 69450 loss: 0.0010 lr: 0.02
2019-10-10 17:58:22 iteration: 69500 loss: 0.0009 lr: 0.02
2019-10-10 17:58:27 iteration: 69550 loss: 0.0009 lr: 0.02
2019-10-10 17:58:32 iteration: 69600 loss: 0.0008 lr: 0.02
2019-10-10 17:58:37 iteration: 69650 loss: 0.0009 lr: 0.02
2019-10-10 17:58:43 iteration: 69700 loss: 0.0010 lr: 0.02
2019-10-10 17:58:50 iteration: 69750 loss: 0.0009 lr: 0.02
2019-10-10 17:58:57 iteration: 69800 loss: 0.0010 lr: 0.02
2019-10-10 17:59:02 iteration: 69850 loss: 0.0008 lr: 0.02
2019-10-10 17:59:09 iteration: 69900 loss: 0.0008 lr: 0.02
2019-10-10 17:59:15 iteration: 69950 loss: 0.0009 lr: 0.02
2019-10-10 17:59:20 iteration: 70000 loss: 0.0008 lr: 0.02
2019-10-10 17:59:28 iteration: 70050 loss: 0.0008 lr: 0.02
2019-10-10 17:59:34 iteration: 70100 loss: 0.0009 lr: 0.02
2019-10-10 17:59:40 iteration: 70150 loss: 0.0008 lr: 0.02
2019-10-10 17:59:45 iteration: 70200 loss: 0.0009 lr: 0.02
2019-10-10 17:59:50 iteration: 70250 loss: 0.0008 lr: 0.02
2019-10-10 17:59:56 iteration: 70300 loss: 0.0009 lr: 0.02
2019-10-10 18:00:01 iteration: 70350 loss: 0.0008 lr: 0.02
2019-10-10 18:00:05 iteration: 70400 loss: 0.0008 lr: 0.02
2019-10-10 18:00:11 iteration: 70450 loss: 0.0009 lr: 0.02
2019-10-10 18:00:16 iteration: 70500 loss: 0.0010 lr: 0.02
2019-10-10 18:00:21 iteration: 70550 loss: 0.0010 lr: 0.02
2019-10-10 18:00:27 iteration: 70600 loss: 0.0008 lr: 0.02
2019-10-10 18:00:35 iteration: 70650 loss: 0.0009 lr: 0.02
2019-10-10 18:00:39 iteration: 70700 loss: 0.0009 lr: 0.02
2019-10-10 18:00:44 iteration: 70750 loss: 0.0009 lr: 0.02
2019-10-10 18:00:50 iteration: 70800 loss: 0.0008 lr: 0.02
2019-10-10 18:00:56 iteration: 70850 loss: 0.0009 lr: 0.02
2019-10-10 18:01:01 iteration: 70900 loss: 0.0008 lr: 0.02
2019-10-10 18:01:06 iteration: 70950 loss: 0.0008 lr: 0.02
2019-10-10 18:01:13 iteration: 71000 loss: 0.0010 lr: 0.02
2019-10-10 18:01:20 iteration: 71050 loss: 0.0010 lr: 0.02
2019-10-10 18:01:25 iteration: 71100 loss: 0.0009 lr: 0.02
2019-10-10 18:01:32 iteration: 71150 loss: 0.0009 lr: 0.02
2019-10-10 18:01:38 iteration: 71200 loss: 0.0009 lr: 0.02
2019-10-10 18:01:43 iteration: 71250 loss: 0.0008 lr: 0.02
2019-10-10 18:01:49 iteration: 71300 loss: 0.0009 lr: 0.02
2019-10-10 18:01:55 iteration: 71350 loss: 0.0009 lr: 0.02
2019-10-10 18:02:02 iteration: 71400 loss: 0.0010 lr: 0.02
2019-10-10 18:02:08 iteration: 71450 loss: 0.0009 lr: 0.02
2019-10-10 18:02:13 iteration: 71500 loss: 0.0010 lr: 0.02
2019-10-10 18:02:19 iteration: 71550 loss: 0.0009 lr: 0.02
2019-10-10 18:02:24 iteration: 71600 loss: 0.0008 lr: 0.02
2019-10-10 18:02:28 iteration: 71650 loss: 0.0009 lr: 0.02
2019-10-10 18:02:34 iteration: 71700 loss: 0.0008 lr: 0.02
2019-10-10 18:02:40 iteration: 71750 loss: 0.0008 lr: 0.02
2019-10-10 18:02:46 iteration: 71800 loss: 0.0009 lr: 0.02
2019-10-10 18:02:53 iteration: 71850 loss: 0.0010 lr: 0.02
2019-10-10 18:02:59 iteration: 71900 loss: 0.0008 lr: 0.02
2019-10-10 18:03:04 iteration: 71950 loss: 0.0009 lr: 0.02
2019-10-10 18:03:10 iteration: 72000 loss: 0.0008 lr: 0.02
2019-10-10 18:03:16 iteration: 72050 loss: 0.0010 lr: 0.02
2019-10-10 18:03:23 iteration: 72100 loss: 0.0009 lr: 0.02
2019-10-10 18:03:28 iteration: 72150 loss: 0.0007 lr: 0.02
2019-10-10 18:03:33 iteration: 72200 loss: 0.0009 lr: 0.02
2019-10-10 18:03:39 iteration: 72250 loss: 0.0009 lr: 0.02
2019-10-10 18:03:45 iteration: 72300 loss: 0.0008 lr: 0.02
2019-10-10 18:03:50 iteration: 72350 loss: 0.0010 lr: 0.02
2019-10-10 18:03:55 iteration: 72400 loss: 0.0009 lr: 0.02
2019-10-10 18:04:01 iteration: 72450 loss: 0.0009 lr: 0.02
2019-10-10 18:04:07 iteration: 72500 loss: 0.0010 lr: 0.02
2019-10-10 18:04:12 iteration: 72550 loss: 0.0008 lr: 0.02
2019-10-10 18:04:17 iteration: 72600 loss: 0.0008 lr: 0.02
2019-10-10 18:04:23 iteration: 72650 loss: 0.0008 lr: 0.02
2019-10-10 18:04:27 iteration: 72700 loss: 0.0010 lr: 0.02
2019-10-10 18:04:33 iteration: 72750 loss: 0.0010 lr: 0.02
2019-10-10 18:04:38 iteration: 72800 loss: 0.0009 lr: 0.02
2019-10-10 18:04:43 iteration: 72850 loss: 0.0008 lr: 0.02
2019-10-10 18:04:50 iteration: 72900 loss: 0.0009 lr: 0.02
2019-10-10 18:04:55 iteration: 72950 loss: 0.0008 lr: 0.02
2019-10-10 18:05:00 iteration: 73000 loss: 0.0009 lr: 0.02
2019-10-10 18:05:06 iteration: 73050 loss: 0.0008 lr: 0.02
2019-10-10 18:05:11 iteration: 73100 loss: 0.0009 lr: 0.02
2019-10-10 18:05:17 iteration: 73150 loss: 0.0008 lr: 0.02
2019-10-10 18:05:23 iteration: 73200 loss: 0.0009 lr: 0.02
2019-10-10 18:05:29 iteration: 73250 loss: 0.0010 lr: 0.02
2019-10-10 18:05:35 iteration: 73300 loss: 0.0009 lr: 0.02
2019-10-10 18:05:41 iteration: 73350 loss: 0.0008 lr: 0.02
2019-10-10 18:05:47 iteration: 73400 loss: 0.0010 lr: 0.02
2019-10-10 18:05:53 iteration: 73450 loss: 0.0011 lr: 0.02
2019-10-10 18:05:59 iteration: 73500 loss: 0.0009 lr: 0.02
2019-10-10 18:06:03 iteration: 73550 loss: 0.0008 lr: 0.02
2019-10-10 18:06:07 iteration: 73600 loss: 0.0008 lr: 0.02
2019-10-10 18:06:13 iteration: 73650 loss: 0.0009 lr: 0.02
2019-10-10 18:06:19 iteration: 73700 loss: 0.0010 lr: 0.02
2019-10-10 18:06:24 iteration: 73750 loss: 0.0008 lr: 0.02
2019-10-10 18:06:29 iteration: 73800 loss: 0.0009 lr: 0.02
2019-10-10 18:06:36 iteration: 73850 loss: 0.0008 lr: 0.02
2019-10-10 18:06:41 iteration: 73900 loss: 0.0009 lr: 0.02
2019-10-10 18:06:45 iteration: 73950 loss: 0.0008 lr: 0.02
2019-10-10 18:06:50 iteration: 74000 loss: 0.0008 lr: 0.02
2019-10-10 18:06:56 iteration: 74050 loss: 0.0008 lr: 0.02
2019-10-10 18:07:02 iteration: 74100 loss: 0.0008 lr: 0.02
2019-10-10 18:07:09 iteration: 74150 loss: 0.0008 lr: 0.02
2019-10-10 18:07:14 iteration: 74200 loss: 0.0008 lr: 0.02
2019-10-10 18:07:21 iteration: 74250 loss: 0.0009 lr: 0.02
2019-10-10 18:07:26 iteration: 74300 loss: 0.0008 lr: 0.02
2019-10-10 18:07:32 iteration: 74350 loss: 0.0009 lr: 0.02
2019-10-10 18:07:38 iteration: 74400 loss: 0.0009 lr: 0.02
2019-10-10 18:07:42 iteration: 74450 loss: 0.0008 lr: 0.02
2019-10-10 18:07:48 iteration: 74500 loss: 0.0008 lr: 0.02
2019-10-10 18:07:53 iteration: 74550 loss: 0.0008 lr: 0.02
2019-10-10 18:07:58 iteration: 74600 loss: 0.0007 lr: 0.02
2019-10-10 18:08:04 iteration: 74650 loss: 0.0007 lr: 0.02
2019-10-10 18:08:09 iteration: 74700 loss: 0.0008 lr: 0.02
2019-10-10 18:08:14 iteration: 74750 loss: 0.0008 lr: 0.02
2019-10-10 18:08:20 iteration: 74800 loss: 0.0009 lr: 0.02
2019-10-10 18:08:27 iteration: 74850 loss: 0.0009 lr: 0.02
2019-10-10 18:08:35 iteration: 74900 loss: 0.0009 lr: 0.02
2019-10-10 18:08:40 iteration: 74950 loss: 0.0008 lr: 0.02
2019-10-10 18:08:45 iteration: 75000 loss: 0.0010 lr: 0.02
2019-10-10 18:08:50 iteration: 75050 loss: 0.0008 lr: 0.02
2019-10-10 18:08:55 iteration: 75100 loss: 0.0009 lr: 0.02
2019-10-10 18:09:02 iteration: 75150 loss: 0.0008 lr: 0.02
2019-10-10 18:09:07 iteration: 75200 loss: 0.0009 lr: 0.02
2019-10-10 18:09:13 iteration: 75250 loss: 0.0010 lr: 0.02
2019-10-10 18:09:19 iteration: 75300 loss: 0.0009 lr: 0.02
2019-10-10 18:09:24 iteration: 75350 loss: 0.0009 lr: 0.02
2019-10-10 18:09:31 iteration: 75400 loss: 0.0009 lr: 0.02
2019-10-10 18:09:36 iteration: 75450 loss: 0.0009 lr: 0.02
2019-10-10 18:09:41 iteration: 75500 loss: 0.0009 lr: 0.02
2019-10-10 18:09:46 iteration: 75550 loss: 0.0009 lr: 0.02
2019-10-10 18:09:52 iteration: 75600 loss: 0.0009 lr: 0.02
2019-10-10 18:09:57 iteration: 75650 loss: 0.0008 lr: 0.02
2019-10-10 18:10:04 iteration: 75700 loss: 0.0009 lr: 0.02
2019-10-10 18:10:09 iteration: 75750 loss: 0.0009 lr: 0.02
2019-10-10 18:10:15 iteration: 75800 loss: 0.0008 lr: 0.02
2019-10-10 18:10:20 iteration: 75850 loss: 0.0008 lr: 0.02
2019-10-10 18:10:25 iteration: 75900 loss: 0.0008 lr: 0.02
2019-10-10 18:10:30 iteration: 75950 loss: 0.0009 lr: 0.02
2019-10-10 18:10:37 iteration: 76000 loss: 0.0008 lr: 0.02
2019-10-10 18:10:41 iteration: 76050 loss: 0.0009 lr: 0.02
2019-10-10 18:10:48 iteration: 76100 loss: 0.0008 lr: 0.02
2019-10-10 18:10:53 iteration: 76150 loss: 0.0008 lr: 0.02
2019-10-10 18:10:58 iteration: 76200 loss: 0.0009 lr: 0.02
2019-10-10 18:11:04 iteration: 76250 loss: 0.0008 lr: 0.02
2019-10-10 18:11:09 iteration: 76300 loss: 0.0009 lr: 0.02
2019-10-10 18:11:15 iteration: 76350 loss: 0.0007 lr: 0.02
2019-10-10 18:11:20 iteration: 76400 loss: 0.0009 lr: 0.02
2019-10-10 18:11:26 iteration: 76450 loss: 0.0009 lr: 0.02
2019-10-10 18:11:31 iteration: 76500 loss: 0.0009 lr: 0.02
2019-10-10 18:11:35 iteration: 76550 loss: 0.0008 lr: 0.02
2019-10-10 18:11:40 iteration: 76600 loss: 0.0009 lr: 0.02
2019-10-10 18:11:46 iteration: 76650 loss: 0.0007 lr: 0.02
2019-10-10 18:11:52 iteration: 76700 loss: 0.0008 lr: 0.02
2019-10-10 18:11:58 iteration: 76750 loss: 0.0009 lr: 0.02
2019-10-10 18:12:03 iteration: 76800 loss: 0.0009 lr: 0.02
2019-10-10 18:12:08 iteration: 76850 loss: 0.0008 lr: 0.02
2019-10-10 18:12:14 iteration: 76900 loss: 0.0009 lr: 0.02
2019-10-10 18:12:19 iteration: 76950 loss: 0.0008 lr: 0.02
2019-10-10 18:12:25 iteration: 77000 loss: 0.0008 lr: 0.02
2019-10-10 18:12:30 iteration: 77050 loss: 0.0009 lr: 0.02
2019-10-10 18:12:35 iteration: 77100 loss: 0.0010 lr: 0.02
2019-10-10 18:12:41 iteration: 77150 loss: 0.0008 lr: 0.02
2019-10-10 18:12:45 iteration: 77200 loss: 0.0009 lr: 0.02
2019-10-10 18:12:50 iteration: 77250 loss: 0.0009 lr: 0.02
2019-10-10 18:12:56 iteration: 77300 loss: 0.0008 lr: 0.02
2019-10-10 18:13:01 iteration: 77350 loss: 0.0009 lr: 0.02
2019-10-10 18:13:06 iteration: 77400 loss: 0.0008 lr: 0.02
2019-10-10 18:13:11 iteration: 77450 loss: 0.0007 lr: 0.02
2019-10-10 18:13:17 iteration: 77500 loss: 0.0009 lr: 0.02
2019-10-10 18:13:23 iteration: 77550 loss: 0.0009 lr: 0.02
2019-10-10 18:13:28 iteration: 77600 loss: 0.0010 lr: 0.02
2019-10-10 18:13:33 iteration: 77650 loss: 0.0009 lr: 0.02
2019-10-10 18:13:38 iteration: 77700 loss: 0.0009 lr: 0.02
2019-10-10 18:13:43 iteration: 77750 loss: 0.0009 lr: 0.02
2019-10-10 18:13:48 iteration: 77800 loss: 0.0009 lr: 0.02
2019-10-10 18:13:55 iteration: 77850 loss: 0.0008 lr: 0.02
2019-10-10 18:14:03 iteration: 77900 loss: 0.0009 lr: 0.02
2019-10-10 18:14:08 iteration: 77950 loss: 0.0009 lr: 0.02
2019-10-10 18:14:14 iteration: 78000 loss: 0.0008 lr: 0.02
2019-10-10 18:14:19 iteration: 78050 loss: 0.0010 lr: 0.02
2019-10-10 18:14:26 iteration: 78100 loss: 0.0009 lr: 0.02
2019-10-10 18:14:31 iteration: 78150 loss: 0.0007 lr: 0.02
2019-10-10 18:14:35 iteration: 78200 loss: 0.0008 lr: 0.02
2019-10-10 18:14:41 iteration: 78250 loss: 0.0009 lr: 0.02
2019-10-10 18:14:47 iteration: 78300 loss: 0.0009 lr: 0.02
2019-10-10 18:14:51 iteration: 78350 loss: 0.0008 lr: 0.02
2019-10-10 18:14:57 iteration: 78400 loss: 0.0008 lr: 0.02
2019-10-10 18:15:03 iteration: 78450 loss: 0.0010 lr: 0.02
2019-10-10 18:15:10 iteration: 78500 loss: 0.0008 lr: 0.02
2019-10-10 18:15:16 iteration: 78550 loss: 0.0009 lr: 0.02
2019-10-10 18:15:20 iteration: 78600 loss: 0.0008 lr: 0.02
2019-10-10 18:15:26 iteration: 78650 loss: 0.0009 lr: 0.02
2019-10-10 18:15:32 iteration: 78700 loss: 0.0009 lr: 0.02
2019-10-10 18:15:37 iteration: 78750 loss: 0.0009 lr: 0.02
2019-10-10 18:15:44 iteration: 78800 loss: 0.0009 lr: 0.02
2019-10-10 18:15:48 iteration: 78850 loss: 0.0008 lr: 0.02
2019-10-10 18:15:54 iteration: 78900 loss: 0.0010 lr: 0.02
2019-10-10 18:16:00 iteration: 78950 loss: 0.0009 lr: 0.02
2019-10-10 18:16:05 iteration: 79000 loss: 0.0008 lr: 0.02
2019-10-10 18:16:11 iteration: 79050 loss: 0.0009 lr: 0.02
2019-10-10 18:16:17 iteration: 79100 loss: 0.0009 lr: 0.02
2019-10-10 18:16:24 iteration: 79150 loss: 0.0010 lr: 0.02
2019-10-10 18:16:30 iteration: 79200 loss: 0.0009 lr: 0.02
2019-10-10 18:16:36 iteration: 79250 loss: 0.0009 lr: 0.02
2019-10-10 18:16:40 iteration: 79300 loss: 0.0009 lr: 0.02
2019-10-10 18:16:45 iteration: 79350 loss: 0.0009 lr: 0.02
2019-10-10 18:16:50 iteration: 79400 loss: 0.0008 lr: 0.02
2019-10-10 18:16:56 iteration: 79450 loss: 0.0008 lr: 0.02
2019-10-10 18:17:01 iteration: 79500 loss: 0.0009 lr: 0.02
2019-10-10 18:17:07 iteration: 79550 loss: 0.0009 lr: 0.02
2019-10-10 18:17:13 iteration: 79600 loss: 0.0008 lr: 0.02
2019-10-10 18:17:18 iteration: 79650 loss: 0.0008 lr: 0.02
2019-10-10 18:17:24 iteration: 79700 loss: 0.0009 lr: 0.02
2019-10-10 18:17:29 iteration: 79750 loss: 0.0008 lr: 0.02
2019-10-10 18:17:35 iteration: 79800 loss: 0.0009 lr: 0.02
2019-10-10 18:17:41 iteration: 79850 loss: 0.0009 lr: 0.02
2019-10-10 18:17:45 iteration: 79900 loss: 0.0008 lr: 0.02
2019-10-10 18:17:51 iteration: 79950 loss: 0.0008 lr: 0.02
2019-10-10 18:17:57 iteration: 80000 loss: 0.0009 lr: 0.02
2019-10-10 18:18:02 iteration: 80050 loss: 0.0008 lr: 0.02
2019-10-10 18:18:08 iteration: 80100 loss: 0.0009 lr: 0.02
2019-10-10 18:18:12 iteration: 80150 loss: 0.0009 lr: 0.02
2019-10-10 18:18:18 iteration: 80200 loss: 0.0010 lr: 0.02
2019-10-10 18:18:23 iteration: 80250 loss: 0.0008 lr: 0.02
2019-10-10 18:18:30 iteration: 80300 loss: 0.0008 lr: 0.02
2019-10-10 18:18:34 iteration: 80350 loss: 0.0010 lr: 0.02
2019-10-10 18:18:42 iteration: 80400 loss: 0.0008 lr: 0.02
2019-10-10 18:18:48 iteration: 80450 loss: 0.0010 lr: 0.02
2019-10-10 18:18:53 iteration: 80500 loss: 0.0009 lr: 0.02
2019-10-10 18:18:58 iteration: 80550 loss: 0.0008 lr: 0.02
2019-10-10 18:19:03 iteration: 80600 loss: 0.0010 lr: 0.02
2019-10-10 18:19:09 iteration: 80650 loss: 0.0008 lr: 0.02
2019-10-10 18:19:14 iteration: 80700 loss: 0.0008 lr: 0.02
2019-10-10 18:19:21 iteration: 80750 loss: 0.0009 lr: 0.02
2019-10-10 18:19:25 iteration: 80800 loss: 0.0009 lr: 0.02
2019-10-10 18:19:31 iteration: 80850 loss: 0.0010 lr: 0.02
2019-10-10 18:19:37 iteration: 80900 loss: 0.0009 lr: 0.02
2019-10-10 18:19:43 iteration: 80950 loss: 0.0009 lr: 0.02
2019-10-10 18:19:49 iteration: 81000 loss: 0.0009 lr: 0.02
2019-10-10 18:19:54 iteration: 81050 loss: 0.0007 lr: 0.02
2019-10-10 18:20:00 iteration: 81100 loss: 0.0009 lr: 0.02
2019-10-10 18:20:06 iteration: 81150 loss: 0.0010 lr: 0.02
2019-10-10 18:20:12 iteration: 81200 loss: 0.0009 lr: 0.02
2019-10-10 18:20:18 iteration: 81250 loss: 0.0010 lr: 0.02
2019-10-10 18:20:23 iteration: 81300 loss: 0.0008 lr: 0.02
2019-10-10 18:20:29 iteration: 81350 loss: 0.0009 lr: 0.02
2019-10-10 18:20:35 iteration: 81400 loss: 0.0008 lr: 0.02
2019-10-10 18:20:41 iteration: 81450 loss: 0.0009 lr: 0.02
2019-10-10 18:20:46 iteration: 81500 loss: 0.0009 lr: 0.02
2019-10-10 18:20:52 iteration: 81550 loss: 0.0008 lr: 0.02
2019-10-10 18:20:58 iteration: 81600 loss: 0.0009 lr: 0.02
2019-10-10 18:21:03 iteration: 81650 loss: 0.0010 lr: 0.02
2019-10-10 18:21:11 iteration: 81700 loss: 0.0008 lr: 0.02
2019-10-10 18:21:16 iteration: 81750 loss: 0.0009 lr: 0.02
2019-10-10 18:21:21 iteration: 81800 loss: 0.0009 lr: 0.02
2019-10-10 18:21:27 iteration: 81850 loss: 0.0008 lr: 0.02
2019-10-10 18:21:31 iteration: 81900 loss: 0.0009 lr: 0.02
2019-10-10 18:21:37 iteration: 81950 loss: 0.0009 lr: 0.02
2019-10-10 18:21:43 iteration: 82000 loss: 0.0008 lr: 0.02
2019-10-10 18:21:49 iteration: 82050 loss: 0.0009 lr: 0.02
2019-10-10 18:21:54 iteration: 82100 loss: 0.0009 lr: 0.02
2019-10-10 18:22:01 iteration: 82150 loss: 0.0008 lr: 0.02
2019-10-10 18:22:06 iteration: 82200 loss: 0.0008 lr: 0.02
2019-10-10 18:22:11 iteration: 82250 loss: 0.0009 lr: 0.02
2019-10-10 18:22:17 iteration: 82300 loss: 0.0009 lr: 0.02
2019-10-10 18:22:21 iteration: 82350 loss: 0.0009 lr: 0.02
2019-10-10 18:22:27 iteration: 82400 loss: 0.0008 lr: 0.02
2019-10-10 18:22:32 iteration: 82450 loss: 0.0008 lr: 0.02
2019-10-10 18:22:38 iteration: 82500 loss: 0.0009 lr: 0.02
2019-10-10 18:22:42 iteration: 82550 loss: 0.0009 lr: 0.02
2019-10-10 18:22:49 iteration: 82600 loss: 0.0009 lr: 0.02
2019-10-10 18:22:54 iteration: 82650 loss: 0.0009 lr: 0.02
2019-10-10 18:22:58 iteration: 82700 loss: 0.0008 lr: 0.02
2019-10-10 18:23:03 iteration: 82750 loss: 0.0009 lr: 0.02
2019-10-10 18:23:08 iteration: 82800 loss: 0.0009 lr: 0.02
2019-10-10 18:23:14 iteration: 82850 loss: 0.0009 lr: 0.02
2019-10-10 18:23:19 iteration: 82900 loss: 0.0009 lr: 0.02
2019-10-10 18:23:23 iteration: 82950 loss: 0.0009 lr: 0.02
2019-10-10 18:23:27 iteration: 83000 loss: 0.0010 lr: 0.02
2019-10-10 18:23:33 iteration: 83050 loss: 0.0007 lr: 0.02
2019-10-10 18:23:38 iteration: 83100 loss: 0.0010 lr: 0.02
2019-10-10 18:23:44 iteration: 83150 loss: 0.0007 lr: 0.02
2019-10-10 18:23:49 iteration: 83200 loss: 0.0008 lr: 0.02
2019-10-10 18:23:55 iteration: 83250 loss: 0.0010 lr: 0.02
2019-10-10 18:24:01 iteration: 83300 loss: 0.0008 lr: 0.02
2019-10-10 18:24:07 iteration: 83350 loss: 0.0009 lr: 0.02
2019-10-10 18:24:13 iteration: 83400 loss: 0.0009 lr: 0.02
2019-10-10 18:24:17 iteration: 83450 loss: 0.0009 lr: 0.02
2019-10-10 18:24:22 iteration: 83500 loss: 0.0007 lr: 0.02
2019-10-10 18:24:27 iteration: 83550 loss: 0.0008 lr: 0.02
2019-10-10 18:24:34 iteration: 83600 loss: 0.0008 lr: 0.02
2019-10-10 18:24:40 iteration: 83650 loss: 0.0008 lr: 0.02
2019-10-10 18:24:45 iteration: 83700 loss: 0.0009 lr: 0.02
2019-10-10 18:24:50 iteration: 83750 loss: 0.0009 lr: 0.02
2019-10-10 18:24:55 iteration: 83800 loss: 0.0008 lr: 0.02
2019-10-10 18:25:01 iteration: 83850 loss: 0.0008 lr: 0.02
2019-10-10 18:25:06 iteration: 83900 loss: 0.0009 lr: 0.02
2019-10-10 18:25:12 iteration: 83950 loss: 0.0009 lr: 0.02
2019-10-10 18:25:16 iteration: 84000 loss: 0.0009 lr: 0.02
2019-10-10 18:25:21 iteration: 84050 loss: 0.0009 lr: 0.02
2019-10-10 18:25:27 iteration: 84100 loss: 0.0008 lr: 0.02
2019-10-10 18:25:32 iteration: 84150 loss: 0.0009 lr: 0.02
2019-10-10 18:25:39 iteration: 84200 loss: 0.0009 lr: 0.02
2019-10-10 18:25:45 iteration: 84250 loss: 0.0009 lr: 0.02
2019-10-10 18:25:50 iteration: 84300 loss: 0.0009 lr: 0.02
2019-10-10 18:25:54 iteration: 84350 loss: 0.0008 lr: 0.02
2019-10-10 18:26:01 iteration: 84400 loss: 0.0009 lr: 0.02
2019-10-10 18:26:06 iteration: 84450 loss: 0.0008 lr: 0.02
2019-10-10 18:26:12 iteration: 84500 loss: 0.0008 lr: 0.02
2019-10-10 18:26:16 iteration: 84550 loss: 0.0008 lr: 0.02
2019-10-10 18:26:21 iteration: 84600 loss: 0.0008 lr: 0.02
2019-10-10 18:26:26 iteration: 84650 loss: 0.0009 lr: 0.02
2019-10-10 18:26:31 iteration: 84700 loss: 0.0008 lr: 0.02
2019-10-10 18:26:35 iteration: 84750 loss: 0.0009 lr: 0.02
2019-10-10 18:26:39 iteration: 84800 loss: 0.0008 lr: 0.02
2019-10-10 18:26:46 iteration: 84850 loss: 0.0009 lr: 0.02
2019-10-10 18:26:52 iteration: 84900 loss: 0.0008 lr: 0.02
2019-10-10 18:26:57 iteration: 84950 loss: 0.0009 lr: 0.02
2019-10-10 18:27:03 iteration: 85000 loss: 0.0008 lr: 0.02
2019-10-10 18:27:10 iteration: 85050 loss: 0.0007 lr: 0.02
2019-10-10 18:27:15 iteration: 85100 loss: 0.0009 lr: 0.02
2019-10-10 18:27:21 iteration: 85150 loss: 0.0008 lr: 0.02
2019-10-10 18:27:25 iteration: 85200 loss: 0.0009 lr: 0.02
2019-10-10 18:27:31 iteration: 85250 loss: 0.0010 lr: 0.02
2019-10-10 18:27:37 iteration: 85300 loss: 0.0010 lr: 0.02
2019-10-10 18:27:43 iteration: 85350 loss: 0.0008 lr: 0.02
2019-10-10 18:27:48 iteration: 85400 loss: 0.0008 lr: 0.02
2019-10-10 18:27:53 iteration: 85450 loss: 0.0008 lr: 0.02
2019-10-10 18:28:00 iteration: 85500 loss: 0.0008 lr: 0.02
2019-10-10 18:28:05 iteration: 85550 loss: 0.0007 lr: 0.02
2019-10-10 18:28:13 iteration: 85600 loss: 0.0011 lr: 0.02
2019-10-10 18:28:19 iteration: 85650 loss: 0.0009 lr: 0.02
2019-10-10 18:28:25 iteration: 85700 loss: 0.0008 lr: 0.02
2019-10-10 18:28:31 iteration: 85750 loss: 0.0009 lr: 0.02
2019-10-10 18:28:36 iteration: 85800 loss: 0.0008 lr: 0.02
2019-10-10 18:28:40 iteration: 85850 loss: 0.0009 lr: 0.02
2019-10-10 18:28:47 iteration: 85900 loss: 0.0010 lr: 0.02
2019-10-10 18:28:53 iteration: 85950 loss: 0.0008 lr: 0.02
2019-10-10 18:28:59 iteration: 86000 loss: 0.0008 lr: 0.02
2019-10-10 18:29:04 iteration: 86050 loss: 0.0009 lr: 0.02
2019-10-10 18:29:09 iteration: 86100 loss: 0.0008 lr: 0.02
2019-10-10 18:29:13 iteration: 86150 loss: 0.0008 lr: 0.02
2019-10-10 18:29:18 iteration: 86200 loss: 0.0008 lr: 0.02
2019-10-10 18:29:24 iteration: 86250 loss: 0.0009 lr: 0.02
2019-10-10 18:29:30 iteration: 86300 loss: 0.0010 lr: 0.02
2019-10-10 18:29:36 iteration: 86350 loss: 0.0009 lr: 0.02
2019-10-10 18:29:42 iteration: 86400 loss: 0.0008 lr: 0.02
2019-10-10 18:29:48 iteration: 86450 loss: 0.0008 lr: 0.02
2019-10-10 18:29:53 iteration: 86500 loss: 0.0009 lr: 0.02
2019-10-10 18:29:58 iteration: 86550 loss: 0.0008 lr: 0.02
2019-10-10 18:30:04 iteration: 86600 loss: 0.0009 lr: 0.02
2019-10-10 18:30:09 iteration: 86650 loss: 0.0008 lr: 0.02
2019-10-10 18:30:14 iteration: 86700 loss: 0.0008 lr: 0.02
2019-10-10 18:30:18 iteration: 86750 loss: 0.0008 lr: 0.02
2019-10-10 18:30:24 iteration: 86800 loss: 0.0008 lr: 0.02
2019-10-10 18:30:31 iteration: 86850 loss: 0.0008 lr: 0.02
2019-10-10 18:30:37 iteration: 86900 loss: 0.0009 lr: 0.02
2019-10-10 18:30:42 iteration: 86950 loss: 0.0009 lr: 0.02
2019-10-10 18:30:47 iteration: 87000 loss: 0.0009 lr: 0.02
2019-10-10 18:30:52 iteration: 87050 loss: 0.0008 lr: 0.02
2019-10-10 18:30:59 iteration: 87100 loss: 0.0008 lr: 0.02
2019-10-10 18:31:05 iteration: 87150 loss: 0.0008 lr: 0.02
2019-10-10 18:31:11 iteration: 87200 loss: 0.0009 lr: 0.02
2019-10-10 18:31:15 iteration: 87250 loss: 0.0010 lr: 0.02
2019-10-10 18:31:22 iteration: 87300 loss: 0.0008 lr: 0.02
2019-10-10 18:31:28 iteration: 87350 loss: 0.0009 lr: 0.02
2019-10-10 18:31:34 iteration: 87400 loss: 0.0008 lr: 0.02
2019-10-10 18:31:39 iteration: 87450 loss: 0.0008 lr: 0.02
2019-10-10 18:31:44 iteration: 87500 loss: 0.0009 lr: 0.02
2019-10-10 18:31:49 iteration: 87550 loss: 0.0009 lr: 0.02
2019-10-10 18:31:56 iteration: 87600 loss: 0.0009 lr: 0.02
2019-10-10 18:32:02 iteration: 87650 loss: 0.0007 lr: 0.02
2019-10-10 18:32:07 iteration: 87700 loss: 0.0008 lr: 0.02
2019-10-10 18:32:11 iteration: 87750 loss: 0.0007 lr: 0.02
2019-10-10 18:32:18 iteration: 87800 loss: 0.0008 lr: 0.02
2019-10-10 18:32:23 iteration: 87850 loss: 0.0008 lr: 0.02
2019-10-10 18:32:28 iteration: 87900 loss: 0.0008 lr: 0.02
2019-10-10 18:32:33 iteration: 87950 loss: 0.0008 lr: 0.02
2019-10-10 18:32:39 iteration: 88000 loss: 0.0008 lr: 0.02
2019-10-10 18:32:44 iteration: 88050 loss: 0.0008 lr: 0.02
2019-10-10 18:32:48 iteration: 88100 loss: 0.0009 lr: 0.02
2019-10-10 18:32:56 iteration: 88150 loss: 0.0008 lr: 0.02
2019-10-10 18:33:01 iteration: 88200 loss: 0.0009 lr: 0.02
2019-10-10 18:33:08 iteration: 88250 loss: 0.0008 lr: 0.02
2019-10-10 18:33:14 iteration: 88300 loss: 0.0009 lr: 0.02
2019-10-10 18:33:18 iteration: 88350 loss: 0.0008 lr: 0.02
2019-10-10 18:33:23 iteration: 88400 loss: 0.0008 lr: 0.02
2019-10-10 18:33:29 iteration: 88450 loss: 0.0009 lr: 0.02
2019-10-10 18:33:34 iteration: 88500 loss: 0.0009 lr: 0.02
2019-10-10 18:33:39 iteration: 88550 loss: 0.0010 lr: 0.02
2019-10-10 18:33:44 iteration: 88600 loss: 0.0008 lr: 0.02
2019-10-10 18:33:49 iteration: 88650 loss: 0.0008 lr: 0.02
2019-10-10 18:33:56 iteration: 88700 loss: 0.0009 lr: 0.02
2019-10-10 18:34:02 iteration: 88750 loss: 0.0010 lr: 0.02
2019-10-10 18:34:07 iteration: 88800 loss: 0.0008 lr: 0.02
2019-10-10 18:34:12 iteration: 88850 loss: 0.0009 lr: 0.02
2019-10-10 18:34:17 iteration: 88900 loss: 0.0008 lr: 0.02
2019-10-10 18:34:23 iteration: 88950 loss: 0.0009 lr: 0.02
2019-10-10 18:34:28 iteration: 89000 loss: 0.0008 lr: 0.02
2019-10-10 18:34:33 iteration: 89050 loss: 0.0008 lr: 0.02
2019-10-10 18:34:38 iteration: 89100 loss: 0.0008 lr: 0.02
2019-10-10 18:34:44 iteration: 89150 loss: 0.0008 lr: 0.02
2019-10-10 18:34:49 iteration: 89200 loss: 0.0008 lr: 0.02
2019-10-10 18:34:55 iteration: 89250 loss: 0.0008 lr: 0.02
2019-10-10 18:35:00 iteration: 89300 loss: 0.0008 lr: 0.02
2019-10-10 18:35:05 iteration: 89350 loss: 0.0008 lr: 0.02
2019-10-10 18:35:10 iteration: 89400 loss: 0.0008 lr: 0.02
2019-10-10 18:35:15 iteration: 89450 loss: 0.0007 lr: 0.02
2019-10-10 18:35:20 iteration: 89500 loss: 0.0009 lr: 0.02
2019-10-10 18:35:25 iteration: 89550 loss: 0.0008 lr: 0.02
2019-10-10 18:35:30 iteration: 89600 loss: 0.0008 lr: 0.02
2019-10-10 18:35:35 iteration: 89650 loss: 0.0009 lr: 0.02
2019-10-10 18:35:40 iteration: 89700 loss: 0.0008 lr: 0.02
2019-10-10 18:35:46 iteration: 89750 loss: 0.0008 lr: 0.02
2019-10-10 18:35:51 iteration: 89800 loss: 0.0009 lr: 0.02
2019-10-10 18:35:56 iteration: 89850 loss: 0.0009 lr: 0.02
2019-10-10 18:36:02 iteration: 89900 loss: 0.0008 lr: 0.02
2019-10-10 18:36:07 iteration: 89950 loss: 0.0008 lr: 0.02
2019-10-10 18:36:12 iteration: 90000 loss: 0.0007 lr: 0.02
2019-10-10 18:36:17 iteration: 90050 loss: 0.0008 lr: 0.02
2019-10-10 18:36:24 iteration: 90100 loss: 0.0009 lr: 0.02
2019-10-10 18:36:30 iteration: 90150 loss: 0.0008 lr: 0.02
2019-10-10 18:36:35 iteration: 90200 loss: 0.0009 lr: 0.02
2019-10-10 18:36:42 iteration: 90250 loss: 0.0009 lr: 0.02
2019-10-10 18:36:49 iteration: 90300 loss: 0.0009 lr: 0.02
2019-10-10 18:36:55 iteration: 90350 loss: 0.0010 lr: 0.02
2019-10-10 18:37:00 iteration: 90400 loss: 0.0008 lr: 0.02
2019-10-10 18:37:06 iteration: 90450 loss: 0.0009 lr: 0.02
2019-10-10 18:37:12 iteration: 90500 loss: 0.0009 lr: 0.02
2019-10-10 18:37:17 iteration: 90550 loss: 0.0009 lr: 0.02
2019-10-10 18:37:23 iteration: 90600 loss: 0.0008 lr: 0.02
2019-10-10 18:37:29 iteration: 90650 loss: 0.0009 lr: 0.02
2019-10-10 18:37:36 iteration: 90700 loss: 0.0009 lr: 0.02
2019-10-10 18:37:41 iteration: 90750 loss: 0.0009 lr: 0.02
2019-10-10 18:37:46 iteration: 90800 loss: 0.0009 lr: 0.02
2019-10-10 18:37:53 iteration: 90850 loss: 0.0008 lr: 0.02
2019-10-10 18:37:59 iteration: 90900 loss: 0.0008 lr: 0.02
2019-10-10 18:38:04 iteration: 90950 loss: 0.0008 lr: 0.02
2019-10-10 18:38:08 iteration: 91000 loss: 0.0008 lr: 0.02
2019-10-10 18:38:15 iteration: 91050 loss: 0.0009 lr: 0.02
2019-10-10 18:38:20 iteration: 91100 loss: 0.0008 lr: 0.02
2019-10-10 18:38:25 iteration: 91150 loss: 0.0009 lr: 0.02
2019-10-10 18:38:30 iteration: 91200 loss: 0.0007 lr: 0.02
2019-10-10 18:38:35 iteration: 91250 loss: 0.0007 lr: 0.02
2019-10-10 18:38:40 iteration: 91300 loss: 0.0008 lr: 0.02
2019-10-10 18:38:44 iteration: 91350 loss: 0.0008 lr: 0.02
2019-10-10 18:38:50 iteration: 91400 loss: 0.0007 lr: 0.02
2019-10-10 18:38:55 iteration: 91450 loss: 0.0009 lr: 0.02
2019-10-10 18:39:01 iteration: 91500 loss: 0.0009 lr: 0.02
2019-10-10 18:39:07 iteration: 91550 loss: 0.0008 lr: 0.02
2019-10-10 18:39:11 iteration: 91600 loss: 0.0008 lr: 0.02
2019-10-10 18:39:17 iteration: 91650 loss: 0.0009 lr: 0.02
2019-10-10 18:39:23 iteration: 91700 loss: 0.0008 lr: 0.02
2019-10-10 18:39:30 iteration: 91750 loss: 0.0009 lr: 0.02
2019-10-10 18:39:35 iteration: 91800 loss: 0.0009 lr: 0.02
2019-10-10 18:39:40 iteration: 91850 loss: 0.0008 lr: 0.02
2019-10-10 18:39:46 iteration: 91900 loss: 0.0009 lr: 0.02
2019-10-10 18:39:50 iteration: 91950 loss: 0.0009 lr: 0.02
2019-10-10 18:39:55 iteration: 92000 loss: 0.0009 lr: 0.02
2019-10-10 18:40:00 iteration: 92050 loss: 0.0009 lr: 0.02
2019-10-10 18:40:06 iteration: 92100 loss: 0.0008 lr: 0.02
2019-10-10 18:40:11 iteration: 92150 loss: 0.0009 lr: 0.02
2019-10-10 18:40:16 iteration: 92200 loss: 0.0009 lr: 0.02
2019-10-10 18:40:22 iteration: 92250 loss: 0.0008 lr: 0.02
2019-10-10 18:40:28 iteration: 92300 loss: 0.0008 lr: 0.02
2019-10-10 18:40:34 iteration: 92350 loss: 0.0008 lr: 0.02
2019-10-10 18:40:39 iteration: 92400 loss: 0.0009 lr: 0.02
2019-10-10 18:40:44 iteration: 92450 loss: 0.0009 lr: 0.02
2019-10-10 18:40:51 iteration: 92500 loss: 0.0008 lr: 0.02
2019-10-10 18:40:57 iteration: 92550 loss: 0.0007 lr: 0.02
2019-10-10 18:41:03 iteration: 92600 loss: 0.0009 lr: 0.02
2019-10-10 18:41:07 iteration: 92650 loss: 0.0009 lr: 0.02
2019-10-10 18:41:13 iteration: 92700 loss: 0.0008 lr: 0.02
2019-10-10 18:41:19 iteration: 92750 loss: 0.0007 lr: 0.02
2019-10-10 18:41:25 iteration: 92800 loss: 0.0009 lr: 0.02
2019-10-10 18:41:32 iteration: 92850 loss: 0.0008 lr: 0.02
2019-10-10 18:41:36 iteration: 92900 loss: 0.0009 lr: 0.02
2019-10-10 18:41:43 iteration: 92950 loss: 0.0008 lr: 0.02
2019-10-10 18:41:48 iteration: 93000 loss: 0.0009 lr: 0.02
2019-10-10 18:41:55 iteration: 93050 loss: 0.0008 lr: 0.02
2019-10-10 18:42:00 iteration: 93100 loss: 0.0008 lr: 0.02
2019-10-10 18:42:04 iteration: 93150 loss: 0.0007 lr: 0.02
2019-10-10 18:42:12 iteration: 93200 loss: 0.0008 lr: 0.02
2019-10-10 18:42:17 iteration: 93250 loss: 0.0009 lr: 0.02
2019-10-10 18:42:22 iteration: 93300 loss: 0.0007 lr: 0.02
2019-10-10 18:42:29 iteration: 93350 loss: 0.0008 lr: 0.02
2019-10-10 18:42:34 iteration: 93400 loss: 0.0009 lr: 0.02
2019-10-10 18:42:39 iteration: 93450 loss: 0.0008 lr: 0.02
2019-10-10 18:42:45 iteration: 93500 loss: 0.0009 lr: 0.02
2019-10-10 18:42:51 iteration: 93550 loss: 0.0010 lr: 0.02
2019-10-10 18:42:57 iteration: 93600 loss: 0.0009 lr: 0.02
2019-10-10 18:43:03 iteration: 93650 loss: 0.0009 lr: 0.02
2019-10-10 18:43:08 iteration: 93700 loss: 0.0008 lr: 0.02
2019-10-10 18:43:14 iteration: 93750 loss: 0.0009 lr: 0.02
2019-10-10 18:43:20 iteration: 93800 loss: 0.0009 lr: 0.02
2019-10-10 18:43:26 iteration: 93850 loss: 0.0008 lr: 0.02
2019-10-10 18:43:32 iteration: 93900 loss: 0.0008 lr: 0.02
2019-10-10 18:43:37 iteration: 93950 loss: 0.0008 lr: 0.02
2019-10-10 18:43:41 iteration: 94000 loss: 0.0009 lr: 0.02
2019-10-10 18:43:46 iteration: 94050 loss: 0.0008 lr: 0.02
2019-10-10 18:43:51 iteration: 94100 loss: 0.0009 lr: 0.02
2019-10-10 18:43:56 iteration: 94150 loss: 0.0008 lr: 0.02
2019-10-10 18:44:00 iteration: 94200 loss: 0.0008 lr: 0.02
2019-10-10 18:44:06 iteration: 94250 loss: 0.0008 lr: 0.02
2019-10-10 18:44:10 iteration: 94300 loss: 0.0008 lr: 0.02
2019-10-10 18:44:16 iteration: 94350 loss: 0.0008 lr: 0.02
2019-10-10 18:44:22 iteration: 94400 loss: 0.0010 lr: 0.02
2019-10-10 18:44:26 iteration: 94450 loss: 0.0008 lr: 0.02
2019-10-10 18:44:31 iteration: 94500 loss: 0.0008 lr: 0.02
2019-10-10 18:44:37 iteration: 94550 loss: 0.0008 lr: 0.02
2019-10-10 18:44:43 iteration: 94600 loss: 0.0010 lr: 0.02
2019-10-10 18:44:49 iteration: 94650 loss: 0.0009 lr: 0.02
2019-10-10 18:44:55 iteration: 94700 loss: 0.0009 lr: 0.02
2019-10-10 18:44:59 iteration: 94750 loss: 0.0010 lr: 0.02
2019-10-10 18:45:05 iteration: 94800 loss: 0.0008 lr: 0.02
2019-10-10 18:45:10 iteration: 94850 loss: 0.0008 lr: 0.02
2019-10-10 18:45:15 iteration: 94900 loss: 0.0008 lr: 0.02
2019-10-10 18:45:21 iteration: 94950 loss: 0.0009 lr: 0.02
2019-10-10 18:45:27 iteration: 95000 loss: 0.0008 lr: 0.02
2019-10-10 18:45:33 iteration: 95050 loss: 0.0008 lr: 0.02
2019-10-10 18:45:40 iteration: 95100 loss: 0.0008 lr: 0.02
2019-10-10 18:45:45 iteration: 95150 loss: 0.0008 lr: 0.02
2019-10-10 18:45:50 iteration: 95200 loss: 0.0009 lr: 0.02
2019-10-10 18:45:56 iteration: 95250 loss: 0.0008 lr: 0.02
2019-10-10 18:46:01 iteration: 95300 loss: 0.0008 lr: 0.02
2019-10-10 18:46:08 iteration: 95350 loss: 0.0008 lr: 0.02
2019-10-10 18:46:13 iteration: 95400 loss: 0.0010 lr: 0.02
2019-10-10 18:46:19 iteration: 95450 loss: 0.0009 lr: 0.02
2019-10-10 18:46:25 iteration: 95500 loss: 0.0008 lr: 0.02
2019-10-10 18:46:29 iteration: 95550 loss: 0.0008 lr: 0.02
2019-10-10 18:46:35 iteration: 95600 loss: 0.0007 lr: 0.02
2019-10-10 18:46:39 iteration: 95650 loss: 0.0008 lr: 0.02
2019-10-10 18:46:46 iteration: 95700 loss: 0.0008 lr: 0.02
2019-10-10 18:46:51 iteration: 95750 loss: 0.0010 lr: 0.02
2019-10-10 18:46:56 iteration: 95800 loss: 0.0009 lr: 0.02
2019-10-10 18:47:01 iteration: 95850 loss: 0.0008 lr: 0.02
2019-10-10 18:47:07 iteration: 95900 loss: 0.0008 lr: 0.02
2019-10-10 18:47:13 iteration: 95950 loss: 0.0008 lr: 0.02
2019-10-10 18:47:19 iteration: 96000 loss: 0.0008 lr: 0.02
2019-10-10 18:47:24 iteration: 96050 loss: 0.0007 lr: 0.02
2019-10-10 18:47:30 iteration: 96100 loss: 0.0010 lr: 0.02
2019-10-10 18:47:35 iteration: 96150 loss: 0.0009 lr: 0.02
2019-10-10 18:47:41 iteration: 96200 loss: 0.0008 lr: 0.02
2019-10-10 18:47:46 iteration: 96250 loss: 0.0008 lr: 0.02
2019-10-10 18:47:52 iteration: 96300 loss: 0.0009 lr: 0.02
2019-10-10 18:47:57 iteration: 96350 loss: 0.0009 lr: 0.02
2019-10-10 18:48:01 iteration: 96400 loss: 0.0009 lr: 0.02
2019-10-10 18:48:06 iteration: 96450 loss: 0.0009 lr: 0.02
2019-10-10 18:48:11 iteration: 96500 loss: 0.0008 lr: 0.02
2019-10-10 18:48:17 iteration: 96550 loss: 0.0007 lr: 0.02
2019-10-10 18:48:22 iteration: 96600 loss: 0.0009 lr: 0.02
2019-10-10 18:48:28 iteration: 96650 loss: 0.0008 lr: 0.02
2019-10-10 18:48:33 iteration: 96700 loss: 0.0009 lr: 0.02
2019-10-10 18:48:38 iteration: 96750 loss: 0.0008 lr: 0.02
2019-10-10 18:48:42 iteration: 96800 loss: 0.0009 lr: 0.02
2019-10-10 18:48:49 iteration: 96850 loss: 0.0009 lr: 0.02
2019-10-10 18:48:53 iteration: 96900 loss: 0.0007 lr: 0.02
2019-10-10 18:49:00 iteration: 96950 loss: 0.0009 lr: 0.02
2019-10-10 18:49:05 iteration: 97000 loss: 0.0007 lr: 0.02
2019-10-10 18:49:10 iteration: 97050 loss: 0.0008 lr: 0.02
2019-10-10 18:49:15 iteration: 97100 loss: 0.0008 lr: 0.02
2019-10-10 18:49:21 iteration: 97150 loss: 0.0007 lr: 0.02
2019-10-10 18:49:25 iteration: 97200 loss: 0.0009 lr: 0.02
2019-10-10 18:49:32 iteration: 97250 loss: 0.0008 lr: 0.02
2019-10-10 18:49:38 iteration: 97300 loss: 0.0009 lr: 0.02
2019-10-10 18:49:43 iteration: 97350 loss: 0.0007 lr: 0.02
2019-10-10 18:49:49 iteration: 97400 loss: 0.0009 lr: 0.02
2019-10-10 18:49:55 iteration: 97450 loss: 0.0009 lr: 0.02
2019-10-10 18:50:01 iteration: 97500 loss: 0.0010 lr: 0.02
2019-10-10 18:50:07 iteration: 97550 loss: 0.0007 lr: 0.02
2019-10-10 18:50:12 iteration: 97600 loss: 0.0007 lr: 0.02
2019-10-10 18:50:19 iteration: 97650 loss: 0.0009 lr: 0.02
2019-10-10 18:50:24 iteration: 97700 loss: 0.0009 lr: 0.02
2019-10-10 18:50:29 iteration: 97750 loss: 0.0008 lr: 0.02
2019-10-10 18:50:34 iteration: 97800 loss: 0.0008 lr: 0.02
2019-10-10 18:50:39 iteration: 97850 loss: 0.0009 lr: 0.02
2019-10-10 18:50:43 iteration: 97900 loss: 0.0010 lr: 0.02
2019-10-10 18:50:49 iteration: 97950 loss: 0.0007 lr: 0.02
2019-10-10 18:50:54 iteration: 98000 loss: 0.0009 lr: 0.02
2019-10-10 18:50:58 iteration: 98050 loss: 0.0008 lr: 0.02
2019-10-10 18:51:04 iteration: 98100 loss: 0.0007 lr: 0.02
2019-10-10 18:51:08 iteration: 98150 loss: 0.0007 lr: 0.02
2019-10-10 18:51:15 iteration: 98200 loss: 0.0008 lr: 0.02
2019-10-10 18:51:22 iteration: 98250 loss: 0.0009 lr: 0.02
2019-10-10 18:51:27 iteration: 98300 loss: 0.0009 lr: 0.02
2019-10-10 18:51:32 iteration: 98350 loss: 0.0009 lr: 0.02
2019-10-10 18:51:37 iteration: 98400 loss: 0.0009 lr: 0.02
2019-10-10 18:51:43 iteration: 98450 loss: 0.0008 lr: 0.02
2019-10-10 18:51:48 iteration: 98500 loss: 0.0008 lr: 0.02
2019-10-10 18:51:55 iteration: 98550 loss: 0.0009 lr: 0.02
2019-10-10 18:52:01 iteration: 98600 loss: 0.0007 lr: 0.02
2019-10-10 18:52:07 iteration: 98650 loss: 0.0007 lr: 0.02
2019-10-10 18:52:13 iteration: 98700 loss: 0.0008 lr: 0.02
2019-10-10 18:52:17 iteration: 98750 loss: 0.0008 lr: 0.02
2019-10-10 18:52:23 iteration: 98800 loss: 0.0008 lr: 0.02
2019-10-10 18:52:28 iteration: 98850 loss: 0.0007 lr: 0.02
2019-10-10 18:52:34 iteration: 98900 loss: 0.0009 lr: 0.02
2019-10-10 18:52:39 iteration: 98950 loss: 0.0009 lr: 0.02
2019-10-10 18:52:44 iteration: 99000 loss: 0.0008 lr: 0.02
2019-10-10 18:52:51 iteration: 99050 loss: 0.0008 lr: 0.02
2019-10-10 18:52:56 iteration: 99100 loss: 0.0008 lr: 0.02
2019-10-10 18:53:00 iteration: 99150 loss: 0.0009 lr: 0.02
2019-10-10 18:53:06 iteration: 99200 loss: 0.0009 lr: 0.02
2019-10-10 18:53:11 iteration: 99250 loss: 0.0008 lr: 0.02
2019-10-10 18:53:16 iteration: 99300 loss: 0.0008 lr: 0.02
2019-10-10 18:53:22 iteration: 99350 loss: 0.0008 lr: 0.02
2019-10-10 18:53:27 iteration: 99400 loss: 0.0008 lr: 0.02
2019-10-10 18:53:33 iteration: 99450 loss: 0.0008 lr: 0.02
2019-10-10 18:53:39 iteration: 99500 loss: 0.0009 lr: 0.02
2019-10-10 18:53:44 iteration: 99550 loss: 0.0009 lr: 0.02
2019-10-10 18:53:50 iteration: 99600 loss: 0.0009 lr: 0.02
2019-10-10 18:53:55 iteration: 99650 loss: 0.0010 lr: 0.02
2019-10-10 18:54:01 iteration: 99700 loss: 0.0009 lr: 0.02
2019-10-10 18:54:06 iteration: 99750 loss: 0.0008 lr: 0.02
2019-10-10 18:54:10 iteration: 99800 loss: 0.0010 lr: 0.02
2019-10-10 18:54:16 iteration: 99850 loss: 0.0008 lr: 0.02
2019-10-10 18:54:21 iteration: 99900 loss: 0.0008 lr: 0.02
2019-10-10 18:54:26 iteration: 99950 loss: 0.0008 lr: 0.02
2019-10-10 18:54:32 iteration: 100000 loss: 0.0009 lr: 0.02
2019-10-11 02:38:31 Config:
{'all_joints': [[0], [1], [2]],
 'all_joints_names': ['COM', 'wand_1', 'wand_2'],
 'batch_size': 1,
 'bottomheight': 400,
 'crop': True,
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_arena_chaseOct10\\arena_chase_Andy95shuffle1.mat',
 'dataset_type': 'default',
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'leftwidth': 400,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_arena_chaseOct10\\Documentation_data-arena_chase_95shuffle1.pickle',
 'min_input_size': 64,
 'minsize': 100,
 'mirror': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 3,
 'optimizer': 'sgd',
 'pos_dist_thresh': 17,
 'project_path': 'E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-10-10',
 'regularize': False,
 'rightwidth': 400,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-10-10\\dlc-models\\iteration-0\\arena_chaseOct10-trainset95shuffle1\\test\\snapshot',
 'stride': 8.0,
 'topheight': 400,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2019-10-11 02:40:27 Config:
{'all_joints': [[0], [1], [2]],
 'all_joints_names': ['COM', 'wand_1', 'wand_2'],
 'batch_size': 1,
 'bottomheight': 400,
 'crop': True,
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_arena_chaseOct10\\arena_chase_Andy95shuffle1.mat',
 'dataset_type': 'default',
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'leftwidth': 400,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_arena_chaseOct10\\Documentation_data-arena_chase_95shuffle1.pickle',
 'min_input_size': 64,
 'minsize': 100,
 'mirror': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 3,
 'optimizer': 'sgd',
 'pos_dist_thresh': 17,
 'project_path': 'E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-10-10',
 'regularize': False,
 'rightwidth': 400,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-10-10\\dlc-models\\iteration-0\\arena_chaseOct10-trainset95shuffle1\\test\\snapshot',
 'stride': 8.0,
 'topheight': 400,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2019-10-11 02:40:39 Config:
{'all_joints': [[0], [1], [2]],
 'all_joints_names': ['COM', 'wand_1', 'wand_2'],
 'batch_size': 8,
 'bottomheight': 400,
 'crop': True,
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_arena_chaseOct10\\arena_chase_Andy95shuffle1.mat',
 'dataset_type': 'default',
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'leftwidth': 400,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_arena_chaseOct10\\Documentation_data-arena_chase_95shuffle1.pickle',
 'min_input_size': 64,
 'minsize': 100,
 'mirror': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 3,
 'num_outputs': 1,
 'optimizer': 'sgd',
 'pos_dist_thresh': 17,
 'project_path': 'E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-10-10',
 'regularize': False,
 'rightwidth': 400,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-10-10\\dlc-models\\iteration-0\\arena_chaseOct10-trainset95shuffle1\\test\\snapshot',
 'stride': 8.0,
 'topheight': 400,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2019-10-11 02:43:17 Config:
{'all_joints': [[0], [1], [2]],
 'all_joints_names': ['COM', 'wand_1', 'wand_2'],
 'batch_size': 8,
 'bottomheight': 400,
 'crop': True,
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_arena_chaseOct10\\arena_chase_Andy95shuffle1.mat',
 'dataset_type': 'default',
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'leftwidth': 400,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_arena_chaseOct10\\Documentation_data-arena_chase_95shuffle1.pickle',
 'min_input_size': 64,
 'minsize': 100,
 'mirror': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 3,
 'num_outputs': 1,
 'optimizer': 'sgd',
 'pos_dist_thresh': 17,
 'project_path': 'E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-10-10',
 'regularize': False,
 'rightwidth': 400,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-10-10\\dlc-models\\iteration-0\\arena_chaseOct10-trainset95shuffle1\\test\\snapshot',
 'stride': 8.0,
 'topheight': 400,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2019-10-11 02:43:34 Config:
{'all_joints': [[0], [1], [2]],
 'all_joints_names': ['COM', 'wand_1', 'wand_2'],
 'batch_size': 8,
 'bottomheight': 400,
 'crop': True,
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_arena_chaseOct10\\arena_chase_Andy95shuffle1.mat',
 'dataset_type': 'default',
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'leftwidth': 400,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_arena_chaseOct10\\Documentation_data-arena_chase_95shuffle1.pickle',
 'min_input_size': 64,
 'minsize': 100,
 'mirror': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 3,
 'num_outputs': 1,
 'optimizer': 'sgd',
 'pos_dist_thresh': 17,
 'project_path': 'E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-10-10',
 'regularize': False,
 'rightwidth': 400,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-10-10\\dlc-models\\iteration-0\\arena_chaseOct10-trainset95shuffle1\\test\\snapshot',
 'stride': 8.0,
 'topheight': 400,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2019-10-11 02:43:55 Config:
{'all_joints': [[0], [1], [2]],
 'all_joints_names': ['COM', 'wand_1', 'wand_2'],
 'batch_size': 8,
 'bottomheight': 400,
 'crop': True,
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_arena_chaseOct10\\arena_chase_Andy95shuffle1.mat',
 'dataset_type': 'default',
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'leftwidth': 400,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_arena_chaseOct10\\Documentation_data-arena_chase_95shuffle1.pickle',
 'min_input_size': 64,
 'minsize': 100,
 'mirror': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 3,
 'num_outputs': 1,
 'optimizer': 'sgd',
 'pos_dist_thresh': 17,
 'project_path': 'E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-10-10',
 'regularize': False,
 'rightwidth': 400,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-10-10\\dlc-models\\iteration-0\\arena_chaseOct10-trainset95shuffle1\\test\\snapshot',
 'stride': 8.0,
 'topheight': 400,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2019-10-11 02:44:24 Config:
{'all_joints': [[0], [1], [2]],
 'all_joints_names': ['COM', 'wand_1', 'wand_2'],
 'batch_size': 8,
 'bottomheight': 400,
 'crop': True,
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_arena_chaseOct10\\arena_chase_Andy95shuffle1.mat',
 'dataset_type': 'default',
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'leftwidth': 400,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_arena_chaseOct10\\Documentation_data-arena_chase_95shuffle1.pickle',
 'min_input_size': 64,
 'minsize': 100,
 'mirror': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 3,
 'num_outputs': 1,
 'optimizer': 'sgd',
 'pos_dist_thresh': 17,
 'project_path': 'E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-10-10',
 'regularize': False,
 'rightwidth': 400,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-10-10\\dlc-models\\iteration-0\\arena_chaseOct10-trainset95shuffle1\\test\\snapshot',
 'stride': 8.0,
 'topheight': 400,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2019-10-11 02:46:31 Config:
{'all_joints': [[0], [1], [2]],
 'all_joints_names': ['COM', 'wand_1', 'wand_2'],
 'batch_size': 8,
 'bottomheight': 400,
 'crop': True,
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_arena_chaseOct10\\arena_chase_Andy95shuffle1.mat',
 'dataset_type': 'default',
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'leftwidth': 400,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_arena_chaseOct10\\Documentation_data-arena_chase_95shuffle1.pickle',
 'min_input_size': 64,
 'minsize': 100,
 'mirror': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 3,
 'num_outputs': 1,
 'optimizer': 'sgd',
 'pos_dist_thresh': 17,
 'project_path': 'E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-10-10',
 'regularize': False,
 'rightwidth': 400,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-10-10\\dlc-models\\iteration-0\\arena_chaseOct10-trainset95shuffle1\\test\\snapshot',
 'stride': 8.0,
 'topheight': 400,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
2019-10-11 02:47:15 Config:
{'all_joints': [[0], [1], [2]],
 'all_joints_names': ['COM', 'wand_1', 'wand_2'],
 'batch_size': 8,
 'bottomheight': 400,
 'crop': True,
 'crop_pad': 0,
 'cropratio': 0.4,
 'dataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_arena_chaseOct10\\arena_chase_Andy95shuffle1.mat',
 'dataset_type': 'default',
 'deterministic': False,
 'display_iters': 1000,
 'fg_fraction': 0.25,
 'global_scale': 0.8,
 'init_weights': 'C:\\Users\\LabAdmin\\.conda\\envs\\dlc-windowsGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\models\\pretrained\\resnet_v1_50.ckpt',
 'intermediate_supervision': False,
 'intermediate_supervision_layer': 12,
 'leftwidth': 400,
 'location_refinement': True,
 'locref_huber_loss': True,
 'locref_loss_weight': 0.05,
 'locref_stdev': 7.2801,
 'log_dir': 'log',
 'max_input_size': 1500,
 'mean_pixel': [123.68, 116.779, 103.939],
 'metadataset': 'training-datasets\\iteration-0\\UnaugmentedDataSet_arena_chaseOct10\\Documentation_data-arena_chase_95shuffle1.pickle',
 'min_input_size': 64,
 'minsize': 100,
 'mirror': False,
 'multi_step': [[0.005, 10000],
                [0.02, 430000],
                [0.002, 730000],
                [0.001, 1030000]],
 'net_type': 'resnet_50',
 'num_joints': 3,
 'num_outputs': 1,
 'optimizer': 'sgd',
 'pos_dist_thresh': 17,
 'project_path': 'E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-10-10',
 'regularize': False,
 'rightwidth': 400,
 'save_iters': 50000,
 'scale_jitter_lo': 0.5,
 'scale_jitter_up': 1.25,
 'scoremap_dir': 'test',
 'shuffle': True,
 'snapshot_prefix': 'E:\\Users\\Phil\\DeepLabCut\\dev\\arena_chase-Andy-2019-10-10\\dlc-models\\iteration-0\\arena_chaseOct10-trainset95shuffle1\\test\\snapshot',
 'stride': 8.0,
 'topheight': 400,
 'weigh_negatives': False,
 'weigh_only_present_joints': False,
 'weigh_part_predictions': False,
 'weight_decay': 0.0001}
